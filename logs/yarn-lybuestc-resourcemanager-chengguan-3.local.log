2017-01-18 11:30:52,086 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = chengguan-3.local/10.1.4.199
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/rm-config/log4j.properties
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_91
************************************************************/
2017-01-18 11:30:52,094 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-18 11:30:52,379 INFO org.apache.hadoop.conf.Configuration: found resource core-site.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/core-site.xml
2017-01-18 11:30:52,428 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-01-18 11:30:52,464 INFO org.apache.hadoop.security.Groups: clearing userToGroupsMap cache
2017-01-18 11:30:52,530 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/yarn-site.xml
2017-01-18 11:30:52,757 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2017-01-18 11:30:53,040 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2017-01-18 11:30:53,045 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2017-01-18 11:30:53,054 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2017-01-18 11:30:53,131 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2017-01-18 11:30:53,135 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2017-01-18 11:30:53,135 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2017-01-18 11:30:53,160 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher
2017-01-18 11:30:53,161 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2017-01-18 11:30:53,161 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2017-01-18 11:30:53,162 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2017-01-18 11:30:53,270 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-18 11:30:53,389 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-18 11:30:53,389 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2017-01-18 11:30:53,420 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2017-01-18 11:30:53,430 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2017-01-18 11:30:53,434 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2017-01-18 11:30:53,438 INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instiantiated.
2017-01-18 11:30:53,439 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2017-01-18 11:30:53,441 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/capacity-scheduler.xml
2017-01-18 11:30:53,515 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2017-01-18 11:30:53,515 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2017-01-18 11:30:53,519 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, asboluteCapacity=1.0, maxCapacity=1.0, asboluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
, reservationsContinueLooking=true
2017-01-18 11:30:53,519 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2017-01-18 11:30:53,527 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2017-01-18 11:30:53,527 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2017-01-18 11:30:53,528 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
asboluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:8192, vCores:32> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
nodeLocalityDelay = 40
labels=*,
nodeLocalityDelay = 40
reservationsContinueLooking = true
preemptionDisabled = true

2017-01-18 11:30:53,528 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0
2017-01-18 11:30:53,528 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2017-01-18 11:30:53,529 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2017-01-18 11:30:53,529 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue mappings, override: false
2017-01-18 11:30:53,530 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:32>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2017-01-18 11:30:53,542 INFO org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher: YARN system metrics publishing service is not enabled
2017-01-18 11:30:53,543 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state
2017-01-18 11:30:53,564 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2017-01-18 11:30:53,565 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2017-01-18 11:30:53,565 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2017-01-18 11:30:53,565 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-18 11:30:53,566 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 1
2017-01-18 11:30:53,566 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2017-01-18 11:30:53,568 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2017-01-18 11:30:53,568 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-18 11:30:53,568 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 2
2017-01-18 11:30:53,568 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2017-01-18 11:30:53,571 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
2017-01-18 11:30:53,603 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-18 11:30:53,632 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8031
2017-01-18 11:30:53,650 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
2017-01-18 11:30:53,650 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-18 11:30:53,650 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8031: starting
2017-01-18 11:30:53,689 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-18 11:30:53,696 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8030
2017-01-18 11:30:53,705 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
2017-01-18 11:30:53,705 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-18 11:30:53,706 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8030: starting
2017-01-18 11:30:53,771 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-18 11:30:53,772 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8032
2017-01-18 11:30:53,775 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
2017-01-18 11:30:53,776 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-18 11:30:53,777 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8032: starting
2017-01-18 11:30:53,787 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
2017-01-18 11:30:53,947 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-18 11:30:53,956 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-18 11:30:53,986 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2017-01-18 11:30:54,006 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-18 11:30:54,011 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
2017-01-18 11:30:54,011 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
2017-01-18 11:30:54,011 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
2017-01-18 11:30:54,012 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2017-01-18 11:30:54,012 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-18 11:30:54,012 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-18 11:30:54,019 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /cluster/*
2017-01-18 11:30:54,019 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2017-01-18 11:30:54,634 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2017-01-18 11:30:54,638 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8088
2017-01-18 11:30:54,638 INFO org.mortbay.log: jetty-6.1.26
2017-01-18 11:30:54,685 INFO org.mortbay.log: Extract jar:file:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar!/webapps/cluster to /var/folders/xl/5zp3ynpd6l5cg_y4f6qjqxxm0000gn/T/Jetty_0_0_0_0_8088_cluster____u0rgz3/webapp
2017-01-18 11:30:54,952 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-18 11:30:54,953 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2017-01-18 11:30:54,953 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-18 11:30:56,875 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2017-01-18 11:30:56,875 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at 8088
2017-01-18 11:30:57,025 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-18 11:30:57,026 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2017-01-18 11:30:57,038 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2017-01-18 11:30:57,039 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-18 11:30:57,039 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8033: starting
2017-01-18 11:30:58,242 INFO org.apache.hadoop.yarn.util.RackResolver: Resolved 10.1.4.199 to /default-rack
2017-01-18 11:30:58,247 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node 10.1.4.199(cmPort: 62182 httpPort: 8042) registered with capability: <memory:8192, vCores:8>, assigned nodeId 10.1.4.199:62182
2017-01-18 11:30:58,253 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: 10.1.4.199:62182 Node Transitioned from NEW to RUNNING
2017-01-18 11:30:58,258 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node 10.1.4.199:62182 clusterResource: <memory:8192, vCores:8>
2017-01-18 11:35:02,498 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 1
2017-01-18 11:35:03,402 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 1 submitted by user lybuestc
2017-01-18 11:35:03,402 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0001
2017-01-18 11:35:03,403 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0001
2017-01-18 11:35:03,410 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0001 State change from NEW to NEW_SAVING
2017-01-18 11:35:03,410 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0001
2017-01-18 11:35:03,411 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0001 State change from NEW_SAVING to SUBMITTED
2017-01-18 11:35:03,413 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0001 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 11:35:03,414 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0001 from user: lybuestc, in queue: default
2017-01-18 11:35:03,430 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0001 State change from SUBMITTED to ACCEPTED
2017-01-18 11:35:03,456 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0001_000001
2017-01-18 11:35:03,457 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0001_000001 State change from NEW to SUBMITTED
2017-01-18 11:35:03,474 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 11:35:03,474 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 11:35:03,475 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0001 from user: lybuestc activated in queue: default
2017-01-18 11:35:03,475 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0001 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@72ca8717, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 11:35:03,475 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0001_000001 to scheduler from user lybuestc in queue default
2017-01-18 11:35:03,478 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0001_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 11:35:04,444 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 11:35:04,445 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0001	CONTAINERID=container_1484710253544_0001_01_000001
2017-01-18 11:35:04,445 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0001_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 11:35:04,445 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0001_000001 container=Container: [ContainerId: container_1484710253544_0001_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 11:35:04,446 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 11:35:04,446 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 11:35:04,462 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0001_01_000001
2017-01-18 11:35:04,474 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 11:35:04,475 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0001_000001
2017-01-18 11:35:04,479 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0001 AttemptId: appattempt_1484710253544_0001_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0001_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 11:35:04,496 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 11:35:04,498 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 11:35:04,502 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0001_000001
2017-01-18 11:35:04,546 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0001_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0001_000001
2017-01-18 11:35:04,546 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0001_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 11:35:04,548 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0001_000001
2017-01-18 11:35:04,551 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0001_000001
2017-01-18 11:35:05,119 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484710253544_0001_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0001_000001
2017-01-18 11:35:05,119 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0001_000001 State change from ALLOCATED to LAUNCHED
2017-01-18 11:35:05,446 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 11:35:11,605 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484710253544_0001_000001 (auth:SIMPLE)
2017-01-18 11:35:11,618 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484710253544_0001_000001
2017-01-18 11:35:11,619 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484710253544_0001	APPATTEMPTID=appattempt_1484710253544_0001_000001
2017-01-18 11:35:11,619 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0001_000001 State change from LAUNCHED to RUNNING
2017-01-18 11:35:11,619 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0001 State change from ACCEPTED to RUNNING
2017-01-18 11:35:13,475 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000002 Container Transitioned from NEW to ALLOCATED
2017-01-18 11:35:13,475 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0001	CONTAINERID=container_1484710253544_0001_01_000002
2017-01-18 11:35:13,475 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0001_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 11:35:13,475 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0001_000001 container=Container: [ContainerId: container_1484710253544_0001_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=NODE_LOCAL
2017-01-18 11:35:13,476 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 11:35:13,476 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 11:35:13,696 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0001_01_000002
2017-01-18 11:35:13,698 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 11:35:14,481 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 11:35:14,718 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0001
2017-01-18 11:35:16,995 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000002 Container Transitioned from RUNNING to COMPLETED
2017-01-18 11:35:16,995 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0001_01_000002 in state: COMPLETED event:FINISHED
2017-01-18 11:35:16,995 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0001	CONTAINERID=container_1484710253544_0001_01_000002
2017-01-18 11:35:16,995 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0001_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 11:35:16,995 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 11:35:16,996 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0001_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 11:35:16,996 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 11:35:16,997 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 11:35:16,997 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0001_000001 released container container_1484710253544_0001_01_000002 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 11:35:19,005 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000003 Container Transitioned from NEW to ALLOCATED
2017-01-18 11:35:19,005 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0001	CONTAINERID=container_1484710253544_0001_01_000003
2017-01-18 11:35:19,005 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0001_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 11:35:19,006 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0001_000001 container=Container: [ContainerId: container_1484710253544_0001_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 11:35:19,006 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 11:35:19,006 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 11:35:19,760 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 11:35:20,012 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 11:35:20,766 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0001
2017-01-18 11:35:22,717 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000003 Container Transitioned from RUNNING to COMPLETED
2017-01-18 11:35:22,718 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0001_01_000003 in state: COMPLETED event:FINISHED
2017-01-18 11:35:22,718 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0001	CONTAINERID=container_1484710253544_0001_01_000003
2017-01-18 11:35:22,718 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0001_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 11:35:22,718 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 11:35:22,719 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0001_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 11:35:22,719 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 11:35:22,719 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 11:35:22,719 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0001_000001 released container container_1484710253544_0001_01_000003 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 11:35:22,904 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0001_000001 with final state: FINISHING, and exit status: -1000
2017-01-18 11:35:22,905 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0001_000001 State change from RUNNING to FINAL_SAVING
2017-01-18 11:35:22,905 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0001 with final state: FINISHING
2017-01-18 11:35:22,906 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0001 State change from RUNNING to FINAL_SAVING
2017-01-18 11:35:22,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0001
2017-01-18 11:35:22,906 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0001_000001 State change from FINAL_SAVING to FINISHING
2017-01-18 11:35:22,906 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0001 State change from FINAL_SAVING to FINISHING
2017-01-18 11:35:23,908 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484710253544_0001 unregistered successfully. 
2017-01-18 11:35:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0001_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-18 11:35:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0001_01_000001 in state: COMPLETED event:FINISHED
2017-01-18 11:35:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0001_000001
2017-01-18 11:35:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0001	CONTAINERID=container_1484710253544_0001_01_000001
2017-01-18 11:35:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0001_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 11:35:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 11:35:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0001_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 11:35:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 11:35:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 11:35:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0001_000001 released container container_1484710253544_0001_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-18 11:35:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0001_000001
2017-01-18 11:35:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0001_000001 State change from FINISHING to FINISHED
2017-01-18 11:35:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0001 State change from FINISHING to FINISHED
2017-01-18 11:35:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0001_000001 is done. finalState=FINISHED
2017-01-18 11:35:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0001 requests cleared
2017-01-18 11:35:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0001 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 11:35:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0001_000001
2017-01-18 11:35:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0001 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 11:35:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0001
2017-01-18 11:35:28,986 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0001,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484710253544_0001/,appMasterHost=10.1.4.199,startTime=1484710503401,finishTime=1484710522905,finalStatus=SUCCEEDED,memorySeconds=57649,vcoreSeconds=30,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 11:40:53,473 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Release request cache is cleaned up
2017-01-18 11:43:33,317 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0001 which is the app master GUI of application_1484710253544_0001 owned by lybuestc
2017-01-18 11:43:35,887 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0001 which is the app master GUI of application_1484710253544_0001 owned by lybuestc
2017-01-18 11:47:23,550 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0001 which is the app master GUI of application_1484710253544_0001 owned by lybuestc
2017-01-18 11:47:25,625 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0001 which is the app master GUI of application_1484710253544_0001 owned by lybuestc
2017-01-18 11:47:26,682 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0001 which is the app master GUI of application_1484710253544_0001 owned by lybuestc
2017-01-18 11:54:24,345 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 2
2017-01-18 11:54:25,639 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 2 submitted by user lybuestc
2017-01-18 11:54:25,639 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0002
2017-01-18 11:54:25,639 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0002
2017-01-18 11:54:25,639 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0002 State change from NEW to NEW_SAVING
2017-01-18 11:54:25,641 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0002
2017-01-18 11:54:25,641 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0002 State change from NEW_SAVING to SUBMITTED
2017-01-18 11:54:25,642 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0002 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 11:54:25,642 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0002 from user: lybuestc, in queue: default
2017-01-18 11:54:25,647 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0002 State change from SUBMITTED to ACCEPTED
2017-01-18 11:54:25,647 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0002_000001
2017-01-18 11:54:25,647 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0002_000001 State change from NEW to SUBMITTED
2017-01-18 11:54:25,647 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 11:54:25,647 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 11:54:25,647 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0002 from user: lybuestc activated in queue: default
2017-01-18 11:54:25,648 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0002 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@1ea4ffff, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 11:54:25,648 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0002_000001 to scheduler from user lybuestc in queue default
2017-01-18 11:54:25,649 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0002_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 11:54:26,567 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0002_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 11:54:26,567 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0002	CONTAINERID=container_1484710253544_0002_01_000001
2017-01-18 11:54:26,567 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0002_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 11:54:26,567 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0002_000001 container=Container: [ContainerId: container_1484710253544_0002_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 11:54:26,567 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 11:54:26,568 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 11:54:26,571 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0002_01_000001
2017-01-18 11:54:26,574 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0002_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 11:54:26,574 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0002_000001
2017-01-18 11:54:26,574 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0002 AttemptId: appattempt_1484710253544_0002_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0002_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 11:54:26,574 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0002_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 11:54:26,574 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0002_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 11:54:26,575 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0002_000001
2017-01-18 11:54:26,580 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0002_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0002_000001
2017-01-18 11:54:26,580 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0002_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 11:54:26,580 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0002_000001
2017-01-18 11:54:26,580 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0002_000001
2017-01-18 11:56:00,178 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0001 which is the app master GUI of application_1484710253544_0001 owned by lybuestc
2017-01-18 11:56:00,981 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0001 which is the app master GUI of application_1484710253544_0001 owned by lybuestc
2017-01-18 11:56:38,943 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0002 State change from ACCEPTED to KILLING
2017-01-18 11:56:38,944 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0002_000001 with final state: KILLED, and exit status: -1000
2017-01-18 11:56:38,944 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0002_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-18 11:56:38,944 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0002_000001
2017-01-18 11:56:38,944 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0002_000001
2017-01-18 11:56:38,944 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0002_000001 State change from FINAL_SAVING to KILLED
2017-01-18 11:56:38,944 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0002 with final state: KILLED
2017-01-18 11:56:38,944 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0002 State change from KILLING to FINAL_SAVING
2017-01-18 11:56:38,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0002
2017-01-18 11:56:38,944 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0002_000001 is done. finalState=KILLED
2017-01-18 11:56:38,945 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0002 State change from FINAL_SAVING to KILLED
2017-01-18 11:56:38,945 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Killed	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0002
2017-01-18 11:56:38,945 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0002_000001
2017-01-18 11:56:38,945 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0002_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 11:56:38,945 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0002_01_000001 in state: KILLED event:KILL
2017-01-18 11:56:38,945 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0002	CONTAINERID=container_1484710253544_0002_01_000001
2017-01-18 11:56:38,945 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0002_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 11:56:38,945 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 11:56:38,945 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0002_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 11:56:38,946 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 11:56:38,946 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 11:56:38,946 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0002_000001 released container container_1484710253544_0002_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 11:56:38,946 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0002 requests cleared
2017-01-18 11:56:38,946 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0002 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 11:56:38,946 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0002 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 11:56:38,946 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0002,name=word count mapreduce demo,user=lybuestc,queue=default,state=KILLED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0002,appMasterHost=N/A,startTime=1484711665639,finishTime=1484711798944,finalStatus=KILLED,memorySeconds=0,vcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 11:59:01,368 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 3
2017-01-18 11:59:02,183 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 3 submitted by user lybuestc
2017-01-18 11:59:02,183 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0003
2017-01-18 11:59:02,183 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0003
2017-01-18 11:59:02,183 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0003 State change from NEW to NEW_SAVING
2017-01-18 11:59:02,183 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0003
2017-01-18 11:59:02,184 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0003 State change from NEW_SAVING to SUBMITTED
2017-01-18 11:59:02,184 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0003 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 11:59:02,184 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0003 from user: lybuestc, in queue: default
2017-01-18 11:59:02,184 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0003 State change from SUBMITTED to ACCEPTED
2017-01-18 11:59:02,184 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0003_000001
2017-01-18 11:59:02,184 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0003_000001 State change from NEW to SUBMITTED
2017-01-18 11:59:02,184 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 11:59:02,184 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 11:59:02,184 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0003 from user: lybuestc activated in queue: default
2017-01-18 11:59:02,184 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0003 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@39f17034, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 11:59:02,184 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0003_000001 to scheduler from user lybuestc in queue default
2017-01-18 11:59:02,185 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0003_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 11:59:02,548 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0003_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 11:59:02,548 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0003	CONTAINERID=container_1484710253544_0003_01_000001
2017-01-18 11:59:02,548 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0003_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 11:59:02,548 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0003_000001 container=Container: [ContainerId: container_1484710253544_0003_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 11:59:02,548 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 11:59:02,548 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 11:59:02,549 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0003_01_000001
2017-01-18 11:59:02,554 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0003_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 11:59:02,554 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0003_000001
2017-01-18 11:59:02,554 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0003 AttemptId: appattempt_1484710253544_0003_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0003_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 11:59:02,554 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0003_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 11:59:02,554 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0003_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 11:59:02,555 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0003_000001
2017-01-18 11:59:02,557 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0003_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0003_000001
2017-01-18 11:59:02,557 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0003_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 11:59:02,558 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0003_000001
2017-01-18 11:59:02,558 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0003_000001
2017-01-18 12:01:55,479 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 4
2017-01-18 12:01:56,166 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 4 submitted by user lybuestc
2017-01-18 12:01:56,166 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0004
2017-01-18 12:01:56,166 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0004
2017-01-18 12:01:56,166 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0004 State change from NEW to NEW_SAVING
2017-01-18 12:01:56,166 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0004
2017-01-18 12:01:56,166 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0004 State change from NEW_SAVING to SUBMITTED
2017-01-18 12:01:56,166 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0004 user: lybuestc leaf-queue of parent: root #applications: 2
2017-01-18 12:01:56,166 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0004 from user: lybuestc, in queue: default
2017-01-18 12:01:56,167 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0004 State change from SUBMITTED to ACCEPTED
2017-01-18 12:01:56,167 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0004_000001
2017-01-18 12:01:56,167 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0004_000001 State change from NEW to SUBMITTED
2017-01-18 12:01:56,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 12:01:56,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0004 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@39f17034, leaf-queue: default #user-pending-applications: 1 #user-active-applications: 1 #queue-pending-applications: 1 #queue-active-applications: 1
2017-01-18 12:01:56,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0004_000001 to scheduler from user lybuestc in queue default
2017-01-18 12:01:56,168 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0004_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 12:02:13,228 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0003 State change from ACCEPTED to KILLING
2017-01-18 12:02:13,228 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0003_000001 with final state: KILLED, and exit status: -1000
2017-01-18 12:02:13,228 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0003_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0003_000001
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0003_000001
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0003_000001 State change from FINAL_SAVING to KILLED
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0003 with final state: KILLED
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0003 State change from KILLING to FINAL_SAVING
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0003
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0003_000001 is done. finalState=KILLED
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0003_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0003_01_000001 in state: KILLED event:KILL
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0003	CONTAINERID=container_1484710253544_0003_01_000001
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0003 State change from FINAL_SAVING to KILLED
2017-01-18 12:02:13,230 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Killed	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0003
2017-01-18 12:02:13,230 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0003,name=word count,user=lybuestc,queue=default,state=KILLED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0003,appMasterHost=N/A,startTime=1484711942183,finishTime=1484712133229,finalStatus=KILLED,memorySeconds=390514,vcoreSeconds=190,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 12:02:13,229 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0003_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 12:02:13,230 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 12:02:13,230 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0003_000001
2017-01-18 12:02:13,230 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0003_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 12:02:13,234 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 12:02:13,234 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0
2017-01-18 12:02:13,234 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0003_000001 released container container_1484710253544_0003_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 12:02:13,234 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0003 requests cleared
2017-01-18 12:02:13,234 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:02:13,234 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:02:13,234 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0004 from user: lybuestc activated in queue: default
2017-01-18 12:02:13,234 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0003 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 12:02:13,234 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0003 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 12:02:13,263 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0004_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 12:02:13,264 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0004	CONTAINERID=container_1484710253544_0004_01_000001
2017-01-18 12:02:13,264 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0004_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 12:02:13,264 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0004_000001 container=Container: [ContainerId: container_1484710253544_0004_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 12:02:13,264 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 12:02:13,264 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 12:02:13,265 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0004_01_000001
2017-01-18 12:02:13,267 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0004_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 12:02:13,267 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0004_000001
2017-01-18 12:02:13,267 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0004 AttemptId: appattempt_1484710253544_0004_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0004_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 12:02:13,267 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0004_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 12:02:13,267 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0004_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 12:02:13,268 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0004_000001
2017-01-18 12:02:13,269 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0004_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0004_000001
2017-01-18 12:02:13,270 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0004_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 12:02:13,270 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0004_000001
2017-01-18 12:02:13,270 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0004_000001
2017-01-18 12:03:25,790 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 5
2017-01-18 12:03:26,463 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 5 submitted by user lybuestc
2017-01-18 12:03:26,463 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0005
2017-01-18 12:03:26,463 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0005
2017-01-18 12:03:26,464 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0005 State change from NEW to NEW_SAVING
2017-01-18 12:03:26,464 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0005
2017-01-18 12:03:26,465 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0005 State change from NEW_SAVING to SUBMITTED
2017-01-18 12:03:26,465 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0005 user: lybuestc leaf-queue of parent: root #applications: 2
2017-01-18 12:03:26,465 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0005 from user: lybuestc, in queue: default
2017-01-18 12:03:26,465 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0005 State change from SUBMITTED to ACCEPTED
2017-01-18 12:03:26,465 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0005_000001
2017-01-18 12:03:26,466 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0005_000001 State change from NEW to SUBMITTED
2017-01-18 12:03:26,466 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 12:03:26,466 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0005 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@39f17034, leaf-queue: default #user-pending-applications: 1 #user-active-applications: 1 #queue-pending-applications: 1 #queue-active-applications: 1
2017-01-18 12:03:26,466 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0005_000001 to scheduler from user lybuestc in queue default
2017-01-18 12:03:26,467 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0005_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 12:03:47,817 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0002_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 12:05:04,083 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0005 State change from ACCEPTED to KILLING
2017-01-18 12:05:04,084 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0005_000001 with final state: KILLED, and exit status: -1000
2017-01-18 12:05:04,084 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0005_000001 State change from SCHEDULED to FINAL_SAVING
2017-01-18 12:05:04,084 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0005_000001
2017-01-18 12:05:04,084 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0005_000001
2017-01-18 12:05:04,084 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0005_000001 State change from FINAL_SAVING to KILLED
2017-01-18 12:05:04,086 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0005 with final state: KILLED
2017-01-18 12:05:04,086 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0005 State change from KILLING to FINAL_SAVING
2017-01-18 12:05:04,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0005
2017-01-18 12:05:04,086 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0005_000001 is done. finalState=KILLED
2017-01-18 12:05:04,089 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0005 requests cleared
2017-01-18 12:05:04,089 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0005 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 12:05:04,089 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0005 State change from FINAL_SAVING to KILLED
2017-01-18 12:05:04,090 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Killed	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0005
2017-01-18 12:05:04,090 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0005,name=word count,user=lybuestc,queue=default,state=KILLED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0005,appMasterHost=N/A,startTime=1484712206463,finishTime=1484712304086,finalStatus=KILLED,memorySeconds=0,vcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 12:05:04,090 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0005 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 12:05:12,395 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0004 State change from ACCEPTED to KILLING
2017-01-18 12:05:12,397 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0004_000001 with final state: KILLED, and exit status: -1000
2017-01-18 12:05:12,397 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0004_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-18 12:05:12,399 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0004_000001
2017-01-18 12:05:12,399 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0004_000001
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0004_000001 State change from FINAL_SAVING to KILLED
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0004 with final state: KILLED
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0004 State change from KILLING to FINAL_SAVING
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0004
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0004_000001 is done. finalState=KILLED
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0004_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0004_01_000001 in state: KILLED event:KILL
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0004	CONTAINERID=container_1484710253544_0004_01_000001
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0004_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0004_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 12:05:12,400 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 12:05:12,401 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 12:05:12,401 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0004_000001 released container container_1484710253544_0004_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 12:05:12,401 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0004 requests cleared
2017-01-18 12:05:12,401 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0004 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 12:05:12,401 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0004 State change from FINAL_SAVING to KILLED
2017-01-18 12:05:12,402 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Killed	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0004
2017-01-18 12:05:12,403 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0004,name=word count,user=lybuestc,queue=default,state=KILLED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0004,appMasterHost=N/A,startTime=1484712116166,finishTime=1484712312400,finalStatus=KILLED,memorySeconds=366872,vcoreSeconds=179,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 12:05:12,402 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0004_000001
2017-01-18 12:05:12,405 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0004 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 12:05:50,771 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error cleaning master 
java.net.ConnectException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:110)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.cleanup(AMLauncher.java:138)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:264)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
2017-01-18 12:08:07,845 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0003_000001. Got exception: java.net.ConnectException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 12:08:57,743 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0001 which is the app master GUI of application_1484710253544_0001 owned by lybuestc
2017-01-18 12:11:04,798 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0004_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 12:11:33,411 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error cleaning master 
org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:110)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.cleanup(AMLauncher.java:138)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:264)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
2017-01-18 12:11:39,097 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 6
2017-01-18 12:11:39,859 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 6 submitted by user lybuestc
2017-01-18 12:11:39,859 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0006
2017-01-18 12:11:39,859 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0006
2017-01-18 12:11:39,859 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0006 State change from NEW to NEW_SAVING
2017-01-18 12:11:39,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0006
2017-01-18 12:11:39,859 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0006 State change from NEW_SAVING to SUBMITTED
2017-01-18 12:11:39,859 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0006 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 12:11:39,859 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0006 from user: lybuestc, in queue: default
2017-01-18 12:11:39,866 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0006 State change from SUBMITTED to ACCEPTED
2017-01-18 12:11:39,866 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0006_000001
2017-01-18 12:11:39,866 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000001 State change from NEW to SUBMITTED
2017-01-18 12:11:39,866 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:11:39,866 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:11:39,866 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0006 from user: lybuestc activated in queue: default
2017-01-18 12:11:39,866 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0006 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@6d188ed8, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 12:11:39,866 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0006_000001 to scheduler from user lybuestc in queue default
2017-01-18 12:11:39,867 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 12:11:40,263 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0006_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 12:11:40,264 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0006	CONTAINERID=container_1484710253544_0006_01_000001
2017-01-18 12:11:40,264 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0006_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 12:11:40,264 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0006_000001 container=Container: [ContainerId: container_1484710253544_0006_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 12:11:40,264 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 12:11:40,264 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 12:11:40,265 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0006_01_000001
2017-01-18 12:11:40,266 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0006_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 12:11:40,266 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0006_000001
2017-01-18 12:11:40,266 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0006 AttemptId: appattempt_1484710253544_0006_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0006_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 12:11:40,266 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 12:11:40,266 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 12:11:40,267 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0006_000001
2017-01-18 12:11:40,269 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0006_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0006_000001
2017-01-18 12:11:40,270 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0006_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 12:11:40,270 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0006_000001
2017-01-18 12:11:40,270 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0006_000001
2017-01-18 12:14:32,663 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error cleaning master 
org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:110)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.cleanup(AMLauncher.java:138)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:264)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
2017-01-18 12:15:54,482 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0001 which is the app master GUI of application_1484710253544_0001 owned by lybuestc
2017-01-18 12:18:23,416 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 7
2017-01-18 12:18:24,210 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 7 submitted by user lybuestc
2017-01-18 12:18:24,210 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0007
2017-01-18 12:18:24,210 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0007
2017-01-18 12:18:24,211 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0007 State change from NEW to NEW_SAVING
2017-01-18 12:18:24,211 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0007
2017-01-18 12:18:24,211 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0007 State change from NEW_SAVING to SUBMITTED
2017-01-18 12:18:24,211 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0007 user: lybuestc leaf-queue of parent: root #applications: 2
2017-01-18 12:18:24,211 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0007 from user: lybuestc, in queue: default
2017-01-18 12:18:24,211 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0007 State change from SUBMITTED to ACCEPTED
2017-01-18 12:18:24,212 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0007_000001
2017-01-18 12:18:24,212 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0007_000001 State change from NEW to SUBMITTED
2017-01-18 12:18:24,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 12:18:24,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0007 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@6d188ed8, leaf-queue: default #user-pending-applications: 1 #user-active-applications: 1 #queue-pending-applications: 1 #queue-active-applications: 1
2017-01-18 12:18:24,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0007_000001 to scheduler from user lybuestc in queue default
2017-01-18 12:18:24,213 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0007_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 12:21:00,507 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0006_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 12:21:00,508 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0006_000001 with final state: FAILED, and exit status: -1000
2017-01-18 12:21:00,508 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-18 12:21:00,508 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0006_000001
2017-01-18 12:21:00,508 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0006_000001
2017-01-18 12:21:00,508 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000001 State change from FINAL_SAVING to FAILED
2017-01-18 12:21:00,508 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2017-01-18 12:21:00,508 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0006_000002
2017-01-18 12:21:00,508 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0006_000001 is done. finalState=FAILED
2017-01-18 12:21:00,508 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000002 State change from NEW to SUBMITTED
2017-01-18 12:21:00,508 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0006_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 12:21:00,508 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0006_01_000001 in state: KILLED event:KILL
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0006	CONTAINERID=container_1484710253544_0006_01_000001
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0006_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0006_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0006_000001 released container container_1484710253544_0006_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0006 requests cleared
2017-01-18 12:21:00,509 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:21:00,509 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0007 from user: lybuestc activated in queue: default
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0006 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0006 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@6d188ed8, leaf-queue: default #user-pending-applications: 1 #user-active-applications: 1 #queue-pending-applications: 1 #queue-active-applications: 1
2017-01-18 12:21:00,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0006_000002 to scheduler from user lybuestc in queue default
2017-01-18 12:21:00,510 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000002 State change from SUBMITTED to SCHEDULED
2017-01-18 12:21:01,275 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0007_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 12:21:01,275 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0007	CONTAINERID=container_1484710253544_0007_01_000001
2017-01-18 12:21:01,276 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0007_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 12:21:01,276 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0007_000001 container=Container: [ContainerId: container_1484710253544_0007_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 12:21:01,276 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=2, numContainers=1
2017-01-18 12:21:01,276 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 12:21:01,281 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0007_01_000001
2017-01-18 12:21:01,282 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0007_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 12:21:01,282 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0007_000001
2017-01-18 12:21:01,282 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0007 AttemptId: appattempt_1484710253544_0007_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0007_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 12:21:01,282 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0007_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 12:21:01,282 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0007_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 12:21:01,283 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0007_000001
2017-01-18 12:21:01,284 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0007_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0007_000001
2017-01-18 12:21:01,284 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0007_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 12:21:01,284 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0007_000001
2017-01-18 12:21:01,284 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0007_000001
2017-01-18 12:22:08,110 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 8
2017-01-18 12:22:08,814 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 8 submitted by user lybuestc
2017-01-18 12:22:08,814 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0008
2017-01-18 12:22:08,814 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0008
2017-01-18 12:22:08,814 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0008 State change from NEW to NEW_SAVING
2017-01-18 12:22:08,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0008
2017-01-18 12:22:08,814 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0008 State change from NEW_SAVING to SUBMITTED
2017-01-18 12:22:08,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0008 user: lybuestc leaf-queue of parent: root #applications: 3
2017-01-18 12:22:08,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0008 from user: lybuestc, in queue: default
2017-01-18 12:22:08,815 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0008 State change from SUBMITTED to ACCEPTED
2017-01-18 12:22:08,815 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0008_000001
2017-01-18 12:22:08,815 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0008_000001 State change from NEW to SUBMITTED
2017-01-18 12:22:08,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 12:22:08,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 12:22:08,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0008 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@6d188ed8, leaf-queue: default #user-pending-applications: 2 #user-active-applications: 1 #queue-pending-applications: 2 #queue-active-applications: 1
2017-01-18 12:22:08,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0008_000001 to scheduler from user lybuestc in queue default
2017-01-18 12:22:08,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0008_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 12:22:31,833 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0008 State change from ACCEPTED to KILLING
2017-01-18 12:22:31,833 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0008_000001 with final state: KILLED, and exit status: -1000
2017-01-18 12:22:31,833 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0008_000001 State change from SCHEDULED to FINAL_SAVING
2017-01-18 12:22:31,833 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0008_000001
2017-01-18 12:22:31,833 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0008_000001
2017-01-18 12:22:31,835 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0008_000001 State change from FINAL_SAVING to KILLED
2017-01-18 12:22:31,835 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0008 with final state: KILLED
2017-01-18 12:22:31,835 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0008 State change from KILLING to FINAL_SAVING
2017-01-18 12:22:31,835 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0008
2017-01-18 12:22:31,835 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0008_000001 is done. finalState=KILLED
2017-01-18 12:22:31,835 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0008 requests cleared
2017-01-18 12:22:31,835 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0008 State change from FINAL_SAVING to KILLED
2017-01-18 12:22:31,835 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 12:22:31,835 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0008 user: lybuestc queue: default #user-pending-applications: 1 #user-active-applications: 1 #queue-pending-applications: 1 #queue-active-applications: 1
2017-01-18 12:22:31,835 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0008 user: lybuestc leaf-queue of parent: root #applications: 2
2017-01-18 12:22:31,835 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Killed	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0008
2017-01-18 12:22:31,836 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0008,name=word count,user=lybuestc,queue=default,state=KILLED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0008,appMasterHost=N/A,startTime=1484713328814,finishTime=1484713351835,finalStatus=KILLED,memorySeconds=0,vcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 12:22:46,768 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 9
2017-01-18 12:22:47,898 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 9 submitted by user lybuestc
2017-01-18 12:22:47,898 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0009
2017-01-18 12:22:47,898 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0009
2017-01-18 12:22:47,898 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0009 State change from NEW to NEW_SAVING
2017-01-18 12:22:47,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0009
2017-01-18 12:22:47,899 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0009 State change from NEW_SAVING to SUBMITTED
2017-01-18 12:22:47,899 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0009 user: lybuestc leaf-queue of parent: root #applications: 3
2017-01-18 12:22:47,899 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0009 from user: lybuestc, in queue: default
2017-01-18 12:22:47,899 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0009 State change from SUBMITTED to ACCEPTED
2017-01-18 12:22:47,899 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0009_000001
2017-01-18 12:22:47,899 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0009_000001 State change from NEW to SUBMITTED
2017-01-18 12:22:47,899 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 12:22:47,899 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 12:22:47,899 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0009 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@6d188ed8, leaf-queue: default #user-pending-applications: 2 #user-active-applications: 1 #queue-pending-applications: 2 #queue-active-applications: 1
2017-01-18 12:22:47,899 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0009_000001 to scheduler from user lybuestc in queue default
2017-01-18 12:22:47,900 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0009_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 12:23:15,848 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0007 State change from ACCEPTED to KILLING
2017-01-18 12:23:15,848 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0007_000001 with final state: KILLED, and exit status: -1000
2017-01-18 12:23:15,848 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0007_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-18 12:23:15,848 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0007_000001
2017-01-18 12:23:15,848 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0007_000001
2017-01-18 12:23:15,848 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0007_000001 State change from FINAL_SAVING to KILLED
2017-01-18 12:23:15,848 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0007 with final state: KILLED
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0007 State change from KILLING to FINAL_SAVING
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0007
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0007_000001 is done. finalState=KILLED
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0007 State change from FINAL_SAVING to KILLED
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Killed	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0007
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0007_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0007_01_000001 in state: KILLED event:KILL
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0007	CONTAINERID=container_1484710253544_0007_01_000001
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0007_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0007_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=3, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=3, numContainers=0
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0007_000001 released container container_1484710253544_0007_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 12:23:15,849 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0007 requests cleared
2017-01-18 12:23:15,849 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:23:15,849 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:23:15,850 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0006 from user: lybuestc activated in queue: default
2017-01-18 12:23:15,850 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 12:23:15,850 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0007 user: lybuestc queue: default #user-pending-applications: 1 #user-active-applications: 1 #queue-pending-applications: 1 #queue-active-applications: 1
2017-01-18 12:23:15,850 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0007 user: lybuestc leaf-queue of parent: root #applications: 2
2017-01-18 12:23:15,850 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0007,name=word count,user=lybuestc,queue=default,state=KILLED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0007,appMasterHost=N/A,startTime=1484713104210,finishTime=1484713395848,finalStatus=KILLED,memorySeconds=0,vcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 12:23:15,851 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0007_000001
2017-01-18 12:23:16,717 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0006_02_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 12:23:16,718 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0006	CONTAINERID=container_1484710253544_0006_02_000001
2017-01-18 12:23:16,718 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0006_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 12:23:16,718 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0006_000002 container=Container: [ContainerId: container_1484710253544_0006_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 12:23:16,718 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=2, numContainers=1
2017-01-18 12:23:16,718 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 12:23:16,719 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0006_02_000001
2017-01-18 12:23:16,720 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0006_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 12:23:16,720 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0006_000002
2017-01-18 12:23:16,720 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0006 AttemptId: appattempt_1484710253544_0006_000002 MasterContainer: Container: [ContainerId: container_1484710253544_0006_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 12:23:16,720 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000002 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 12:23:16,720 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000002 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 12:23:16,721 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0006_000002
2017-01-18 12:23:16,723 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0006_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0006_000002
2017-01-18 12:23:16,723 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0006_02_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 12:23:16,723 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0006_000002
2017-01-18 12:23:16,723 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0006_000002
2017-01-18 12:23:22,066 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0006 State change from ACCEPTED to KILLING
2017-01-18 12:23:22,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0006_000002 with final state: KILLED, and exit status: -1000
2017-01-18 12:23:22,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000002 State change from ALLOCATED to FINAL_SAVING
2017-01-18 12:23:22,067 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0006_000002
2017-01-18 12:23:22,067 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0006_000002
2017-01-18 12:23:22,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0006_000002 State change from FINAL_SAVING to KILLED
2017-01-18 12:23:22,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0006 with final state: KILLED
2017-01-18 12:23:22,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0006 State change from KILLING to FINAL_SAVING
2017-01-18 12:23:22,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0006
2017-01-18 12:23:22,067 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0006_000002 is done. finalState=KILLED
2017-01-18 12:23:22,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0006_02_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 12:23:22,067 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0006_02_000001 in state: KILLED event:KILL
2017-01-18 12:23:22,068 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0006	CONTAINERID=container_1484710253544_0006_02_000001
2017-01-18 12:23:22,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0006_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 12:23:22,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 12:23:22,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0006_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 12:23:22,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 12:23:22,068 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0006 State change from FINAL_SAVING to KILLED
2017-01-18 12:23:22,071 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0006_000002
2017-01-18 12:23:22,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0
2017-01-18 12:23:22,071 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0006_000002 released container container_1484710253544_0006_02_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 12:23:22,071 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0006 requests cleared
2017-01-18 12:23:22,071 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:23:22,071 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:23:22,071 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0009 from user: lybuestc activated in queue: default
2017-01-18 12:23:22,071 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0006 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 12:23:22,071 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0006 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 12:23:22,071 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Killed	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0006
2017-01-18 12:23:22,071 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0006,name=word count,user=lybuestc,queue=default,state=KILLED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0006,appMasterHost=N/A,startTime=1484712699859,finishTime=1484713402067,finalStatus=KILLED,memorySeconds=1158337,vcoreSeconds=565,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 12:23:22,740 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0009_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 12:23:22,740 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0009	CONTAINERID=container_1484710253544_0009_01_000001
2017-01-18 12:23:22,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0009_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 12:23:22,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0009_000001 container=Container: [ContainerId: container_1484710253544_0009_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 12:23:22,741 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 12:23:22,741 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 12:23:22,741 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0009_01_000001
2017-01-18 12:23:22,742 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0009_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 12:23:22,742 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0009_000001
2017-01-18 12:23:22,742 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0009 AttemptId: appattempt_1484710253544_0009_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0009_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 12:23:22,742 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0009_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 12:23:22,742 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0009_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 12:23:22,743 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0009_000001
2017-01-18 12:23:22,744 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0009_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0009_000001
2017-01-18 12:23:22,745 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0009_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 12:23:22,745 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0009_000001
2017-01-18 12:23:22,745 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0009_000001
2017-01-18 12:29:51,490 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0009 State change from ACCEPTED to KILLING
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0009_000001 with final state: KILLED, and exit status: -1000
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0009_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0009_000001
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0009_000001
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0009_000001 State change from FINAL_SAVING to KILLED
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0009 with final state: KILLED
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0009 State change from KILLING to FINAL_SAVING
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0009
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0009_000001 is done. finalState=KILLED
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0009_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0009_01_000001 in state: KILLED event:KILL
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0009	CONTAINERID=container_1484710253544_0009_01_000001
2017-01-18 12:29:51,491 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0009_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 12:29:51,492 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 12:29:51,492 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0009 State change from FINAL_SAVING to KILLED
2017-01-18 12:29:51,492 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0009_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 12:29:51,492 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 12:29:51,492 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 12:29:51,492 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0009_000001 released container container_1484710253544_0009_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 12:29:51,492 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0009 requests cleared
2017-01-18 12:29:51,492 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Killed	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0009
2017-01-18 12:29:51,493 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0009,name=word count,user=lybuestc,queue=default,state=KILLED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0009,appMasterHost=N/A,startTime=1484713367898,finishTime=1484713791491,finalStatus=KILLED,memorySeconds=796162,vcoreSeconds=388,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 12:29:51,492 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0009_000001
2017-01-18 12:29:51,492 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0009 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 12:29:51,496 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0009 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 12:30:21,494 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0007_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 12:31:55,745 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 10
2017-01-18 12:31:56,581 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 10 submitted by user lybuestc
2017-01-18 12:31:56,581 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0010
2017-01-18 12:31:56,581 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0010
2017-01-18 12:31:56,582 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0010 State change from NEW to NEW_SAVING
2017-01-18 12:31:56,582 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0010
2017-01-18 12:31:56,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0010 State change from NEW_SAVING to SUBMITTED
2017-01-18 12:31:56,583 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0010 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 12:31:56,583 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0010 from user: lybuestc, in queue: default
2017-01-18 12:31:56,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0010 State change from SUBMITTED to ACCEPTED
2017-01-18 12:31:56,583 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0010_000001
2017-01-18 12:31:56,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000001 State change from NEW to SUBMITTED
2017-01-18 12:31:56,583 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:31:56,583 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:31:56,583 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0010 from user: lybuestc activated in queue: default
2017-01-18 12:31:56,583 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0010 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@3072e60, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 12:31:56,584 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0010_000001 to scheduler from user lybuestc in queue default
2017-01-18 12:31:56,584 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 12:31:57,473 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0010_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 12:31:57,473 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0010	CONTAINERID=container_1484710253544_0010_01_000001
2017-01-18 12:31:57,473 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0010_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 12:31:57,473 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0010_000001 container=Container: [ContainerId: container_1484710253544_0010_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 12:31:57,474 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 12:31:57,474 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 12:31:57,475 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0010_01_000001
2017-01-18 12:31:57,476 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0010_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 12:31:57,477 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0010_000001
2017-01-18 12:31:57,477 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0010 AttemptId: appattempt_1484710253544_0010_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0010_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 12:31:57,477 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 12:31:57,477 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 12:31:57,478 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0010_000001
2017-01-18 12:31:57,483 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0010_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0010_000001
2017-01-18 12:31:57,483 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0010_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 12:31:57,483 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0010_000001
2017-01-18 12:31:57,483 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0010_000001
2017-01-18 12:32:25,412 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error cleaning master 
org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:110)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.cleanup(AMLauncher.java:138)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:264)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
2017-01-18 12:32:36,101 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error cleaning master 
org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:110)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.cleanup(AMLauncher.java:138)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:264)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
2017-01-18 12:32:36,905 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0006_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 12:32:42,939 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0009_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 12:39:12,928 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error cleaning master 
org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.stopContainers(ContainerManagementProtocolPBClientImpl.java:110)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.stopContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.cleanup(AMLauncher.java:138)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:264)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
2017-01-18 12:41:17,738 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0010_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 12:41:17,739 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0010_000001 with final state: FAILED, and exit status: -1000
2017-01-18 12:41:17,739 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-18 12:41:17,739 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0010_000001
2017-01-18 12:41:17,739 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0010_000001
2017-01-18 12:41:17,739 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000001 State change from FINAL_SAVING to FAILED
2017-01-18 12:41:17,739 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2017-01-18 12:41:17,739 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0010_000002
2017-01-18 12:41:17,739 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0010_000001 is done. finalState=FAILED
2017-01-18 12:41:17,739 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000002 State change from NEW to SUBMITTED
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0010_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0010_01_000001 in state: KILLED event:KILL
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0010	CONTAINERID=container_1484710253544_0010_01_000001
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0010_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0010_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0010_000001 released container container_1484710253544_0010_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0010 requests cleared
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0010 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 12:41:17,740 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:41:17,740 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0010 from user: lybuestc activated in queue: default
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0010 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@1a6b9712, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 12:41:17,740 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0010_000002 to scheduler from user lybuestc in queue default
2017-01-18 12:41:17,741 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000002 State change from SUBMITTED to SCHEDULED
2017-01-18 12:41:18,512 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0010_02_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 12:41:18,512 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0010	CONTAINERID=container_1484710253544_0010_02_000001
2017-01-18 12:41:18,512 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0010_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 12:41:18,512 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0010_000002 container=Container: [ContainerId: container_1484710253544_0010_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 12:41:18,512 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 12:41:18,513 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 12:41:18,513 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0010_02_000001
2017-01-18 12:41:18,515 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0010_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 12:41:18,515 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0010_000002
2017-01-18 12:41:18,515 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0010 AttemptId: appattempt_1484710253544_0010_000002 MasterContainer: Container: [ContainerId: container_1484710253544_0010_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 12:41:18,516 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000002 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 12:41:18,516 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000002 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 12:41:18,516 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0010_000002
2017-01-18 12:41:18,517 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0010_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0010_000002
2017-01-18 12:41:18,517 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0010_02_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 12:41:18,517 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0010_000002
2017-01-18 12:41:18,517 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0010_000002
2017-01-18 12:50:38,815 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0010_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 12:50:38,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0010_000002 with final state: FAILED, and exit status: -1000
2017-01-18 12:50:38,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000002 State change from ALLOCATED to FINAL_SAVING
2017-01-18 12:50:38,816 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0010_000002
2017-01-18 12:50:38,816 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0010_000002
2017-01-18 12:50:38,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0010_000002 State change from FINAL_SAVING to FAILED
2017-01-18 12:50:38,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 2. The max attempts is 2
2017-01-18 12:50:38,817 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0010 with final state: FAILED
2017-01-18 12:50:38,817 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0010 State change from ACCEPTED to FINAL_SAVING
2017-01-18 12:50:38,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0010
2017-01-18 12:50:38,817 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0010_000002 is done. finalState=FAILED
2017-01-18 12:50:38,817 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Application application_1484710253544_0010 failed 2 times due to Error launching appattempt_1484710253544_0010_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.
2017-01-18 12:50:38,817 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0010_02_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 12:50:38,817 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0010 State change from FINAL_SAVING to FAILED
2017-01-18 12:50:38,817 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0010_02_000001 in state: KILLED event:KILL
2017-01-18 12:50:38,818 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0010	CONTAINERID=container_1484710253544_0010_02_000001
2017-01-18 12:50:38,818 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0010_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 12:50:38,818 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Failed	TARGET=RMAppManager	RESULT=FAILURE	DESCRIPTION=App failed with state: FAILED	PERMISSIONS=Application application_1484710253544_0010 failed 2 times due to Error launching appattempt_1484710253544_0010_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.	APPID=application_1484710253544_0010
2017-01-18 12:50:38,818 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 12:50:38,818 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0010_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 12:50:38,818 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0010,name=word count,user=lybuestc,queue=default,state=FAILED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0010,appMasterHost=N/A,startTime=1484713916581,finishTime=1484715038817,finalStatus=FAILED,memorySeconds=2294930,vcoreSeconds=1120,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 12:50:38,818 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 12:50:38,819 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 12:50:38,819 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0010_000002 released container container_1484710253544_0010_02_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 12:50:38,819 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0010 requests cleared
2017-01-18 12:50:38,819 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0010 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 12:50:38,819 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0010 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 13:50:28,025 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8031: readAndProcess from client 10.1.4.199 threw exception [java.io.IOException: Operation timed out]
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2017-01-18 13:51:45,622 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 11
2017-01-18 13:51:46,745 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 11 submitted by user lybuestc
2017-01-18 13:51:46,746 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0011
2017-01-18 13:51:46,746 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0011
2017-01-18 13:51:46,746 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0011 State change from NEW to NEW_SAVING
2017-01-18 13:51:46,746 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0011
2017-01-18 13:51:46,746 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0011 State change from NEW_SAVING to SUBMITTED
2017-01-18 13:51:46,746 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0011 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 13:51:46,747 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0011 from user: lybuestc, in queue: default
2017-01-18 13:51:46,766 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0011 State change from SUBMITTED to ACCEPTED
2017-01-18 13:51:46,766 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0011_000001
2017-01-18 13:51:46,766 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0011_000001 State change from NEW to SUBMITTED
2017-01-18 13:51:46,767 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 13:51:46,767 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 13:51:46,767 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0011 from user: lybuestc activated in queue: default
2017-01-18 13:51:46,767 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0011 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@282934bb, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 13:51:46,767 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0011_000001 to scheduler from user lybuestc in queue default
2017-01-18 13:51:46,769 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0011_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 13:51:47,212 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 13:51:47,212 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0011	CONTAINERID=container_1484710253544_0011_01_000001
2017-01-18 13:51:47,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0011_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 13:51:47,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0011_000001 container=Container: [ContainerId: container_1484710253544_0011_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 13:51:47,213 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 13:51:47,213 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0011_01_000001
2017-01-18 13:51:47,214 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 13:51:47,214 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 13:51:47,214 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0011_000001
2017-01-18 13:51:47,215 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0011 AttemptId: appattempt_1484710253544_0011_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0011_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 13:51:47,215 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0011_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 13:51:47,215 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0011_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 13:51:47,215 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0011_000001
2017-01-18 13:51:47,217 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0011_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0011_000001
2017-01-18 13:51:47,217 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0011_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 13:51:47,217 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0011_000001
2017-01-18 13:51:47,217 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0011_000001
2017-01-18 13:59:00,574 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484710253544_0011_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0011_000001
2017-01-18 13:59:00,574 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0011_000001 State change from ALLOCATED to LAUNCHED
2017-01-18 13:59:06,141 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484710253544_0011_000001 (auth:SIMPLE)
2017-01-18 13:59:06,146 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484710253544_0011_000001
2017-01-18 13:59:06,146 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484710253544_0011	APPATTEMPTID=appattempt_1484710253544_0011_000001
2017-01-18 13:59:06,146 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0011_000001 State change from LAUNCHED to RUNNING
2017-01-18 13:59:06,146 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0011 State change from ACCEPTED to RUNNING
2017-01-18 13:59:24,128 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0001 which is the app master GUI of application_1484710253544_0001 owned by lybuestc
2017-01-18 13:59:52,522 INFO logs: Aliases are enabled
2017-01-18 14:00:03,064 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000002 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:00:03,064 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0011	CONTAINERID=container_1484710253544_0011_01_000002
2017-01-18 14:00:03,064 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0011_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:00:03,064 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0011_000001 container=Container: [ContainerId: container_1484710253544_0011_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=NODE_LOCAL
2017-01-18 14:00:03,064 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:00:03,064 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:00:03,489 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0011_01_000002
2017-01-18 14:00:03,490 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:00:04,073 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:00:04,073 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000002 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:00:04,500 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0011
2017-01-18 14:00:06,303 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000002 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:00:06,303 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0011_01_000002 in state: COMPLETED event:FINISHED
2017-01-18 14:00:06,303 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0011	CONTAINERID=container_1484710253544_0011_01_000002
2017-01-18 14:00:06,303 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0011_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:00:06,303 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:00:06,304 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0011_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:00:06,304 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:00:06,304 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:00:06,304 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0011_000001 released container container_1484710253544_0011_01_000002 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:00:08,314 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000003 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:00:08,314 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0011	CONTAINERID=container_1484710253544_0011_01_000003
2017-01-18 14:00:08,314 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0011_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:00:08,314 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0011_000001 container=Container: [ContainerId: container_1484710253544_0011_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 14:00:08,315 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:00:08,315 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:00:08,529 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:00:09,317 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000003 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:00:09,542 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0011
2017-01-18 14:00:11,664 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000003 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:00:11,664 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0011_01_000003 in state: COMPLETED event:FINISHED
2017-01-18 14:00:11,664 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0011	CONTAINERID=container_1484710253544_0011_01_000003
2017-01-18 14:00:11,664 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0011_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:00:11,664 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:00:11,665 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0011_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:00:11,665 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:00:11,665 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:00:11,665 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0011_000001 released container container_1484710253544_0011_01_000003 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:00:11,812 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0011_000001 with final state: FINISHING, and exit status: -1000
2017-01-18 14:00:11,812 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0011_000001 State change from RUNNING to FINAL_SAVING
2017-01-18 14:00:11,812 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0011 with final state: FINISHING
2017-01-18 14:00:11,813 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0011 State change from RUNNING to FINAL_SAVING
2017-01-18 14:00:11,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0011
2017-01-18 14:00:11,813 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0011_000001 State change from FINAL_SAVING to FINISHING
2017-01-18 14:00:11,813 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0011 State change from FINAL_SAVING to FINISHING
2017-01-18 14:00:12,814 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484710253544_0011 unregistered successfully. 
2017-01-18 14:00:17,989 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0011_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:00:17,989 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0011_01_000001 in state: COMPLETED event:FINISHED
2017-01-18 14:00:17,989 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0011	CONTAINERID=container_1484710253544_0011_01_000001
2017-01-18 14:00:17,989 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0011_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 14:00:17,989 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 14:00:17,990 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0011_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 14:00:17,990 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 14:00:17,990 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 14:00:17,990 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0011_000001 released container container_1484710253544_0011_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-18 14:00:17,990 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0011_000001
2017-01-18 14:00:17,990 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0011_000001
2017-01-18 14:00:17,990 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0011_000001 State change from FINISHING to FINISHED
2017-01-18 14:00:17,990 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0011 State change from FINISHING to FINISHED
2017-01-18 14:00:17,990 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0011
2017-01-18 14:00:17,991 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0011_000001 is done. finalState=FINISHED
2017-01-18 14:00:17,991 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0011 requests cleared
2017-01-18 14:00:17,991 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0011 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 14:00:17,991 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0011 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 14:00:17,991 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0011,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484710253544_0011/,appMasterHost=10.1.4.199,startTime=1484718706745,finishTime=1484719211812,finalStatus=SUCCEEDED,memorySeconds=1052817,vcoreSeconds=516,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 14:00:17,991 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0011_000001
2017-01-18 14:00:20,000 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Null container completed...
2017-01-18 14:00:23,187 INFO org.apache.hadoop.http.HttpServer2: Process Thread Dump: jsp requested
216 active threads
Thread 565 (IPC Parameter Sending Thread #2):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
    java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
    java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 564 (IPC Client (1655565591) connection to /10.1.4.199:62182 from appattempt_1484710253544_0011_000001):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:933)
    org.apache.hadoop.ipc.Client$Connection.run(Client.java:978)
Thread 563 (ApplicationMasterLauncher #18):
  State: WAITING
  Blocked count: 1
  Waited count: 3
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 562 (2015394411@qtp-120689887-38):
  State: RUNNABLE
  Blocked count: 7
  Waited count: 7
  Stack:
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:178)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:139)
    org.apache.hadoop.util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:168)
    org.apache.hadoop.util.ReflectionUtils.logThreadInfo(ReflectionUtils.java:223)
    org.apache.hadoop.http.HttpServer2$StackServlet.doGet(HttpServer2.java:1114)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
    org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
    com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)
    com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)
    com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
    org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:172)
    com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
    com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
    com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
    com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
    com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
Thread 561 (779330505@qtp-120689887-37):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 3
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 559 (803867804@qtp-120689887-35):
  State: TIMED_WAITING
  Blocked count: 4
  Waited count: 5
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 558 (717520500@qtp-120689887-34):
  State: TIMED_WAITING
  Blocked count: 4
  Waited count: 5
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 548 (693274008@qtp-120689887-33):
  State: TIMED_WAITING
  Blocked count: 29
  Waited count: 31
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 539 (ApplicationMasterLauncher #17):
  State: WAITING
  Blocked count: 1
  Waited count: 18
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 518 (ApplicationMasterLauncher #16):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 478 (ApplicationMasterLauncher #15):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 454 (ApplicationMasterLauncher #14):
  State: WAITING
  Blocked count: 1
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 388 (ApplicationMasterLauncher #13):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 386 (ApplicationMasterLauncher #12):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 384 (ApplicationMasterLauncher #11):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 382 (ApplicationMasterLauncher #10):
  State: WAITING
  Blocked count: 1
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 373 (ApplicationMasterLauncher #9):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 340 (ApplicationMasterLauncher #8):
  State: WAITING
  Blocked count: 1
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 293 (ApplicationMasterLauncher #7):
  State: WAITING
  Blocked count: 1
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 265 (ApplicationMasterLauncher #6):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 263 (ApplicationMasterLauncher #5):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 242 (ApplicationMasterLauncher #4):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 227 (ApplicationMasterLauncher #3):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 216 (ApplicationMasterLauncher #2):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 206 (ApplicationMasterLauncher #1):
  State: WAITING
  Blocked count: 1
  Waited count: 3
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 203 (ApplicationMasterLauncher #0):
  State: WAITING
  Blocked count: 1
  Waited count: 3
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 202 (DestroyJavaVM):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 201 (IPC Server handler 0 on 8033):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 8934
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 197 (IPC Server listener on 8033):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:682)
Thread 200 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:856)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:839)
Thread 199 (IPC Server idle connection scanner for port 8033):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 898
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 198 (Socket Reader #1 for port 8033):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:629)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
Thread 196 (AsyncDispatcher event handler):
  State: WAITING
  Blocked count: 29
  Waited count: 8954
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1d060319
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:102)
    java.lang.Thread.run(Thread.java:745)
Thread 195 (Thread[Thread-179,5,main]):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1793
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:657)
    java.lang.Thread.run(Thread.java:745)
Thread 193 (Timer-4):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 300
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 192 (307605969@qtp-120689887-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088):
  State: RUNNABLE
  Blocked count: 5
  Waited count: 1
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 190 (com.google.inject.internal.util.$Finalizer):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.lang.ref.ReferenceQueue$Lock@4ccfd78a
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
    com.google.inject.internal.util.$Finalizer.run(Finalizer.java:114)
Thread 189 (pool-3-thread-1):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 16 (ApplicationMaster Launcher):
  State: WAITING
  Blocked count: 0
  Waited count: 20
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@38ad7906
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher$LauncherThread.run(ApplicationMasterLauncher.java:119)
Thread 187 (IPC Server handler 49 on 8032):
  State: TIMED_WAITING
  Blocked count: 5
  Waited count: 15466
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 186 (IPC Server handler 48 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15868
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 185 (IPC Server handler 47 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15041
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 184 (IPC Server handler 46 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 14752
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 183 (IPC Server handler 45 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 14754
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 182 (IPC Server handler 44 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14753
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 181 (IPC Server handler 43 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14407
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 180 (IPC Server handler 42 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14328
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 179 (IPC Server handler 41 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15238
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 178 (IPC Server handler 40 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 13823
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 177 (IPC Server handler 39 on 8032):
  State: TIMED_WAITING
  Blocked count: 7
  Waited count: 14131
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 176 (IPC Server handler 38 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15668
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 175 (IPC Server handler 37 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15385
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 174 (IPC Server handler 36 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15316
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 173 (IPC Server handler 35 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 14644
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 172 (IPC Server handler 34 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 14654
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 171 (IPC Server handler 33 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14695
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 170 (IPC Server handler 32 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 14883
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 169 (IPC Server handler 31 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15134
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 168 (IPC Server handler 30 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15263
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 167 (IPC Server handler 29 on 8032):
  State: TIMED_WAITING
  Blocked count: 4
  Waited count: 14410
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 166 (IPC Server handler 28 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14836
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 165 (IPC Server handler 27 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 14179
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 164 (IPC Server handler 26 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 15829
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 163 (IPC Server handler 25 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14998
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 162 (IPC Server handler 24 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15496
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 161 (IPC Server handler 23 on 8032):
  State: TIMED_WAITING
  Blocked count: 5
  Waited count: 14797
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 160 (IPC Server handler 22 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 15244
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 159 (IPC Server handler 21 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14851
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 158 (IPC Server handler 20 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 14581
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 157 (IPC Server handler 19 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14650
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 156 (IPC Server handler 18 on 8032):
  State: TIMED_WAITING
  Blocked count: 4
  Waited count: 14948
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 155 (IPC Server handler 17 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15169
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 154 (IPC Server handler 16 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 15018
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 153 (IPC Server handler 15 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15177
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 152 (IPC Server handler 14 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 14888
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 151 (IPC Server handler 13 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15191
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 150 (IPC Server handler 12 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14789
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 149 (IPC Server handler 11 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14919
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 148 (IPC Server handler 10 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 14389
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 147 (IPC Server handler 9 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 14686
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 146 (IPC Server handler 8 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14993
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 145 (IPC Server handler 7 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15151
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 144 (IPC Server handler 6 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14613
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 143 (IPC Server handler 5 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15748
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 142 (IPC Server handler 4 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14443
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 141 (IPC Server handler 3 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14958
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 140 (IPC Server handler 2 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15621
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 139 (IPC Server handler 1 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 14993
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 138 (IPC Server handler 0 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14313
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 134 (IPC Server listener on 8032):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:682)
Thread 137 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:856)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:839)
Thread 136 (IPC Server idle connection scanner for port 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 898
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 135 (Socket Reader #1 for port 8032):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:629)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
Thread 133 (IPC Server handler 49 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14468
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 132 (IPC Server handler 48 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14980
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 131 (IPC Server handler 47 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14885
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 130 (IPC Server handler 46 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13140
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 129 (IPC Server handler 45 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13938
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 128 (IPC Server handler 44 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15399
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 127 (IPC Server handler 43 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14026
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 126 (IPC Server handler 42 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14870
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 125 (IPC Server handler 41 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13158
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 124 (IPC Server handler 40 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13933
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 123 (IPC Server handler 39 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12691
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 122 (IPC Server handler 38 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12927
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 121 (IPC Server handler 37 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12904
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 120 (IPC Server handler 36 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12578
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 119 (IPC Server handler 35 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12275
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 118 (IPC Server handler 34 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14465
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 117 (IPC Server handler 33 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14584
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 116 (IPC Server handler 32 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13407
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 115 (IPC Server handler 31 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12540
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 114 (IPC Server handler 30 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13451
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 113 (IPC Server handler 29 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12472
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 112 (IPC Server handler 28 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13621
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 111 (IPC Server handler 27 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15398
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 110 (IPC Server handler 26 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15078
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 109 (IPC Server handler 25 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14942
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 108 (IPC Server handler 24 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15149
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 107 (IPC Server handler 23 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14043
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 106 (IPC Server handler 22 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12080
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 105 (IPC Server handler 21 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14501
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 104 (IPC Server handler 20 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14452
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 103 (IPC Server handler 19 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15200
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 102 (IPC Server handler 18 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14577
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 101 (IPC Server handler 17 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15150
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 100 (IPC Server handler 16 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14011
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 99 (IPC Server handler 15 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14212
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 98 (IPC Server handler 14 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14507
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 97 (IPC Server handler 13 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13013
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 96 (IPC Server handler 12 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13951
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 95 (IPC Server handler 11 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14401
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 94 (IPC Server handler 10 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14925
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 93 (IPC Server handler 9 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14934
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 92 (IPC Server handler 8 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13007
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 91 (IPC Server handler 7 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15271
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 90 (IPC Server handler 6 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13195
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 89 (IPC Server handler 5 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15147
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 88 (IPC Server handler 4 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14855
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 87 (IPC Server handler 3 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14742
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 86 (IPC Server handler 2 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14777
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 85 (IPC Server handler 1 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12348
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 84 (IPC Server handler 0 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14368
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 80 (IPC Server listener on 8030):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:682)
Thread 83 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:856)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:839)
Thread 82 (IPC Server idle connection scanner for port 8030):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 898
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 81 (Socket Reader #1 for port 8030):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:629)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
Thread 79 (IPC Server handler 49 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15981
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 78 (IPC Server handler 48 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16003
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 77 (IPC Server handler 47 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15850
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 76 (IPC Server handler 46 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15918
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 75 (IPC Server handler 45 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15866
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 74 (IPC Server handler 44 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16154
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 73 (IPC Server handler 43 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15793
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 72 (IPC Server handler 42 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15897
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 71 (IPC Server handler 41 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16026
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 70 (IPC Server handler 40 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15802
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 69 (IPC Server handler 39 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15931
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 68 (IPC Server handler 38 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15994
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 67 (IPC Server handler 37 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16030
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 66 (IPC Server handler 36 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15899
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 65 (IPC Server handler 35 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15877
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 64 (IPC Server handler 34 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15593
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 63 (IPC Server handler 33 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15788
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 62 (IPC Server handler 32 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15966
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 61 (IPC Server handler 31 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15803
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 60 (IPC Server handler 30 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15920
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 59 (IPC Server handler 29 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15839
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 58 (IPC Server handler 28 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15996
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 57 (IPC Server handler 27 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15999
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 56 (IPC Server handler 26 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16111
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 55 (IPC Server handler 25 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16020
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 54 (IPC Server handler 24 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16063
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 53 (IPC Server handler 23 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15825
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 52 (IPC Server handler 22 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15993
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 51 (IPC Server handler 21 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15719
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 50 (IPC Server handler 20 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15943
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 49 (IPC Server handler 19 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15891
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 48 (IPC Server handler 18 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16049
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 47 (IPC Server handler 17 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15953
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 46 (IPC Server handler 16 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15942
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 45 (IPC Server handler 15 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15749
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 44 (IPC Server handler 14 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15957
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 43 (IPC Server handler 13 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15923
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 42 (IPC Server handler 12 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16008
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 41 (IPC Server handler 11 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15891
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 40 (IPC Server handler 10 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15912
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 39 (IPC Server handler 9 on 8031):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 16215
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 38 (IPC Server handler 8 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15996
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 37 (IPC Server handler 7 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16007
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 36 (IPC Server handler 6 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16025
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 35 (IPC Server handler 5 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16010
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 34 (IPC Server handler 4 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16041
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 33 (IPC Server handler 3 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16040
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 32 (IPC Server handler 2 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16067
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 31 (IPC Server handler 1 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16064
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 30 (IPC Server handler 0 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16195
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 26 (IPC Server listener on 8031):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:682)
Thread 29 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:856)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:839)
Thread 28 (IPC Server idle connection scanner for port 8031):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 898
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 27 (Socket Reader #1 for port 8031):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 28
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:629)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
Thread 25 (Ping Checker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 45
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.yarn.util.AbstractLivelinessMonitor$PingChecker.run(AbstractLivelinessMonitor.java:133)
    java.lang.Thread.run(Thread.java:745)
Thread 14 (ResourceManager Event Processor):
  State: WAITING
  Blocked count: 7
  Waited count: 8791
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@a34aa7d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:670)
    java.lang.Thread.run(Thread.java:745)
Thread 24 (AsyncDispatcher event handler):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@24024f1c
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:102)
    java.lang.Thread.run(Thread.java:745)
Thread 23 (Ping Checker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 45
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.yarn.util.AbstractLivelinessMonitor$PingChecker.run(AbstractLivelinessMonitor.java:133)
    java.lang.Thread.run(Thread.java:745)
Thread 22 (Ping Checker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 45
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.yarn.util.AbstractLivelinessMonitor$PingChecker.run(AbstractLivelinessMonitor.java:133)
    java.lang.Thread.run(Thread.java:745)
Thread 21 (Ping Checker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 45
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.yarn.util.AbstractLivelinessMonitor$PingChecker.run(AbstractLivelinessMonitor.java:133)
    java.lang.Thread.run(Thread.java:745)
Thread 20 (Thread[Thread-9,5,main]):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1793
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:657)
    java.lang.Thread.run(Thread.java:745)
Thread 18 (AsyncDispatcher event handler):
  State: WAITING
  Blocked count: 6
  Waited count: 47
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@b18a4c7
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:102)
    java.lang.Thread.run(Thread.java:745)
Thread 15 (Timer for 'ResourceManager' metrics system):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 898
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 13 (Timer-2):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 2
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 12 (Timer-1):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 2
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 11 (Timer-0):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 2
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 4 (Signal Dispatcher):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 3 (Finalizer):
  State: WAITING
  Blocked count: 457
  Waited count: 11
  Waiting on java.lang.ref.ReferenceQueue$Lock@5965d37
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
    java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)
Thread 2 (Reference Handler):
  State: WAITING
  Blocked count: 19
  Waited count: 9
  Waiting on java.lang.ref.Reference$Lock@7a5ceedd
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:502)
    java.lang.ref.Reference.tryHandlePending(Reference.java:191)
    java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

2017-01-18 14:01:49,764 INFO org.apache.hadoop.http.HttpServer2: Process Thread Dump: jsp requested
214 active threads
Thread 566 (1271131001@qtp-120689887-39):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 3
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 563 (ApplicationMasterLauncher #18):
  State: WAITING
  Blocked count: 1
  Waited count: 3
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 562 (2015394411@qtp-120689887-38):
  State: TIMED_WAITING
  Blocked count: 15
  Waited count: 16
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 561 (779330505@qtp-120689887-37):
  State: TIMED_WAITING
  Blocked count: 6
  Waited count: 8
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 559 (803867804@qtp-120689887-35):
  State: TIMED_WAITING
  Blocked count: 7
  Waited count: 9
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 558 (717520500@qtp-120689887-34):
  State: RUNNABLE
  Blocked count: 16
  Waited count: 16
  Stack:
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:178)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:139)
    org.apache.hadoop.util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:168)
    org.apache.hadoop.util.ReflectionUtils.logThreadInfo(ReflectionUtils.java:223)
    org.apache.hadoop.http.HttpServer2$StackServlet.doGet(HttpServer2.java:1114)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
    org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
    com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)
    com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)
    com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
    org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:172)
    com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
    com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
    com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
    com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
    com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
Thread 539 (ApplicationMasterLauncher #17):
  State: WAITING
  Blocked count: 1
  Waited count: 18
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 518 (ApplicationMasterLauncher #16):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 478 (ApplicationMasterLauncher #15):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 454 (ApplicationMasterLauncher #14):
  State: WAITING
  Blocked count: 1
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 388 (ApplicationMasterLauncher #13):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 386 (ApplicationMasterLauncher #12):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 384 (ApplicationMasterLauncher #11):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 382 (ApplicationMasterLauncher #10):
  State: WAITING
  Blocked count: 1
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 373 (ApplicationMasterLauncher #9):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 340 (ApplicationMasterLauncher #8):
  State: WAITING
  Blocked count: 1
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 293 (ApplicationMasterLauncher #7):
  State: WAITING
  Blocked count: 1
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 265 (ApplicationMasterLauncher #6):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 263 (ApplicationMasterLauncher #5):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 242 (ApplicationMasterLauncher #4):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 227 (ApplicationMasterLauncher #3):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 216 (ApplicationMasterLauncher #2):
  State: WAITING
  Blocked count: 0
  Waited count: 19
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 206 (ApplicationMasterLauncher #1):
  State: WAITING
  Blocked count: 1
  Waited count: 3
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 203 (ApplicationMasterLauncher #0):
  State: WAITING
  Blocked count: 1
  Waited count: 3
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@69e2bb9d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 202 (DestroyJavaVM):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 201 (IPC Server handler 0 on 8033):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 9020
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 197 (IPC Server listener on 8033):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:682)
Thread 200 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:856)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:839)
Thread 199 (IPC Server idle connection scanner for port 8033):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 906
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 198 (Socket Reader #1 for port 8033):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:629)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
Thread 196 (AsyncDispatcher event handler):
  State: WAITING
  Blocked count: 29
  Waited count: 9040
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1d060319
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:102)
    java.lang.Thread.run(Thread.java:745)
Thread 195 (Thread[Thread-179,5,main]):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1810
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:657)
    java.lang.Thread.run(Thread.java:745)
Thread 193 (Timer-4):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 303
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 192 (307605969@qtp-120689887-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088):
  State: RUNNABLE
  Blocked count: 5
  Waited count: 1
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 190 (com.google.inject.internal.util.$Finalizer):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.lang.ref.ReferenceQueue$Lock@4ccfd78a
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
    com.google.inject.internal.util.$Finalizer.run(Finalizer.java:114)
Thread 189 (pool-3-thread-1):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Thread 16 (ApplicationMaster Launcher):
  State: WAITING
  Blocked count: 0
  Waited count: 20
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@38ad7906
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher$LauncherThread.run(ApplicationMasterLauncher.java:119)
Thread 187 (IPC Server handler 49 on 8032):
  State: TIMED_WAITING
  Blocked count: 5
  Waited count: 15597
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 186 (IPC Server handler 48 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15993
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 185 (IPC Server handler 47 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15201
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 184 (IPC Server handler 46 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 14901
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 183 (IPC Server handler 45 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 14900
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 182 (IPC Server handler 44 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14859
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 181 (IPC Server handler 43 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14557
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 180 (IPC Server handler 42 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14489
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 179 (IPC Server handler 41 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15402
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 178 (IPC Server handler 40 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 13972
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 177 (IPC Server handler 39 on 8032):
  State: TIMED_WAITING
  Blocked count: 7
  Waited count: 14289
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 176 (IPC Server handler 38 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15823
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 175 (IPC Server handler 37 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15491
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 174 (IPC Server handler 36 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15463
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 173 (IPC Server handler 35 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 14801
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 172 (IPC Server handler 34 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 14761
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 171 (IPC Server handler 33 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14795
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 170 (IPC Server handler 32 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15034
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 169 (IPC Server handler 31 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15292
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 168 (IPC Server handler 30 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15387
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 167 (IPC Server handler 29 on 8032):
  State: TIMED_WAITING
  Blocked count: 4
  Waited count: 14567
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 166 (IPC Server handler 28 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14977
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 165 (IPC Server handler 27 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 14333
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 164 (IPC Server handler 26 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 15955
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 163 (IPC Server handler 25 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15144
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 162 (IPC Server handler 24 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15630
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 161 (IPC Server handler 23 on 8032):
  State: TIMED_WAITING
  Blocked count: 5
  Waited count: 14920
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 160 (IPC Server handler 22 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 15390
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 159 (IPC Server handler 21 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15007
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 158 (IPC Server handler 20 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 14667
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 157 (IPC Server handler 19 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14770
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 156 (IPC Server handler 18 on 8032):
  State: TIMED_WAITING
  Blocked count: 4
  Waited count: 15076
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 155 (IPC Server handler 17 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15281
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 154 (IPC Server handler 16 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 15168
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 153 (IPC Server handler 15 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15307
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 152 (IPC Server handler 14 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 15043
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 151 (IPC Server handler 13 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15357
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 150 (IPC Server handler 12 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14944
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 149 (IPC Server handler 11 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15051
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 148 (IPC Server handler 10 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 14533
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 147 (IPC Server handler 9 on 8032):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 14799
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 146 (IPC Server handler 8 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15142
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 145 (IPC Server handler 7 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15304
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 144 (IPC Server handler 6 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 14749
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 143 (IPC Server handler 5 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15891
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 142 (IPC Server handler 4 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14599
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 141 (IPC Server handler 3 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15106
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 140 (IPC Server handler 2 on 8032):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 15772
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 139 (IPC Server handler 1 on 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 15131
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 138 (IPC Server handler 0 on 8032):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14442
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 134 (IPC Server listener on 8032):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:682)
Thread 137 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:856)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:839)
Thread 136 (IPC Server idle connection scanner for port 8032):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 907
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 135 (Socket Reader #1 for port 8032):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:629)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
Thread 133 (IPC Server handler 49 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14590
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 132 (IPC Server handler 48 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15105
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 131 (IPC Server handler 47 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15047
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 130 (IPC Server handler 46 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13298
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 129 (IPC Server handler 45 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14063
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 128 (IPC Server handler 44 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15566
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 127 (IPC Server handler 43 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14185
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 126 (IPC Server handler 42 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15036
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 125 (IPC Server handler 41 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13322
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 124 (IPC Server handler 40 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14061
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 123 (IPC Server handler 39 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12847
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 122 (IPC Server handler 38 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13075
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 121 (IPC Server handler 37 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13048
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 120 (IPC Server handler 36 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12711
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 119 (IPC Server handler 35 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12429
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 118 (IPC Server handler 34 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14621
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 117 (IPC Server handler 33 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14684
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 116 (IPC Server handler 32 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13566
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 115 (IPC Server handler 31 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12698
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 114 (IPC Server handler 30 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13573
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 113 (IPC Server handler 29 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12601
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 112 (IPC Server handler 28 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13773
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 111 (IPC Server handler 27 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15551
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 110 (IPC Server handler 26 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15223
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 109 (IPC Server handler 25 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15077
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 108 (IPC Server handler 24 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15297
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 107 (IPC Server handler 23 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14203
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 106 (IPC Server handler 22 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12231
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 105 (IPC Server handler 21 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14666
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 104 (IPC Server handler 20 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14611
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 103 (IPC Server handler 19 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15352
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 102 (IPC Server handler 18 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14720
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 101 (IPC Server handler 17 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15290
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 100 (IPC Server handler 16 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14167
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 99 (IPC Server handler 15 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14368
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 98 (IPC Server handler 14 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14666
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 97 (IPC Server handler 13 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13180
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 96 (IPC Server handler 12 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14112
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 95 (IPC Server handler 11 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14566
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 94 (IPC Server handler 10 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15053
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 93 (IPC Server handler 9 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15084
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 92 (IPC Server handler 8 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13165
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 91 (IPC Server handler 7 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15422
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 90 (IPC Server handler 6 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 13290
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 89 (IPC Server handler 5 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15314
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 88 (IPC Server handler 4 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14994
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 87 (IPC Server handler 3 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14884
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 86 (IPC Server handler 2 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14913
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 85 (IPC Server handler 1 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 12505
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 84 (IPC Server handler 0 on 8030):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 14455
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 80 (IPC Server listener on 8030):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:682)
Thread 83 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:856)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:839)
Thread 82 (IPC Server idle connection scanner for port 8030):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 907
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 81 (Socket Reader #1 for port 8030):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:629)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
Thread 79 (IPC Server handler 49 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16132
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 78 (IPC Server handler 48 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16160
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 77 (IPC Server handler 47 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16013
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 76 (IPC Server handler 46 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16065
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 75 (IPC Server handler 45 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16020
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 74 (IPC Server handler 44 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16305
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 73 (IPC Server handler 43 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15949
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 72 (IPC Server handler 42 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16051
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 71 (IPC Server handler 41 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16182
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 70 (IPC Server handler 40 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15949
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 69 (IPC Server handler 39 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16081
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 68 (IPC Server handler 38 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16154
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 67 (IPC Server handler 37 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16167
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 66 (IPC Server handler 36 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16051
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 65 (IPC Server handler 35 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16001
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 64 (IPC Server handler 34 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15748
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 63 (IPC Server handler 33 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15932
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 62 (IPC Server handler 32 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16111
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 61 (IPC Server handler 31 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15955
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 60 (IPC Server handler 30 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16080
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 59 (IPC Server handler 29 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15998
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 58 (IPC Server handler 28 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16147
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 57 (IPC Server handler 27 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16155
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 56 (IPC Server handler 26 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16267
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 55 (IPC Server handler 25 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16155
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 54 (IPC Server handler 24 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16221
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 53 (IPC Server handler 23 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15989
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 52 (IPC Server handler 22 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16148
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 51 (IPC Server handler 21 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15852
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 50 (IPC Server handler 20 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16075
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 49 (IPC Server handler 19 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16040
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 48 (IPC Server handler 18 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16206
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 47 (IPC Server handler 17 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16114
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 46 (IPC Server handler 16 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16093
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 45 (IPC Server handler 15 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15873
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 44 (IPC Server handler 14 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16117
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 43 (IPC Server handler 13 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16080
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 42 (IPC Server handler 12 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16163
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 41 (IPC Server handler 11 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16047
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 40 (IPC Server handler 10 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16072
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 39 (IPC Server handler 9 on 8031):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 16366
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 38 (IPC Server handler 8 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16148
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 37 (IPC Server handler 7 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16168
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 36 (IPC Server handler 6 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16185
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 35 (IPC Server handler 5 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16169
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 34 (IPC Server handler 4 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16198
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 33 (IPC Server handler 3 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16201
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 32 (IPC Server handler 2 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16228
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 31 (IPC Server handler 1 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16205
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 30 (IPC Server handler 0 on 8031):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 16359
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:119)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 26 (IPC Server listener on 8031):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:682)
Thread 29 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:856)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:839)
Thread 28 (IPC Server idle connection scanner for port 8031):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 907
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 27 (Socket Reader #1 for port 8031):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 28
  Stack:
    sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)
    sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:629)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
Thread 25 (Ping Checker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 46
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.yarn.util.AbstractLivelinessMonitor$PingChecker.run(AbstractLivelinessMonitor.java:133)
    java.lang.Thread.run(Thread.java:745)
Thread 14 (ResourceManager Event Processor):
  State: WAITING
  Blocked count: 7
  Waited count: 8877
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@a34aa7d
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:670)
    java.lang.Thread.run(Thread.java:745)
Thread 24 (AsyncDispatcher event handler):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@24024f1c
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:102)
    java.lang.Thread.run(Thread.java:745)
Thread 23 (Ping Checker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 46
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.yarn.util.AbstractLivelinessMonitor$PingChecker.run(AbstractLivelinessMonitor.java:133)
    java.lang.Thread.run(Thread.java:745)
Thread 22 (Ping Checker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 46
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.yarn.util.AbstractLivelinessMonitor$PingChecker.run(AbstractLivelinessMonitor.java:133)
    java.lang.Thread.run(Thread.java:745)
Thread 21 (Ping Checker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 46
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.yarn.util.AbstractLivelinessMonitor$PingChecker.run(AbstractLivelinessMonitor.java:133)
    java.lang.Thread.run(Thread.java:745)
Thread 20 (Thread[Thread-9,5,main]):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1810
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:657)
    java.lang.Thread.run(Thread.java:745)
Thread 18 (AsyncDispatcher event handler):
  State: WAITING
  Blocked count: 6
  Waited count: 47
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@b18a4c7
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:102)
    java.lang.Thread.run(Thread.java:745)
Thread 15 (Timer for 'ResourceManager' metrics system):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 907
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 13 (Timer-2):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 2
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 12 (Timer-1):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 2
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 11 (Timer-0):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 2
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 4 (Signal Dispatcher):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 3 (Finalizer):
  State: WAITING
  Blocked count: 457
  Waited count: 11
  Waiting on java.lang.ref.ReferenceQueue$Lock@5965d37
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
    java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)
Thread 2 (Reference Handler):
  State: WAITING
  Blocked count: 19
  Waited count: 9
  Waiting on java.lang.ref.Reference$Lock@7a5ceedd
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:502)
    java.lang.ref.Reference.tryHandlePending(Reference.java:191)
    java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

2017-01-18 14:07:35,626 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 12
2017-01-18 14:07:36,678 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 12 submitted by user lybuestc
2017-01-18 14:07:36,679 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0012
2017-01-18 14:07:36,679 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0012
2017-01-18 14:07:36,679 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0012 State change from NEW to NEW_SAVING
2017-01-18 14:07:36,679 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0012
2017-01-18 14:07:36,679 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0012 State change from NEW_SAVING to SUBMITTED
2017-01-18 14:07:36,679 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0012 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 14:07:36,679 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0012 from user: lybuestc, in queue: default
2017-01-18 14:07:36,682 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0012 State change from SUBMITTED to ACCEPTED
2017-01-18 14:07:36,682 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0012_000001
2017-01-18 14:07:36,682 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0012_000001 State change from NEW to SUBMITTED
2017-01-18 14:07:36,682 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 14:07:36,682 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 14:07:36,682 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0012 from user: lybuestc activated in queue: default
2017-01-18 14:07:36,682 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0012 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@16e1128, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 14:07:36,682 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0012_000001 to scheduler from user lybuestc in queue default
2017-01-18 14:07:36,684 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0012_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 14:07:37,493 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:07:37,493 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0012	CONTAINERID=container_1484710253544_0012_01_000001
2017-01-18 14:07:37,493 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0012_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 14:07:37,494 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0012_000001 container=Container: [ContainerId: container_1484710253544_0012_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 14:07:37,494 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:07:37,494 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:07:37,494 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0012_01_000001
2017-01-18 14:07:37,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:07:37,495 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0012_000001
2017-01-18 14:07:37,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0012 AttemptId: appattempt_1484710253544_0012_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0012_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 14:07:37,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0012_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 14:07:37,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0012_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 14:07:37,495 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0012_000001
2017-01-18 14:07:37,497 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0012_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0012_000001
2017-01-18 14:07:37,497 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0012_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 14:07:37,497 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0012_000001
2017-01-18 14:07:37,497 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0012_000001
2017-01-18 14:07:37,506 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484710253544_0012_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0012_000001
2017-01-18 14:07:37,507 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0012_000001 State change from ALLOCATED to LAUNCHED
2017-01-18 14:07:38,499 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:07:42,495 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484710253544_0012_000001 (auth:SIMPLE)
2017-01-18 14:07:42,500 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484710253544_0012_000001
2017-01-18 14:07:42,500 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484710253544_0012	APPATTEMPTID=appattempt_1484710253544_0012_000001
2017-01-18 14:07:42,500 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0012_000001 State change from LAUNCHED to RUNNING
2017-01-18 14:07:42,500 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0012 State change from ACCEPTED to RUNNING
2017-01-18 14:07:44,516 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000002 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:07:44,516 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0012	CONTAINERID=container_1484710253544_0012_01_000002
2017-01-18 14:07:44,516 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0012_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:07:44,516 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0012_000001 container=Container: [ContainerId: container_1484710253544_0012_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=NODE_LOCAL
2017-01-18 14:07:44,516 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:07:44,516 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:07:44,549 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0012_01_000002
2017-01-18 14:07:44,550 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:07:45,526 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000002 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:07:45,562 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0012
2017-01-18 14:07:47,332 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000002 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:07:47,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0012_01_000002 in state: COMPLETED event:FINISHED
2017-01-18 14:07:47,333 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0012	CONTAINERID=container_1484710253544_0012_01_000002
2017-01-18 14:07:47,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0012_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:07:47,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:07:47,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0012_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:07:47,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:07:47,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:07:47,334 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0012_000001 released container container_1484710253544_0012_01_000002 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:07:49,337 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000003 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:07:49,337 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0012	CONTAINERID=container_1484710253544_0012_01_000003
2017-01-18 14:07:49,337 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0012_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:07:49,337 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0012_000001 container=Container: [ContainerId: container_1484710253544_0012_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 14:07:49,337 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:07:49,337 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:07:49,589 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:07:50,341 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000003 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:07:50,595 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0012
2017-01-18 14:07:52,322 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000003 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:07:52,322 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0012_01_000003 in state: COMPLETED event:FINISHED
2017-01-18 14:07:52,322 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0012	CONTAINERID=container_1484710253544_0012_01_000003
2017-01-18 14:07:52,322 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0012_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:07:52,322 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:07:52,323 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0012_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:07:52,323 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:07:52,323 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:07:52,323 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0012_000001 released container container_1484710253544_0012_01_000003 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:07:52,458 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0012_000001 with final state: FINISHING, and exit status: -1000
2017-01-18 14:07:52,459 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0012_000001 State change from RUNNING to FINAL_SAVING
2017-01-18 14:07:52,459 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0012 with final state: FINISHING
2017-01-18 14:07:52,459 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0012 State change from RUNNING to FINAL_SAVING
2017-01-18 14:07:52,459 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0012
2017-01-18 14:07:52,459 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0012_000001 State change from FINAL_SAVING to FINISHING
2017-01-18 14:07:52,459 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0012 State change from FINAL_SAVING to FINISHING
2017-01-18 14:07:53,461 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484710253544_0012 unregistered successfully. 
2017-01-18 14:07:58,640 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0012_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:07:58,640 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0012_01_000001 in state: COMPLETED event:FINISHED
2017-01-18 14:07:58,640 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0012	CONTAINERID=container_1484710253544_0012_01_000001
2017-01-18 14:07:58,640 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0012_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 14:07:58,640 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0012_000001
2017-01-18 14:07:58,640 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 14:07:58,640 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0012_000001
2017-01-18 14:07:58,640 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0012_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 14:07:58,640 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0012_000001 State change from FINISHING to FINISHED
2017-01-18 14:07:58,641 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 14:07:58,641 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0012 State change from FINISHING to FINISHED
2017-01-18 14:07:58,641 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 14:07:58,641 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0012
2017-01-18 14:07:58,641 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0012_000001 released container container_1484710253544_0012_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-18 14:07:58,642 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0012,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484710253544_0012/,appMasterHost=10.1.4.199,startTime=1484719656678,finishTime=1484719672459,finalStatus=SUCCEEDED,memorySeconds=49249,vcoreSeconds=25,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 14:07:58,642 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0012_000001 is done. finalState=FINISHED
2017-01-18 14:07:58,642 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0012 requests cleared
2017-01-18 14:07:58,642 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0012 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 14:07:58,642 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0012 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 14:07:58,642 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0012_000001
2017-01-18 14:08:00,647 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Null container completed...
2017-01-18 14:08:55,328 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 13
2017-01-18 14:08:56,034 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 13 submitted by user lybuestc
2017-01-18 14:08:56,034 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0013
2017-01-18 14:08:56,034 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0013
2017-01-18 14:08:56,034 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0013 State change from NEW to NEW_SAVING
2017-01-18 14:08:56,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0013
2017-01-18 14:08:56,035 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0013 State change from NEW_SAVING to SUBMITTED
2017-01-18 14:08:56,035 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0013 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 14:08:56,035 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0013 from user: lybuestc, in queue: default
2017-01-18 14:08:56,038 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0013 State change from SUBMITTED to ACCEPTED
2017-01-18 14:08:56,038 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0013_000001
2017-01-18 14:08:56,038 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0013_000001 State change from NEW to SUBMITTED
2017-01-18 14:08:56,038 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 14:08:56,038 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 14:08:56,038 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0013 from user: lybuestc activated in queue: default
2017-01-18 14:08:56,038 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0013 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@1c58f7db, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 14:08:56,038 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0013_000001 to scheduler from user lybuestc in queue default
2017-01-18 14:08:56,039 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0013_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 14:08:56,839 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:08:56,839 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0013	CONTAINERID=container_1484710253544_0013_01_000001
2017-01-18 14:08:56,839 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0013_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 14:08:56,839 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0013_000001 container=Container: [ContainerId: container_1484710253544_0013_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 14:08:56,839 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:08:56,839 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:08:56,840 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0013_01_000001
2017-01-18 14:08:56,841 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:08:56,842 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0013_000001
2017-01-18 14:08:56,842 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0013 AttemptId: appattempt_1484710253544_0013_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0013_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 14:08:56,842 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0013_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 14:08:56,842 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0013_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 14:08:56,842 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0013_000001
2017-01-18 14:08:56,843 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0013_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0013_000001
2017-01-18 14:08:56,844 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0013_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 14:08:56,844 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0013_000001
2017-01-18 14:08:56,844 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0013_000001
2017-01-18 14:08:56,851 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484710253544_0013_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0013_000001
2017-01-18 14:08:56,851 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0013_000001 State change from ALLOCATED to LAUNCHED
2017-01-18 14:08:57,844 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:09:01,904 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484710253544_0013_000001 (auth:SIMPLE)
2017-01-18 14:09:01,909 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484710253544_0013_000001
2017-01-18 14:09:01,909 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484710253544_0013	APPATTEMPTID=appattempt_1484710253544_0013_000001
2017-01-18 14:09:01,909 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0013_000001 State change from LAUNCHED to RUNNING
2017-01-18 14:09:01,909 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0013 State change from ACCEPTED to RUNNING
2017-01-18 14:09:03,868 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000002 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:09:03,869 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0013	CONTAINERID=container_1484710253544_0013_01_000002
2017-01-18 14:09:03,869 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0013_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:09:03,869 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0013_000001 container=Container: [ContainerId: container_1484710253544_0013_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=NODE_LOCAL
2017-01-18 14:09:03,869 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:09:03,869 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:09:03,960 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0013_01_000002
2017-01-18 14:09:03,961 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:09:04,873 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000002 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:09:04,976 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0013
2017-01-18 14:09:07,130 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000002 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:09:07,130 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0013_01_000002 in state: COMPLETED event:FINISHED
2017-01-18 14:09:07,130 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0013	CONTAINERID=container_1484710253544_0013_01_000002
2017-01-18 14:09:07,130 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0013_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:09:07,130 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:09:07,132 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0013_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:09:07,132 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:09:07,132 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:09:07,132 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0013_000001 released container container_1484710253544_0013_01_000002 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:09:09,138 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000003 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:09:09,138 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0013	CONTAINERID=container_1484710253544_0013_01_000003
2017-01-18 14:09:09,138 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0013_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:09:09,138 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0013_000001 container=Container: [ContainerId: container_1484710253544_0013_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 14:09:09,138 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:09:09,138 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:09:10,007 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:09:10,142 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000003 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:09:11,014 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0013
2017-01-18 14:09:13,091 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000003 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:09:13,091 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0013_01_000003 in state: COMPLETED event:FINISHED
2017-01-18 14:09:13,091 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0013	CONTAINERID=container_1484710253544_0013_01_000003
2017-01-18 14:09:13,091 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0013_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:09:13,091 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:09:13,092 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0013_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:09:13,092 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:09:13,092 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:09:13,092 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0013_000001 released container container_1484710253544_0013_01_000003 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:09:13,233 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0013_000001 with final state: FINISHING, and exit status: -1000
2017-01-18 14:09:13,233 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0013_000001 State change from RUNNING to FINAL_SAVING
2017-01-18 14:09:13,233 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0013 with final state: FINISHING
2017-01-18 14:09:13,233 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0013 State change from RUNNING to FINAL_SAVING
2017-01-18 14:09:13,233 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0013
2017-01-18 14:09:13,233 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0013_000001 State change from FINAL_SAVING to FINISHING
2017-01-18 14:09:13,233 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0013 State change from FINAL_SAVING to FINISHING
2017-01-18 14:09:14,233 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484710253544_0013 unregistered successfully. 
2017-01-18 14:09:19,311 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0013_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:09:19,311 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0013_000001
2017-01-18 14:09:19,311 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0013_01_000001 in state: COMPLETED event:FINISHED
2017-01-18 14:09:19,311 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0013_000001
2017-01-18 14:09:19,311 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0013	CONTAINERID=container_1484710253544_0013_01_000001
2017-01-18 14:09:19,311 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0013_000001 State change from FINISHING to FINISHED
2017-01-18 14:09:19,311 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0013_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 14:09:19,311 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0013 State change from FINISHING to FINISHED
2017-01-18 14:09:19,311 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0013
2017-01-18 14:09:19,311 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 14:09:19,311 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0013_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 14:09:19,312 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 14:09:19,312 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0013,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484710253544_0013/,appMasterHost=10.1.4.199,startTime=1484719736034,finishTime=1484719753233,finalStatus=SUCCEEDED,memorySeconds=53409,vcoreSeconds=28,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 14:09:19,312 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 14:09:19,312 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0013_000001 released container container_1484710253544_0013_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-18 14:09:19,312 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0013_000001 is done. finalState=FINISHED
2017-01-18 14:09:19,312 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0013 requests cleared
2017-01-18 14:09:19,312 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0013_000001
2017-01-18 14:09:19,312 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0013 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 14:09:19,313 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0013 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 14:09:21,317 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Null container completed...
2017-01-18 14:10:12,176 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 14
2017-01-18 14:10:12,940 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 14 submitted by user lybuestc
2017-01-18 14:10:12,940 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0014
2017-01-18 14:10:12,940 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0014
2017-01-18 14:10:12,940 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0014 State change from NEW to NEW_SAVING
2017-01-18 14:10:12,940 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0014
2017-01-18 14:10:12,940 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0014 State change from NEW_SAVING to SUBMITTED
2017-01-18 14:10:12,940 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0014 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 14:10:12,940 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0014 from user: lybuestc, in queue: default
2017-01-18 14:10:12,943 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0014 State change from SUBMITTED to ACCEPTED
2017-01-18 14:10:12,943 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0014_000001
2017-01-18 14:10:12,943 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0014_000001 State change from NEW to SUBMITTED
2017-01-18 14:10:12,943 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 14:10:12,943 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 14:10:12,943 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0014 from user: lybuestc activated in queue: default
2017-01-18 14:10:12,943 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0014 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@6dce4229, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 14:10:12,943 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0014_000001 to scheduler from user lybuestc in queue default
2017-01-18 14:10:12,945 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0014_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 14:10:13,505 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:10:13,505 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0014	CONTAINERID=container_1484710253544_0014_01_000001
2017-01-18 14:10:13,505 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0014_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 14:10:13,505 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0014_000001 container=Container: [ContainerId: container_1484710253544_0014_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 14:10:13,505 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:10:13,505 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:10:13,506 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0014_01_000001
2017-01-18 14:10:13,507 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:10:13,507 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0014_000001
2017-01-18 14:10:13,507 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0014 AttemptId: appattempt_1484710253544_0014_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0014_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 14:10:13,508 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0014_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 14:10:13,508 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0014_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 14:10:13,508 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0014_000001
2017-01-18 14:10:13,510 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0014_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0014_000001
2017-01-18 14:10:13,510 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0014_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 14:10:13,510 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0014_000001
2017-01-18 14:10:13,510 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0014_000001
2017-01-18 14:10:13,525 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484710253544_0014_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0014_000001
2017-01-18 14:10:13,525 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0014_000001 State change from ALLOCATED to LAUNCHED
2017-01-18 14:10:14,509 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:10:19,033 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484710253544_0014_000001 (auth:SIMPLE)
2017-01-18 14:10:19,038 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484710253544_0014_000001
2017-01-18 14:10:19,038 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484710253544_0014	APPATTEMPTID=appattempt_1484710253544_0014_000001
2017-01-18 14:10:19,038 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0014_000001 State change from LAUNCHED to RUNNING
2017-01-18 14:10:19,038 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0014 State change from ACCEPTED to RUNNING
2017-01-18 14:10:20,536 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000002 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:10:20,536 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0014	CONTAINERID=container_1484710253544_0014_01_000002
2017-01-18 14:10:20,536 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0014_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:10:20,536 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0014_000001 container=Container: [ContainerId: container_1484710253544_0014_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=NODE_LOCAL
2017-01-18 14:10:20,536 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:10:20,536 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:10:21,097 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0014_01_000002
2017-01-18 14:10:21,099 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:10:21,538 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000002 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:10:22,116 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0014
2017-01-18 14:10:24,211 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000002 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:10:24,211 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0014_01_000002 in state: COMPLETED event:FINISHED
2017-01-18 14:10:24,211 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0014	CONTAINERID=container_1484710253544_0014_01_000002
2017-01-18 14:10:24,211 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0014_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:10:24,211 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:10:24,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0014_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:10:24,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:10:24,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:10:24,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0014_000001 released container container_1484710253544_0014_01_000002 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:10:26,217 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000003 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:10:26,217 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0014	CONTAINERID=container_1484710253544_0014_01_000003
2017-01-18 14:10:26,217 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0014_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:10:26,217 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0014_000001 container=Container: [ContainerId: container_1484710253544_0014_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 14:10:26,217 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:10:26,217 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:10:27,150 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:10:27,218 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000003 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:10:28,157 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0014
2017-01-18 14:10:30,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000003 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:10:30,816 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0014_01_000003 in state: COMPLETED event:FINISHED
2017-01-18 14:10:30,816 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0014	CONTAINERID=container_1484710253544_0014_01_000003
2017-01-18 14:10:30,816 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0014_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:10:30,816 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:10:30,817 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0014_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:10:30,817 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:10:30,817 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:10:30,817 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0014_000001 released container container_1484710253544_0014_01_000003 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:10:30,949 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0014_000001 with final state: FINISHING, and exit status: -1000
2017-01-18 14:10:30,950 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0014_000001 State change from RUNNING to FINAL_SAVING
2017-01-18 14:10:30,950 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0014 with final state: FINISHING
2017-01-18 14:10:30,950 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0014 State change from RUNNING to FINAL_SAVING
2017-01-18 14:10:30,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0014
2017-01-18 14:10:30,950 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0014_000001 State change from FINAL_SAVING to FINISHING
2017-01-18 14:10:30,950 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0014 State change from FINAL_SAVING to FINISHING
2017-01-18 14:10:31,953 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484710253544_0014 unregistered successfully. 
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0014_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0014_000001
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0014_01_000001 in state: COMPLETED event:FINISHED
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0014_000001
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0014	CONTAINERID=container_1484710253544_0014_01_000001
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0014_000001 State change from FINISHING to FINISHED
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0014_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0014 State change from FINISHING to FINISHED
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0014_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0014_000001 released container container_1484710253544_0014_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0014
2017-01-18 14:10:37,131 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0014_000001 is done. finalState=FINISHED
2017-01-18 14:10:37,132 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0014 requests cleared
2017-01-18 14:10:37,132 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0014,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484710253544_0014/,appMasterHost=10.1.4.199,startTime=1484719812939,finishTime=1484719830950,finalStatus=SUCCEEDED,memorySeconds=56859,vcoreSeconds=30,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 14:10:37,132 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0014 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 14:10:37,132 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0014 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 14:10:37,132 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0014_000001
2017-01-18 14:10:39,140 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Null container completed...
2017-01-18 14:11:44,589 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 15
2017-01-18 14:11:45,855 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 15 submitted by user lybuestc
2017-01-18 14:11:45,855 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0015
2017-01-18 14:11:45,855 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0015
2017-01-18 14:11:45,855 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0015 State change from NEW to NEW_SAVING
2017-01-18 14:11:45,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0015
2017-01-18 14:11:45,856 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0015 State change from NEW_SAVING to SUBMITTED
2017-01-18 14:11:45,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0015 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 14:11:45,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0015 from user: lybuestc, in queue: default
2017-01-18 14:11:45,860 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0015 State change from SUBMITTED to ACCEPTED
2017-01-18 14:11:45,860 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0015_000001
2017-01-18 14:11:45,860 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0015_000001 State change from NEW to SUBMITTED
2017-01-18 14:11:45,860 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 14:11:45,860 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 14:11:45,860 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0015 from user: lybuestc activated in queue: default
2017-01-18 14:11:45,860 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0015 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@548d26c8, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 14:11:45,860 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0015_000001 to scheduler from user lybuestc in queue default
2017-01-18 14:11:45,862 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0015_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 14:11:46,358 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:11:46,358 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0015	CONTAINERID=container_1484710253544_0015_01_000001
2017-01-18 14:11:46,358 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0015_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 14:11:46,358 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0015_000001 container=Container: [ContainerId: container_1484710253544_0015_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 14:11:46,358 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:11:46,358 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:11:46,358 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0015_01_000001
2017-01-18 14:11:46,359 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:11:46,360 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0015_000001
2017-01-18 14:11:46,360 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0015 AttemptId: appattempt_1484710253544_0015_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0015_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 14:11:46,360 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0015_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 14:11:46,360 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0015_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 14:11:46,360 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0015_000001
2017-01-18 14:11:46,362 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0015_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0015_000001
2017-01-18 14:11:46,362 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0015_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 14:11:46,362 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0015_000001
2017-01-18 14:11:46,362 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0015_000001
2017-01-18 14:11:46,377 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484710253544_0015_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0015_000001
2017-01-18 14:11:46,377 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0015_000001 State change from ALLOCATED to LAUNCHED
2017-01-18 14:11:47,361 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:11:52,163 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484710253544_0015_000001 (auth:SIMPLE)
2017-01-18 14:11:52,173 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484710253544_0015_000001
2017-01-18 14:11:52,173 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484710253544_0015	APPATTEMPTID=appattempt_1484710253544_0015_000001
2017-01-18 14:11:52,174 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0015_000001 State change from LAUNCHED to RUNNING
2017-01-18 14:11:52,174 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0015 State change from ACCEPTED to RUNNING
2017-01-18 14:11:53,389 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000002 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:11:53,389 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0015	CONTAINERID=container_1484710253544_0015_01_000002
2017-01-18 14:11:53,389 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0015_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:11:53,389 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0015_000001 container=Container: [ContainerId: container_1484710253544_0015_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=NODE_LOCAL
2017-01-18 14:11:53,389 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:11:53,389 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:11:53,390 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000003 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:11:53,390 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0015	CONTAINERID=container_1484710253544_0015_01_000003
2017-01-18 14:11:53,390 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0015_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 3 containers, <memory:4096, vCores:3> used and <memory:4096, vCores:5> available after allocation
2017-01-18 14:11:53,390 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0015_000001 container=Container: [ContainerId: container_1484710253544_0015_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2 clusterResource=<memory:8192, vCores:8> type=NODE_LOCAL
2017-01-18 14:11:53,390 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:4096, vCores:3>, usedCapacity=0.5, absoluteUsedCapacity=0.5, numApps=1, numContainers=3
2017-01-18 14:11:53,390 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.5 absoluteUsedCapacity=0.5 used=<memory:4096, vCores:3> cluster=<memory:8192, vCores:8>
2017-01-18 14:11:54,231 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0015_01_000002
2017-01-18 14:11:54,233 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:11:54,235 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:11:55,253 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0015
2017-01-18 14:11:55,392 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000002 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:11:55,393 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000003 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:11:58,948 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000003 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:11:58,949 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0015_01_000003 in state: COMPLETED event:FINISHED
2017-01-18 14:11:58,949 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0015	CONTAINERID=container_1484710253544_0015_01_000003
2017-01-18 14:11:58,949 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0015_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available, release resources=true
2017-01-18 14:11:58,949 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:3072, vCores:2> numContainers=2 user=lybuestc user-resources=<memory:3072, vCores:2>
2017-01-18 14:11:58,949 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0015_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2 cluster=<memory:8192, vCores:8>
2017-01-18 14:11:58,949 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:11:58,949 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:11:58,949 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0015_000001 released container container_1484710253544_0015_01_000003 on node: host: 10.1.4.199:62182 #containers=2 available=<memory:5120, vCores:6> used=<memory:3072, vCores:2> with event: FINISHED
2017-01-18 14:11:59,013 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000002 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:11:59,013 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0015_01_000002 in state: COMPLETED event:FINISHED
2017-01-18 14:11:59,013 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0015	CONTAINERID=container_1484710253544_0015_01_000002
2017-01-18 14:11:59,013 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0015_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:11:59,013 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:11:59,015 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0015_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:11:59,015 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:11:59,015 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:11:59,015 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0015_000001 released container container_1484710253544_0015_01_000002 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:12:01,020 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000004 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:12:01,020 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0015	CONTAINERID=container_1484710253544_0015_01_000004
2017-01-18 14:12:01,020 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0015_01_000004 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:12:01,020 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0015_000001 container=Container: [ContainerId: container_1484710253544_0015_01_000004, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 14:12:01,020 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:12:01,020 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:12:01,284 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:12:02,031 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000004 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:12:02,291 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0015
2017-01-18 14:12:04,397 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000004 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:12:04,397 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0015_01_000004 in state: COMPLETED event:FINISHED
2017-01-18 14:12:04,397 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0015	CONTAINERID=container_1484710253544_0015_01_000004
2017-01-18 14:12:04,397 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0015_01_000004 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:12:04,397 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:12:04,398 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0015_01_000004, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:12:04,398 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:12:04,398 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:12:04,398 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0015_000001 released container container_1484710253544_0015_01_000004 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:12:04,941 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0015_000001 with final state: FINISHING, and exit status: -1000
2017-01-18 14:12:04,942 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0015_000001 State change from RUNNING to FINAL_SAVING
2017-01-18 14:12:04,942 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0015 with final state: FINISHING
2017-01-18 14:12:04,942 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0015 State change from RUNNING to FINAL_SAVING
2017-01-18 14:12:04,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0015
2017-01-18 14:12:04,942 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0015_000001 State change from FINAL_SAVING to FINISHING
2017-01-18 14:12:04,942 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0015 State change from FINAL_SAVING to FINISHING
2017-01-18 14:12:05,946 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484710253544_0015 unregistered successfully. 
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0015_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0015_000001
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0015_01_000001 in state: COMPLETED event:FINISHED
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0015_000001
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0015	CONTAINERID=container_1484710253544_0015_01_000001
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0015_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0015_000001 State change from FINISHING to FINISHED
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0015 State change from FINISHING to FINISHED
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0015_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0015
2017-01-18 14:12:11,166 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 14:12:11,167 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0015,name=word count mapreduce demo,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484710253544_0015/,appMasterHost=10.1.4.199,startTime=1484719905855,finishTime=1484719924942,finalStatus=SUCCEEDED,memorySeconds=65714,vcoreSeconds=37,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 14:12:11,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 14:12:11,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0015_000001 released container container_1484710253544_0015_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-18 14:12:11,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0015_000001 is done. finalState=FINISHED
2017-01-18 14:12:11,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0015 requests cleared
2017-01-18 14:12:11,167 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0015_000001
2017-01-18 14:12:11,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0015 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 14:12:11,168 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0015 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 14:12:13,174 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Null container completed...
2017-01-18 14:13:02,691 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 16
2017-01-18 14:13:03,633 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 16 submitted by user lybuestc
2017-01-18 14:13:03,633 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0016
2017-01-18 14:13:03,633 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0016
2017-01-18 14:13:03,633 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0016 State change from NEW to NEW_SAVING
2017-01-18 14:13:03,633 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0016
2017-01-18 14:13:03,633 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0016 State change from NEW_SAVING to SUBMITTED
2017-01-18 14:13:03,634 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0016 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 14:13:03,634 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0016 from user: lybuestc, in queue: default
2017-01-18 14:13:03,638 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0016 State change from SUBMITTED to ACCEPTED
2017-01-18 14:13:03,638 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0016_000001
2017-01-18 14:13:03,638 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0016_000001 State change from NEW to SUBMITTED
2017-01-18 14:13:03,638 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 14:13:03,638 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 14:13:03,638 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0016 from user: lybuestc activated in queue: default
2017-01-18 14:13:03,638 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0016 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@528102b, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 14:13:03,638 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0016_000001 to scheduler from user lybuestc in queue default
2017-01-18 14:13:03,640 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0016_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 14:13:04,336 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:13:04,336 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0016	CONTAINERID=container_1484710253544_0016_01_000001
2017-01-18 14:13:04,336 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0016_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 14:13:04,337 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0016_000001 container=Container: [ContainerId: container_1484710253544_0016_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 14:13:04,337 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:13:04,337 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:13:04,337 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0016_01_000001
2017-01-18 14:13:04,339 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:13:04,339 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0016_000001
2017-01-18 14:13:04,339 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0016 AttemptId: appattempt_1484710253544_0016_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0016_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 14:13:04,339 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0016_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 14:13:04,339 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0016_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 14:13:04,339 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0016_000001
2017-01-18 14:13:04,340 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0016_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0016_000001
2017-01-18 14:13:04,340 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0016_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 14:13:04,340 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0016_000001
2017-01-18 14:13:04,341 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0016_000001
2017-01-18 14:13:04,349 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484710253544_0016_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0016_000001
2017-01-18 14:13:04,349 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0016_000001 State change from ALLOCATED to LAUNCHED
2017-01-18 14:13:05,340 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:13:09,702 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484710253544_0016_000001 (auth:SIMPLE)
2017-01-18 14:13:09,706 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484710253544_0016_000001
2017-01-18 14:13:09,707 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=10.1.4.199	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484710253544_0016	APPATTEMPTID=appattempt_1484710253544_0016_000001
2017-01-18 14:13:09,707 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0016_000001 State change from LAUNCHED to RUNNING
2017-01-18 14:13:09,707 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0016 State change from ACCEPTED to RUNNING
2017-01-18 14:13:11,369 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000002 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:13:11,369 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0016	CONTAINERID=container_1484710253544_0016_01_000002
2017-01-18 14:13:11,369 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0016_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:13:11,369 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0016_000001 container=Container: [ContainerId: container_1484710253544_0016_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=NODE_LOCAL
2017-01-18 14:13:11,369 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:13:11,369 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:13:11,370 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000003 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:13:11,370 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0016	CONTAINERID=container_1484710253544_0016_01_000003
2017-01-18 14:13:11,370 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0016_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 3 containers, <memory:4096, vCores:3> used and <memory:4096, vCores:5> available after allocation
2017-01-18 14:13:11,370 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0016_000001 container=Container: [ContainerId: container_1484710253544_0016_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2 clusterResource=<memory:8192, vCores:8> type=NODE_LOCAL
2017-01-18 14:13:11,370 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:4096, vCores:3>, usedCapacity=0.5, absoluteUsedCapacity=0.5, numApps=1, numContainers=3
2017-01-18 14:13:11,370 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.5 absoluteUsedCapacity=0.5 used=<memory:4096, vCores:3> cluster=<memory:8192, vCores:8>
2017-01-18 14:13:11,764 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0016_01_000002
2017-01-18 14:13:11,764 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:13:11,765 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:13:12,370 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000002 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:13:12,371 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000003 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:13:12,779 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0016
2017-01-18 14:13:15,993 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000002 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:13:15,993 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0016_01_000002 in state: COMPLETED event:FINISHED
2017-01-18 14:13:15,993 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0016	CONTAINERID=container_1484710253544_0016_01_000002
2017-01-18 14:13:15,994 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0016_01_000002 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available, release resources=true
2017-01-18 14:13:15,994 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:3072, vCores:2> numContainers=2 user=lybuestc user-resources=<memory:3072, vCores:2>
2017-01-18 14:13:15,994 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0016_01_000002, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2 cluster=<memory:8192, vCores:8>
2017-01-18 14:13:15,994 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:13:15,994 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:13:15,994 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0016_000001 released container container_1484710253544_0016_01_000002 on node: host: 10.1.4.199:62182 #containers=2 available=<memory:5120, vCores:6> used=<memory:3072, vCores:2> with event: FINISHED
2017-01-18 14:13:16,159 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000003 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:13:16,159 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0016_01_000003 in state: COMPLETED event:FINISHED
2017-01-18 14:13:16,159 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0016	CONTAINERID=container_1484710253544_0016_01_000003
2017-01-18 14:13:16,159 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0016_01_000003 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:13:16,159 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:13:16,160 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0016_01_000003, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:13:16,160 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:13:16,160 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:13:16,160 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0016_000001 released container container_1484710253544_0016_01_000003 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:13:18,163 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000004 Container Transitioned from NEW to ALLOCATED
2017-01-18 14:13:18,163 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0016	CONTAINERID=container_1484710253544_0016_01_000004
2017-01-18 14:13:18,163 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0016_01_000004 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which has 2 containers, <memory:3072, vCores:2> used and <memory:5120, vCores:6> available after allocation
2017-01-18 14:13:18,164 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0016_000001 container=Container: [ContainerId: container_1484710253544_0016_01_000004, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 14:13:18,164 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:3072, vCores:2>, usedCapacity=0.375, absoluteUsedCapacity=0.375, numApps=1, numContainers=2
2017-01-18 14:13:18,164 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.375 absoluteUsedCapacity=0.375 used=<memory:3072, vCores:2> cluster=<memory:8192, vCores:8>
2017-01-18 14:13:18,821 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 14:13:19,164 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000004 Container Transitioned from ACQUIRED to RUNNING
2017-01-18 14:13:19,827 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1484710253544_0016
2017-01-18 14:13:21,815 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000004 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:13:21,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0016_01_000004 in state: COMPLETED event:FINISHED
2017-01-18 14:13:21,816 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0016	CONTAINERID=container_1484710253544_0016_01_000004
2017-01-18 14:13:21,816 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0016_01_000004 of capacity <memory:1024, vCores:1> on host 10.1.4.199:62182, which currently has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available, release resources=true
2017-01-18 14:13:21,816 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:2048, vCores:1> numContainers=1 user=lybuestc user-resources=<memory:2048, vCores:1>
2017-01-18 14:13:21,816 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0016_01_000004, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:1024, vCores:1>, Priority: 10, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1 cluster=<memory:8192, vCores:8>
2017-01-18 14:13:21,816 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 14:13:21,817 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 14:13:21,817 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0016_000001 released container container_1484710253544_0016_01_000004 on node: host: 10.1.4.199:62182 #containers=1 available=<memory:6144, vCores:7> used=<memory:2048, vCores:1> with event: FINISHED
2017-01-18 14:13:21,954 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0016_000001 with final state: FINISHING, and exit status: -1000
2017-01-18 14:13:21,955 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0016_000001 State change from RUNNING to FINAL_SAVING
2017-01-18 14:13:21,955 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0016 with final state: FINISHING
2017-01-18 14:13:21,955 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0016 State change from RUNNING to FINAL_SAVING
2017-01-18 14:13:21,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0016
2017-01-18 14:13:21,955 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0016_000001 State change from FINAL_SAVING to FINISHING
2017-01-18 14:13:21,955 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0016 State change from FINAL_SAVING to FINISHING
2017-01-18 14:13:22,962 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484710253544_0016 unregistered successfully. 
2017-01-18 14:13:28,144 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0016_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-18 14:13:28,144 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0016_000001
2017-01-18 14:13:28,144 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0016_01_000001 in state: COMPLETED event:FINISHED
2017-01-18 14:13:28,144 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0016	CONTAINERID=container_1484710253544_0016_01_000001
2017-01-18 14:13:28,144 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0016_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 14:13:28,144 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 14:13:28,144 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0016_000001
2017-01-18 14:13:28,144 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0016_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 14:13:28,144 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0016_000001 State change from FINISHING to FINISHED
2017-01-18 14:13:28,144 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 14:13:28,144 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0016 State change from FINISHING to FINISHED
2017-01-18 14:13:28,145 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 14:13:28,145 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0016_000001 released container container_1484710253544_0016_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-18 14:13:28,145 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0016_000001 is done. finalState=FINISHED
2017-01-18 14:13:28,145 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484710253544_0016
2017-01-18 14:13:28,145 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0016 requests cleared
2017-01-18 14:13:28,145 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0016 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 14:13:28,145 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0016,name=word count mapreduce demo,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484710253544_0016/,appMasterHost=10.1.4.199,startTime=1484719983633,finishTime=1484720001955,finalStatus=SUCCEEDED,memorySeconds=62134,vcoreSeconds=34,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 14:13:28,145 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0016 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 14:13:28,145 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484710253544_0016_000001
2017-01-18 15:45:54,268 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 17
2017-01-18 15:45:55,407 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 17 submitted by user lybuestc
2017-01-18 15:45:55,407 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0017
2017-01-18 15:45:55,407 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0017
2017-01-18 15:45:55,407 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0017 State change from NEW to NEW_SAVING
2017-01-18 15:45:55,407 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0017
2017-01-18 15:45:55,408 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0017 State change from NEW_SAVING to SUBMITTED
2017-01-18 15:45:55,408 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0017 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 15:45:55,408 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0017 from user: lybuestc, in queue: default
2017-01-18 15:45:55,417 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0017 State change from SUBMITTED to ACCEPTED
2017-01-18 15:45:55,417 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0017_000001
2017-01-18 15:45:55,417 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000001 State change from NEW to SUBMITTED
2017-01-18 15:45:55,417 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 15:45:55,417 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 15:45:55,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0017 from user: lybuestc activated in queue: default
2017-01-18 15:45:55,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0017 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@5e15c97a, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 15:45:55,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0017_000001 to scheduler from user lybuestc in queue default
2017-01-18 15:45:55,420 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 15:45:56,266 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0017_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 15:45:56,266 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0017	CONTAINERID=container_1484710253544_0017_01_000001
2017-01-18 15:45:56,266 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0017_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 15:45:56,266 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0017_000001 container=Container: [ContainerId: container_1484710253544_0017_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 15:45:56,266 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 15:45:56,266 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 15:45:56,267 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0017_01_000001
2017-01-18 15:45:56,268 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0017_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 15:45:56,268 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0017_000001
2017-01-18 15:45:56,268 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0017 AttemptId: appattempt_1484710253544_0017_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0017_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 15:45:56,268 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 15:45:56,268 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 15:45:56,268 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0017_000001
2017-01-18 15:45:56,270 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0017_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0017_000001
2017-01-18 15:45:56,270 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0017_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 15:45:56,270 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0017_000001
2017-01-18 15:45:56,270 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0017_000001
2017-01-18 15:55:21,712 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0017_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 15:55:21,713 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0017_000001 with final state: FAILED, and exit status: -1000
2017-01-18 15:55:21,713 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-18 15:55:21,713 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0017_000001
2017-01-18 15:55:21,713 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0017_000001
2017-01-18 15:55:21,713 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000001 State change from FINAL_SAVING to FAILED
2017-01-18 15:55:21,713 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2017-01-18 15:55:21,713 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0017_000001 is done. finalState=FAILED
2017-01-18 15:55:21,713 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0017_000002
2017-01-18 15:55:21,713 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000002 State change from NEW to SUBMITTED
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0017_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0017_01_000001 in state: KILLED event:KILL
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0017	CONTAINERID=container_1484710253544_0017_01_000001
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0017_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0017_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0017_000001 released container container_1484710253544_0017_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0017 requests cleared
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0017 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 15:55:21,714 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 15:55:21,714 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0017 from user: lybuestc activated in queue: default
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0017 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@41d5179b, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 15:55:21,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0017_000002 to scheduler from user lybuestc in queue default
2017-01-18 15:55:21,715 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000002 State change from SUBMITTED to SCHEDULED
2017-01-18 15:55:22,115 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0017_02_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 15:55:22,115 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0017	CONTAINERID=container_1484710253544_0017_02_000001
2017-01-18 15:55:22,115 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0017_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 15:55:22,116 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0017_000002 container=Container: [ContainerId: container_1484710253544_0017_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 15:55:22,116 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 15:55:22,116 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 15:55:22,116 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0017_02_000001
2017-01-18 15:55:22,118 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0017_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 15:55:22,118 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0017_000002
2017-01-18 15:55:22,118 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0017 AttemptId: appattempt_1484710253544_0017_000002 MasterContainer: Container: [ContainerId: container_1484710253544_0017_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 15:55:22,118 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000002 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 15:55:22,118 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000002 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 15:55:22,118 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0017_000002
2017-01-18 15:55:22,120 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0017_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0017_000002
2017-01-18 15:55:22,120 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0017_02_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 15:55:22,120 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0017_000002
2017-01-18 15:55:22,120 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0017_000002
2017-01-18 16:01:10,540 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8032: readAndProcess from client 172.17.45.96 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2017-01-18 16:01:10,619 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8031: readAndProcess from client 172.17.45.96 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2017-01-18 16:04:25,583 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0017_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 16:04:25,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0017_000002 with final state: FAILED, and exit status: -1000
2017-01-18 16:04:25,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000002 State change from ALLOCATED to FINAL_SAVING
2017-01-18 16:04:25,583 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0017_000002
2017-01-18 16:04:25,583 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0017_000002
2017-01-18 16:04:25,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0017_000002 State change from FINAL_SAVING to FAILED
2017-01-18 16:04:25,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 2. The max attempts is 2
2017-01-18 16:04:25,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0017 with final state: FAILED
2017-01-18 16:04:25,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0017 State change from ACCEPTED to FINAL_SAVING
2017-01-18 16:04:25,583 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0017_000002 is done. finalState=FAILED
2017-01-18 16:04:25,583 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0017
2017-01-18 16:04:25,584 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Application application_1484710253544_0017 failed 2 times due to Error launching appattempt_1484710253544_0017_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.
2017-01-18 16:04:25,584 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0017_02_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 16:04:25,584 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0017_02_000001 in state: KILLED event:KILL
2017-01-18 16:04:25,584 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0017 State change from FINAL_SAVING to FAILED
2017-01-18 16:04:25,584 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0017	CONTAINERID=container_1484710253544_0017_02_000001
2017-01-18 16:04:25,586 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0017_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 16:04:25,586 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Failed	TARGET=RMAppManager	RESULT=FAILURE	DESCRIPTION=App failed with state: FAILED	PERMISSIONS=Application application_1484710253544_0017 failed 2 times due to Error launching appattempt_1484710253544_0017_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.	APPID=application_1484710253544_0017
2017-01-18 16:04:25,586 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 16:04:25,586 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0017_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 16:04:25,586 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 16:04:25,586 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0017,name=word count mapreduce demo,user=lybuestc,queue=default,state=FAILED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0017,appMasterHost=N/A,startTime=1484725555407,finishTime=1484726665583,finalStatus=FAILED,memorySeconds=2271061,vcoreSeconds=1108,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 16:04:25,586 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 16:04:25,586 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0017_000002 released container container_1484710253544_0017_02_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 16:04:25,586 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0017 requests cleared
2017-01-18 16:04:25,586 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0017 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 16:04:25,586 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0017 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 16:09:19,725 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 18
2017-01-18 16:09:20,735 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 18 submitted by user lybuestc
2017-01-18 16:09:20,735 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0018
2017-01-18 16:09:20,735 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0018
2017-01-18 16:09:20,735 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0018 State change from NEW to NEW_SAVING
2017-01-18 16:09:20,735 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0018
2017-01-18 16:09:20,735 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0018 State change from NEW_SAVING to SUBMITTED
2017-01-18 16:09:20,736 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0018 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 16:09:20,736 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0018 from user: lybuestc, in queue: default
2017-01-18 16:09:20,748 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0018 State change from SUBMITTED to ACCEPTED
2017-01-18 16:09:20,748 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0018_000001
2017-01-18 16:09:20,748 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000001 State change from NEW to SUBMITTED
2017-01-18 16:09:20,749 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 16:09:20,749 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 16:09:20,749 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0018 from user: lybuestc activated in queue: default
2017-01-18 16:09:20,749 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0018 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@7c53c20f, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 16:09:20,749 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0018_000001 to scheduler from user lybuestc in queue default
2017-01-18 16:09:20,751 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 16:09:21,721 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0018_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 16:09:21,721 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0018	CONTAINERID=container_1484710253544_0018_01_000001
2017-01-18 16:09:21,721 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0018_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 16:09:21,721 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0018_000001 container=Container: [ContainerId: container_1484710253544_0018_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 16:09:21,721 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 16:09:21,721 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 16:09:21,721 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0018_01_000001
2017-01-18 16:09:21,722 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0018_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 16:09:21,723 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0018_000001
2017-01-18 16:09:21,723 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0018 AttemptId: appattempt_1484710253544_0018_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0018_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 16:09:21,723 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 16:09:21,723 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 16:09:21,723 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0018_000001
2017-01-18 16:09:21,725 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0018_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0018_000001
2017-01-18 16:09:21,725 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0018_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 16:09:21,725 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0018_000001
2017-01-18 16:09:21,725 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0018_000001
2017-01-18 16:18:23,198 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 19
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 19 submitted by user lybuestc
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0019
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0019
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0019 State change from NEW to NEW_SAVING
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0019
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0019 State change from NEW_SAVING to SUBMITTED
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0019 user: lybuestc leaf-queue of parent: root #applications: 2
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0019 from user: lybuestc, in queue: default
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0019 State change from SUBMITTED to ACCEPTED
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0019_000001
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000001 State change from NEW to SUBMITTED
2017-01-18 16:18:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 16:18:23,976 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0019 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@7c53c20f, leaf-queue: default #user-pending-applications: 1 #user-active-applications: 1 #queue-pending-applications: 1 #queue-active-applications: 1
2017-01-18 16:18:23,976 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0019_000001 to scheduler from user lybuestc in queue default
2017-01-18 16:18:23,976 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 16:18:45,086 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0018_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0018_000001 with final state: FAILED, and exit status: -1000
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0018_000001
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0018_000001
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000001 State change from FINAL_SAVING to FAILED
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0018_000002
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000002 State change from NEW to SUBMITTED
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0018_000001 is done. finalState=FAILED
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0018_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0018_01_000001 in state: KILLED event:KILL
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0018	CONTAINERID=container_1484710253544_0018_01_000001
2017-01-18 16:18:45,087 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0018_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0018_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0018_000001 released container container_1484710253544_0018_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0018 requests cleared
2017-01-18 16:18:45,088 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 16:18:45,088 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0019 from user: lybuestc activated in queue: default
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0018 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0018 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@7c53c20f, leaf-queue: default #user-pending-applications: 1 #user-active-applications: 1 #queue-pending-applications: 1 #queue-active-applications: 1
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0018_000002 to scheduler from user lybuestc in queue default
2017-01-18 16:18:45,088 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000002 State change from SUBMITTED to SCHEDULED
2017-01-18 16:18:45,487 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0019_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 16:18:45,487 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0019	CONTAINERID=container_1484710253544_0019_01_000001
2017-01-18 16:18:45,487 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0019_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 16:18:45,487 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0019_000001 container=Container: [ContainerId: container_1484710253544_0019_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 16:18:45,488 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=2, numContainers=1
2017-01-18 16:18:45,488 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 16:18:45,488 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0019_01_000001
2017-01-18 16:18:45,490 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0019_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 16:18:45,490 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0019_000001
2017-01-18 16:18:45,490 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0019 AttemptId: appattempt_1484710253544_0019_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0019_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 16:18:45,490 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 16:18:45,490 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 16:18:45,490 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0019_000001
2017-01-18 16:18:45,492 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0019_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0019_000001
2017-01-18 16:18:45,492 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0019_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 16:18:45,492 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0019_000001
2017-01-18 16:18:45,492 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0019_000001
2017-01-18 16:28:07,224 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0019_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0019_000001 with final state: FAILED, and exit status: -1000
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0019_000001
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0019_000001
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000001 State change from FINAL_SAVING to FAILED
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0019_000002
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000002 State change from NEW to SUBMITTED
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0019_000001 is done. finalState=FAILED
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0019_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0019_01_000001 in state: KILLED event:KILL
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0019	CONTAINERID=container_1484710253544_0019_01_000001
2017-01-18 16:28:07,225 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0019_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0019_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0019_000001 released container container_1484710253544_0019_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0019 requests cleared
2017-01-18 16:28:07,226 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 16:28:07,226 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0018 from user: lybuestc activated in queue: default
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0019 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: not starting application as amIfStarted exceeds amLimit
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0019 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@7c53c20f, leaf-queue: default #user-pending-applications: 1 #user-active-applications: 1 #queue-pending-applications: 1 #queue-active-applications: 1
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0019_000002 to scheduler from user lybuestc in queue default
2017-01-18 16:28:07,226 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000002 State change from SUBMITTED to SCHEDULED
2017-01-18 16:28:07,318 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0018_02_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 16:28:07,318 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0018	CONTAINERID=container_1484710253544_0018_02_000001
2017-01-18 16:28:07,318 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0018_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 16:28:07,318 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0018_000002 container=Container: [ContainerId: container_1484710253544_0018_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 16:28:07,318 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=2, numContainers=1
2017-01-18 16:28:07,318 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 16:28:07,319 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0018_02_000001
2017-01-18 16:28:07,320 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0018_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 16:28:07,320 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0018_000002
2017-01-18 16:28:07,320 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0018 AttemptId: appattempt_1484710253544_0018_000002 MasterContainer: Container: [ContainerId: container_1484710253544_0018_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 16:28:07,320 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000002 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 16:28:07,320 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000002 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 16:28:07,321 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0018_000002
2017-01-18 16:28:07,322 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0018_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0018_000002
2017-01-18 16:28:07,322 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0018_02_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 16:28:07,322 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0018_000002
2017-01-18 16:28:07,322 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0018_000002
2017-01-18 16:31:11,848 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8031: readAndProcess from client 10.1.4.199 threw exception [java.io.IOException: Operation timed out]
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2017-01-18 16:37:27,582 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0018_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 16:37:27,582 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0018_000002 with final state: FAILED, and exit status: -1000
2017-01-18 16:37:27,582 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000002 State change from ALLOCATED to FINAL_SAVING
2017-01-18 16:37:27,582 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0018_000002
2017-01-18 16:37:27,582 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0018_000002
2017-01-18 16:37:27,582 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0018_000002 State change from FINAL_SAVING to FAILED
2017-01-18 16:37:27,582 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 2. The max attempts is 2
2017-01-18 16:37:27,582 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0018 with final state: FAILED
2017-01-18 16:37:27,582 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0018 State change from ACCEPTED to FINAL_SAVING
2017-01-18 16:37:27,582 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0018
2017-01-18 16:37:27,582 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0018_000002 is done. finalState=FAILED
2017-01-18 16:37:27,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Application application_1484710253544_0018 failed 2 times due to Error launching appattempt_1484710253544_0018_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.
2017-01-18 16:37:27,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0018_02_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 16:37:27,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0018 State change from FINAL_SAVING to FAILED
2017-01-18 16:37:27,583 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0018_02_000001 in state: KILLED event:KILL
2017-01-18 16:37:27,583 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0018	CONTAINERID=container_1484710253544_0018_02_000001
2017-01-18 16:37:27,583 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Failed	TARGET=RMAppManager	RESULT=FAILURE	DESCRIPTION=App failed with state: FAILED	PERMISSIONS=Application application_1484710253544_0018 failed 2 times due to Error launching appattempt_1484710253544_0018_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.	APPID=application_1484710253544_0018
2017-01-18 16:37:27,583 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0018_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 16:37:27,583 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0018,name=word count mapreduce demo,user=lybuestc,queue=default,state=FAILED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0018,appMasterHost=N/A,startTime=1484726960735,finishTime=1484728647582,finalStatus=FAILED,memorySeconds=2301195,vcoreSeconds=1123,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 16:37:27,583 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 16:37:27,584 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0018_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 16:37:27,584 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 16:37:27,584 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=2, numContainers=0
2017-01-18 16:37:27,584 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0018_000002 released container container_1484710253544_0018_02_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 16:37:27,584 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0018 requests cleared
2017-01-18 16:37:27,584 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 16:37:27,584 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 16:37:27,584 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0019 from user: lybuestc activated in queue: default
2017-01-18 16:37:27,584 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0018 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 16:37:27,584 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0018 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 16:37:28,205 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0019_02_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 16:37:28,205 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0019	CONTAINERID=container_1484710253544_0019_02_000001
2017-01-18 16:37:28,205 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0019_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 16:37:28,205 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0019_000002 container=Container: [ContainerId: container_1484710253544_0019_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 16:37:28,205 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 16:37:28,205 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 16:37:28,205 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0019_02_000001
2017-01-18 16:37:28,206 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0019_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 16:37:28,207 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0019_000002
2017-01-18 16:37:28,207 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0019 AttemptId: appattempt_1484710253544_0019_000002 MasterContainer: Container: [ContainerId: container_1484710253544_0019_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 16:37:28,207 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000002 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 16:37:28,207 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000002 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 16:37:28,207 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0019_000002
2017-01-18 16:37:28,209 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0019_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0019_000002
2017-01-18 16:37:28,209 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0019_02_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 16:37:28,209 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0019_000002
2017-01-18 16:37:28,209 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0019_000002
2017-01-18 16:45:08,294 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0013 which is the app master GUI of application_1484710253544_0013 owned by lybuestc
2017-01-18 16:46:48,017 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0019_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 16:46:48,017 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0019_000002 with final state: FAILED, and exit status: -1000
2017-01-18 16:46:48,017 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000002 State change from ALLOCATED to FINAL_SAVING
2017-01-18 16:46:48,018 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0019_000002
2017-01-18 16:46:48,018 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0019_000002
2017-01-18 16:46:48,018 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0019_000002 State change from FINAL_SAVING to FAILED
2017-01-18 16:46:48,018 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 2. The max attempts is 2
2017-01-18 16:46:48,018 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0019 with final state: FAILED
2017-01-18 16:46:48,018 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0019 State change from ACCEPTED to FINAL_SAVING
2017-01-18 16:46:48,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0019
2017-01-18 16:46:48,019 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0019_000002 is done. finalState=FAILED
2017-01-18 16:46:48,021 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0019_02_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 16:46:48,021 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0019_02_000001 in state: KILLED event:KILL
2017-01-18 16:46:48,021 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0019	CONTAINERID=container_1484710253544_0019_02_000001
2017-01-18 16:46:48,021 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0019_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 16:46:48,021 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 16:46:48,021 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0019_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 16:46:48,021 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 16:46:48,021 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 16:46:48,021 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Application application_1484710253544_0019 failed 2 times due to Error launching appattempt_1484710253544_0019_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.
2017-01-18 16:46:48,023 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0019 State change from FINAL_SAVING to FAILED
2017-01-18 16:46:48,023 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Failed	TARGET=RMAppManager	RESULT=FAILURE	DESCRIPTION=App failed with state: FAILED	PERMISSIONS=Application application_1484710253544_0019 failed 2 times due to Error launching appattempt_1484710253544_0019_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.	APPID=application_1484710253544_0019
2017-01-18 16:46:48,023 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0019,name=word count,user=lybuestc,queue=default,state=FAILED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0019,appMasterHost=N/A,startTime=1484727503974,finishTime=1484729208018,finalStatus=FAILED,memorySeconds=2296942,vcoreSeconds=1120,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 16:46:48,021 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0019_000002 released container container_1484710253544_0019_02_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 16:46:48,026 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0019 requests cleared
2017-01-18 16:46:48,026 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0019 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 16:46:48,026 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0019 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 21:57:51,196 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 20
2017-01-18 21:57:52,031 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 20 submitted by user lybuestc
2017-01-18 21:57:52,031 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484710253544_0020
2017-01-18 21:57:52,031 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484710253544_0020
2017-01-18 21:57:52,031 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0020 State change from NEW to NEW_SAVING
2017-01-18 21:57:52,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484710253544_0020
2017-01-18 21:57:52,032 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0020 State change from NEW_SAVING to SUBMITTED
2017-01-18 21:57:52,032 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484710253544_0020 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-18 21:57:52,032 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484710253544_0020 from user: lybuestc, in queue: default
2017-01-18 21:57:52,036 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0020 State change from SUBMITTED to ACCEPTED
2017-01-18 21:57:52,036 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0020_000001
2017-01-18 21:57:52,037 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000001 State change from NEW to SUBMITTED
2017-01-18 21:57:52,038 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 21:57:52,038 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 21:57:52,038 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0020 from user: lybuestc activated in queue: default
2017-01-18 21:57:52,038 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0020 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@393f2ce6, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 21:57:52,038 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0020_000001 to scheduler from user lybuestc in queue default
2017-01-18 21:57:52,040 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000001 State change from SUBMITTED to SCHEDULED
2017-01-18 21:57:52,863 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0020_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 21:57:52,863 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0020	CONTAINERID=container_1484710253544_0020_01_000001
2017-01-18 21:57:52,863 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0020_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 21:57:52,863 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0020_000001 container=Container: [ContainerId: container_1484710253544_0020_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 21:57:52,863 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 21:57:52,863 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 21:57:52,867 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0020_01_000001
2017-01-18 21:57:52,871 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0020_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 21:57:52,871 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0020_000001
2017-01-18 21:57:52,872 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0020 AttemptId: appattempt_1484710253544_0020_000001 MasterContainer: Container: [ContainerId: container_1484710253544_0020_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 21:57:52,872 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 21:57:52,872 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 21:57:52,877 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0020_000001
2017-01-18 21:57:52,881 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0020_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0020_000001
2017-01-18 21:57:52,882 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0020_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 21:57:52,882 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0020_000001
2017-01-18 21:57:52,882 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0020_000001
2017-01-18 22:07:13,045 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0020_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/192.168.0.100 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 22:07:13,045 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0020_000001 with final state: FAILED, and exit status: -1000
2017-01-18 22:07:13,045 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-18 22:07:13,045 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0020_000001
2017-01-18 22:07:13,045 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0020_000001
2017-01-18 22:07:13,045 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000001 State change from FINAL_SAVING to FAILED
2017-01-18 22:07:13,045 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2017-01-18 22:07:13,046 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484710253544_0020_000002
2017-01-18 22:07:13,046 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000002 State change from NEW to SUBMITTED
2017-01-18 22:07:13,046 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0020_000001 is done. finalState=FAILED
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0020_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0020_01_000001 in state: KILLED event:KILL
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0020	CONTAINERID=container_1484710253544_0020_01_000001
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0020_01_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0020_01_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0020_000001 released container container_1484710253544_0020_01_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0020 requests cleared
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0020 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 22:07:13,050 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 22:07:13,050 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484710253544_0020 from user: lybuestc activated in queue: default
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484710253544_0020 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@2011d3c4, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-18 22:07:13,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484710253544_0020_000002 to scheduler from user lybuestc in queue default
2017-01-18 22:07:13,051 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000002 State change from SUBMITTED to SCHEDULED
2017-01-18 22:07:13,497 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0020_02_000001 Container Transitioned from NEW to ALLOCATED
2017-01-18 22:07:13,498 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0020	CONTAINERID=container_1484710253544_0020_02_000001
2017-01-18 22:07:13,498 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484710253544_0020_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-18 22:07:13,498 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484710253544_0020_000002 container=Container: [ContainerId: container_1484710253544_0020_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-18 22:07:13,498 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-18 22:07:13,498 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-18 22:07:13,499 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 10.1.4.199:62182 for container : container_1484710253544_0020_02_000001
2017-01-18 22:07:13,500 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0020_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-18 22:07:13,501 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484710253544_0020_000002
2017-01-18 22:07:13,501 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484710253544_0020 AttemptId: appattempt_1484710253544_0020_000002 MasterContainer: Container: [ContainerId: container_1484710253544_0020_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ]
2017-01-18 22:07:13,501 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000002 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-18 22:07:13,501 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000002 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-18 22:07:13,502 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484710253544_0020_000002
2017-01-18 22:07:13,503 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484710253544_0020_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] for AM appattempt_1484710253544_0020_000002
2017-01-18 22:07:13,503 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484710253544_0020_02_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-18 22:07:13,503 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484710253544_0020_000002
2017-01-18 22:07:13,504 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484710253544_0020_000002
2017-01-18 22:16:33,659 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484710253544_0020_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/192.168.0.100 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-18 22:16:33,660 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484710253544_0020_000002 with final state: FAILED, and exit status: -1000
2017-01-18 22:16:33,660 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000002 State change from ALLOCATED to FINAL_SAVING
2017-01-18 22:16:33,660 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484710253544_0020_000002
2017-01-18 22:16:33,660 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484710253544_0020_000002
2017-01-18 22:16:33,660 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484710253544_0020_000002 State change from FINAL_SAVING to FAILED
2017-01-18 22:16:33,660 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 2. The max attempts is 2
2017-01-18 22:16:33,660 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484710253544_0020 with final state: FAILED
2017-01-18 22:16:33,660 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0020 State change from ACCEPTED to FINAL_SAVING
2017-01-18 22:16:33,660 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484710253544_0020
2017-01-18 22:16:33,661 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484710253544_0020_000002 is done. finalState=FAILED
2017-01-18 22:16:33,661 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Application application_1484710253544_0020 failed 2 times due to Error launching appattempt_1484710253544_0020_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/192.168.0.100 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.
2017-01-18 22:16:33,661 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484710253544_0020_02_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-18 22:16:33,661 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484710253544_0020_02_000001 in state: KILLED event:KILL
2017-01-18 22:16:33,661 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484710253544_0020	CONTAINERID=container_1484710253544_0020_02_000001
2017-01-18 22:16:33,661 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484710253544_0020 State change from FINAL_SAVING to FAILED
2017-01-18 22:16:33,662 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484710253544_0020_02_000001 of capacity <memory:2048, vCores:1> on host 10.1.4.199:62182, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-18 22:16:33,662 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-18 22:16:33,662 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Failed	TARGET=RMAppManager	RESULT=FAILURE	DESCRIPTION=App failed with state: FAILED	PERMISSIONS=Application application_1484710253544_0020 failed 2 times due to Error launching appattempt_1484710253544_0020_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/192.168.0.100 to 10.1.4.199:62182 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor61.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=10.1.4.199/10.1.4.199:62182]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.	APPID=application_1484710253544_0020
2017-01-18 22:16:33,663 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484710253544_0020_02_000001, NodeId: 10.1.4.199:62182, NodeHttpAddress: 10.1.4.199:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.1.4.199:62182 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-18 22:16:33,663 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-18 22:16:33,663 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-18 22:16:33,663 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484710253544_0020_000002 released container container_1484710253544_0020_02_000001 on node: host: 10.1.4.199:62182 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-18 22:16:33,663 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484710253544_0020 requests cleared
2017-01-18 22:16:33,663 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484710253544_0020,name=word count,user=lybuestc,queue=default,state=FAILED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484710253544_0020,appMasterHost=N/A,startTime=1484747872030,finishTime=1484748993660,finalStatus=FAILED,memorySeconds=2294480,vcoreSeconds=1120,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-18 22:16:33,663 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484710253544_0020 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-18 22:16:33,664 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484710253544_0020 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-18 23:06:29,400 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8031: readAndProcess from client 172.17.45.96 threw exception [java.io.IOException: Operation timed out]
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2017-01-18 23:51:47,617 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8031: readAndProcess from client 10.1.4.199 threw exception [java.io.IOException: Operation timed out]
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2017-01-18 23:53:29,247 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8031: readAndProcess from client 172.17.45.96 threw exception [java.io.IOException: Operation timed out]
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2017-01-19 00:51:22,748 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0016 which is the app master GUI of application_1484710253544_0016 owned by lybuestc
2017-01-19 00:51:23,998 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0016 which is the app master GUI of application_1484710253544_0016 owned by lybuestc
2017-01-19 00:51:24,868 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0016 which is the app master GUI of application_1484710253544_0016 owned by lybuestc
2017-01-19 00:51:25,832 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://10.1.4.199:19888/jobhistory/job/job_1484710253544_0015 which is the app master GUI of application_1484710253544_0015 owned by lybuestc
2017-01-19 11:30:55,945 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-19 11:30:55,945 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-19 11:30:55,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 3
2017-01-19 11:30:55,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2017-01-19 12:17:13,817 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8031: readAndProcess from client 192.168.0.100 threw exception [java.io.IOException: Operation timed out]
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2017-01-19 15:18:26,525 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8031: readAndProcess from client 172.17.45.96 threw exception [java.io.IOException: Connection reset by peer]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2017-01-19 16:54:24,717 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8031: readAndProcess from client 172.16.178.1 threw exception [java.io.IOException: Operation timed out]
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2017-01-20 00:47:48,673 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8031: readAndProcess from client 172.17.45.96 threw exception [java.io.IOException: Operation timed out]
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
2017-01-20 01:04:06,906 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: RECEIVED SIGNAL 15: SIGTERM
2017-01-20 01:04:06,926 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2017-01-20 01:04:06,930 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2017-01-20 01:04:06,932 INFO org.apache.hadoop.ipc.Server: Stopping server on 8032
2017-01-20 01:04:06,939 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8032
2017-01-20 01:04:06,939 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:04:06,940 INFO org.apache.hadoop.ipc.Server: Stopping server on 8033
2017-01-20 01:04:06,946 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8033
2017-01-20 01:04:06,946 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:04:06,948 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to standby state
2017-01-20 01:04:06,948 WARN org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher: org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher$LauncherThread interrupted. Returning.
2017-01-20 01:04:06,949 INFO org.apache.hadoop.ipc.Server: Stopping server on 8030
2017-01-20 01:04:06,957 INFO org.apache.hadoop.ipc.Server: Stopping server on 8031
2017-01-20 01:04:06,958 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8030
2017-01-20 01:04:06,958 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:04:06,976 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8031
2017-01-20 01:04:06,977 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: NMLivelinessMonitor thread interrupted
2017-01-20 01:04:06,977 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Returning, interrupted : java.lang.InterruptedException
2017-01-20 01:04:06,978 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, igonring any new events.
2017-01-20 01:04:06,978 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:04:06,979 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2017-01-20 01:04:06,979 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2017-01-20 01:04:06,980 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer thread interrupted
2017-01-20 01:04:06,980 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2017-01-20 01:04:06,981 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ResourceManager metrics system...
2017-01-20 01:04:06,982 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system stopped.
2017-01-20 01:04:06,982 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.
2017-01-20 01:04:06,983 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, igonring any new events.
2017-01-20 01:04:06,984 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to standby state
2017-01-20 01:04:06,984 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down ResourceManager at chengguan-3.local/10.1.4.199
************************************************************/
2017-01-20 01:05:05,370 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = chengguan-3.local/192.168.0.100
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/rm-config/log4j.properties
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_91
************************************************************/
2017-01-20 01:05:05,379 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-20 01:05:05,688 INFO org.apache.hadoop.conf.Configuration: found resource core-site.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/core-site.xml
2017-01-20 01:05:05,735 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-01-20 01:05:05,763 INFO org.apache.hadoop.security.Groups: clearing userToGroupsMap cache
2017-01-20 01:05:05,852 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/yarn-site.xml
2017-01-20 01:05:06,161 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2017-01-20 01:05:06,418 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2017-01-20 01:05:06,438 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2017-01-20 01:05:06,447 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2017-01-20 01:05:06,520 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2017-01-20 01:05:06,523 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2017-01-20 01:05:06,523 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2017-01-20 01:05:06,560 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher
2017-01-20 01:05:06,562 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2017-01-20 01:05:06,563 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2017-01-20 01:05:06,564 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2017-01-20 01:05:06,686 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-20 01:05:06,798 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-20 01:05:06,798 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2017-01-20 01:05:06,819 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2017-01-20 01:05:06,831 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2017-01-20 01:05:06,833 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2017-01-20 01:05:06,837 INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instiantiated.
2017-01-20 01:05:06,838 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2017-01-20 01:05:06,839 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/capacity-scheduler.xml
2017-01-20 01:05:06,916 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2017-01-20 01:05:06,916 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2017-01-20 01:05:06,922 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, asboluteCapacity=1.0, maxCapacity=1.0, asboluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
, reservationsContinueLooking=true
2017-01-20 01:05:06,922 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2017-01-20 01:05:06,932 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2017-01-20 01:05:06,932 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2017-01-20 01:05:06,934 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
asboluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:8192, vCores:32> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
nodeLocalityDelay = 40
labels=*,
nodeLocalityDelay = 40
reservationsContinueLooking = true
preemptionDisabled = true

2017-01-20 01:05:06,934 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0
2017-01-20 01:05:06,935 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2017-01-20 01:05:06,936 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2017-01-20 01:05:06,936 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue mappings, override: false
2017-01-20 01:05:06,936 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:32>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2017-01-20 01:05:06,951 INFO org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher: YARN system metrics publishing service is not enabled
2017-01-20 01:05:06,951 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state
2017-01-20 01:05:06,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2017-01-20 01:05:06,978 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2017-01-20 01:05:06,978 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2017-01-20 01:05:06,978 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:05:06,979 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 1
2017-01-20 01:05:06,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2017-01-20 01:05:06,980 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2017-01-20 01:05:06,981 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:05:06,981 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 2
2017-01-20 01:05:06,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2017-01-20 01:05:06,983 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
2017-01-20 01:05:07,025 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:05:07,067 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8031
2017-01-20 01:05:07,085 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
2017-01-20 01:05:07,086 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:05:07,086 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8031: starting
2017-01-20 01:05:07,173 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:05:07,190 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8030
2017-01-20 01:05:07,201 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
2017-01-20 01:05:07,201 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:05:07,201 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8030: starting
2017-01-20 01:05:07,302 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:05:07,303 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8032
2017-01-20 01:05:07,306 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
2017-01-20 01:05:07,307 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:05:07,307 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8032: starting
2017-01-20 01:05:07,324 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
2017-01-20 01:05:07,487 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-20 01:05:07,504 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-20 01:05:07,509 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2017-01-20 01:05:07,518 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-20 01:05:07,524 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
2017-01-20 01:05:07,524 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
2017-01-20 01:05:07,524 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
2017-01-20 01:05:07,525 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2017-01-20 01:05:07,525 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-20 01:05:07,525 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-20 01:05:07,529 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /cluster/*
2017-01-20 01:05:07,531 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2017-01-20 01:05:08,705 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2017-01-20 01:05:08,709 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8088
2017-01-20 01:05:08,709 INFO org.mortbay.log: jetty-6.1.26
2017-01-20 01:05:08,746 INFO org.mortbay.log: Extract jar:file:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar!/webapps/cluster to /var/folders/xl/5zp3ynpd6l5cg_y4f6qjqxxm0000gn/T/Jetty_0_0_0_0_8088_cluster____u0rgz3/webapp
2017-01-20 01:05:08,972 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:05:08,972 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2017-01-20 01:05:08,972 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:05:10,894 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2017-01-20 01:05:10,895 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at 8088
2017-01-20 01:05:11,065 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:05:11,065 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2017-01-20 01:05:11,068 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2017-01-20 01:05:11,068 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:05:11,070 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8033: starting
2017-01-20 01:05:12,205 INFO org.apache.hadoop.yarn.util.RackResolver: Resolved 192.168.0.100 to /default-rack
2017-01-20 01:05:12,207 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node 192.168.0.100(cmPort: 61207 httpPort: 8042) registered with capability: <memory:8192, vCores:8>, assigned nodeId 192.168.0.100:61207
2017-01-20 01:05:12,213 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: 192.168.0.100:61207 Node Transitioned from NEW to RUNNING
2017-01-20 01:05:12,217 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node 192.168.0.100:61207 clusterResource: <memory:8192, vCores:8>
2017-01-20 01:09:52,112 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 1
2017-01-20 01:09:53,648 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 1 submitted by user lybuestc
2017-01-20 01:09:53,649 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484845506952_0001
2017-01-20 01:09:53,650 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484845506952_0001
2017-01-20 01:09:53,659 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0001 State change from NEW to NEW_SAVING
2017-01-20 01:09:53,659 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484845506952_0001
2017-01-20 01:09:53,660 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0001 State change from NEW_SAVING to SUBMITTED
2017-01-20 01:09:53,663 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484845506952_0001 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-20 01:09:53,664 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484845506952_0001 from user: lybuestc, in queue: default
2017-01-20 01:09:53,699 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0001 State change from SUBMITTED to ACCEPTED
2017-01-20 01:09:53,733 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484845506952_0001_000001
2017-01-20 01:09:53,734 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0001_000001 State change from NEW to SUBMITTED
2017-01-20 01:09:53,747 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:09:53,747 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:09:53,747 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484845506952_0001 from user: lybuestc activated in queue: default
2017-01-20 01:09:53,747 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484845506952_0001 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@67a8989d, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 01:09:53,747 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484845506952_0001_000001 to scheduler from user lybuestc in queue default
2017-01-20 01:09:53,750 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0001_000001 State change from SUBMITTED to SCHEDULED
2017-01-20 01:09:54,695 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0001_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 01:09:54,695 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0001	CONTAINERID=container_1484845506952_0001_01_000001
2017-01-20 01:09:54,695 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484845506952_0001_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 01:09:54,695 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484845506952_0001_000001 container=Container: [ContainerId: container_1484845506952_0001_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 01:09:54,696 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 01:09:54,696 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 01:09:54,711 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:61207 for container : container_1484845506952_0001_01_000001
2017-01-20 01:09:54,723 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 01:09:54,723 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484845506952_0001_000001
2017-01-20 01:09:54,727 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484845506952_0001 AttemptId: appattempt_1484845506952_0001_000001 MasterContainer: Container: [ContainerId: container_1484845506952_0001_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ]
2017-01-20 01:09:54,746 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 01:09:54,748 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 01:09:54,751 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484845506952_0001_000001
2017-01-20 01:09:54,794 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484845506952_0001_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0001_000001
2017-01-20 01:09:54,794 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484845506952_0001_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 01:09:54,796 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484845506952_0001_000001
2017-01-20 01:09:54,801 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484845506952_0001_000001
2017-01-20 01:09:55,461 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484845506952_0001_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0001_000001
2017-01-20 01:09:55,462 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0001_000001 State change from ALLOCATED to LAUNCHED
2017-01-20 01:09:55,696 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-20 01:10:02,535 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484845506952_0001_000001 (auth:SIMPLE)
2017-01-20 01:10:02,545 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484845506952_0001_000001
2017-01-20 01:10:02,546 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484845506952_0001	APPATTEMPTID=appattempt_1484845506952_0001_000001
2017-01-20 01:10:02,546 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0001_000001 State change from LAUNCHED to RUNNING
2017-01-20 01:10:02,546 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0001 State change from ACCEPTED to RUNNING
2017-01-20 01:10:04,854 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484845506952_0001_000001 with final state: FINISHING, and exit status: -1000
2017-01-20 01:10:04,855 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0001_000001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:10:04,855 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484845506952_0001 with final state: FINISHING
2017-01-20 01:10:04,856 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:10:04,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484845506952_0001
2017-01-20 01:10:04,856 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0001_000001 State change from FINAL_SAVING to FINISHING
2017-01-20 01:10:04,856 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0001 State change from FINAL_SAVING to FINISHING
2017-01-20 01:10:05,836 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484845506952_0001 unregistered successfully. 
2017-01-20 01:10:11,012 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0001_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-20 01:10:11,012 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484845506952_0001_01_000001 in state: COMPLETED event:FINISHED
2017-01-20 01:10:11,012 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0001	CONTAINERID=container_1484845506952_0001_01_000001
2017-01-20 01:10:11,012 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484845506952_0001_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 01:10:11,012 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 01:10:11,013 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484845506952_0001_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 01:10:11,013 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 01:10:11,013 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 01:10:11,013 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484845506952_0001_000001 released container container_1484845506952_0001_01_000001 on node: host: 192.168.0.100:61207 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-20 01:10:11,014 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484845506952_0001_000001
2017-01-20 01:10:11,017 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484845506952_0001_000001
2017-01-20 01:10:11,018 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0001_000001 State change from FINISHING to FINISHED
2017-01-20 01:10:11,022 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0001 State change from FINISHING to FINISHED
2017-01-20 01:10:11,023 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484845506952_0001
2017-01-20 01:10:11,023 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484845506952_0001_000001 is done. finalState=FINISHED
2017-01-20 01:10:11,023 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484845506952_0001 requests cleared
2017-01-20 01:10:11,024 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484845506952_0001 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 01:10:11,024 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484845506952_0001 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-20 01:10:11,025 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484845506952_0001_000001
2017-01-20 01:10:11,032 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484845506952_0001,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484845506952_0001/,appMasterHost=192.168.0.100,startTime=1484845793647,finishTime=1484845804855,finalStatus=SUCCEEDED,memorySeconds=33417,vcoreSeconds=16,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-20 01:10:47,532 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 2
2017-01-20 01:10:48,349 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 2 submitted by user lybuestc
2017-01-20 01:10:48,349 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484845506952_0002
2017-01-20 01:10:48,349 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484845506952_0002
2017-01-20 01:10:48,350 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0002 State change from NEW to NEW_SAVING
2017-01-20 01:10:48,350 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484845506952_0002
2017-01-20 01:10:48,350 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0002 State change from NEW_SAVING to SUBMITTED
2017-01-20 01:10:48,350 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484845506952_0002 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-20 01:10:48,350 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484845506952_0002 from user: lybuestc, in queue: default
2017-01-20 01:10:48,373 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0002 State change from SUBMITTED to ACCEPTED
2017-01-20 01:10:48,373 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484845506952_0002_000001
2017-01-20 01:10:48,374 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0002_000001 State change from NEW to SUBMITTED
2017-01-20 01:10:48,374 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:10:48,374 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:10:48,375 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484845506952_0002 from user: lybuestc activated in queue: default
2017-01-20 01:10:48,375 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484845506952_0002 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@75d074bd, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 01:10:48,375 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484845506952_0002_000001 to scheduler from user lybuestc in queue default
2017-01-20 01:10:48,376 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0002_000001 State change from SUBMITTED to SCHEDULED
2017-01-20 01:10:49,187 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0002_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 01:10:49,187 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0002	CONTAINERID=container_1484845506952_0002_01_000001
2017-01-20 01:10:49,188 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484845506952_0002_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 01:10:49,188 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484845506952_0002_000001 container=Container: [ContainerId: container_1484845506952_0002_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 01:10:49,188 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 01:10:49,188 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 01:10:49,189 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:61207 for container : container_1484845506952_0002_01_000001
2017-01-20 01:10:49,194 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0002_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 01:10:49,194 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484845506952_0002_000001
2017-01-20 01:10:49,194 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484845506952_0002 AttemptId: appattempt_1484845506952_0002_000001 MasterContainer: Container: [ContainerId: container_1484845506952_0002_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ]
2017-01-20 01:10:49,194 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0002_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 01:10:49,195 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0002_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 01:10:49,195 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484845506952_0002_000001
2017-01-20 01:10:49,198 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484845506952_0002_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0002_000001
2017-01-20 01:10:49,198 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484845506952_0002_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 01:10:49,198 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484845506952_0002_000001
2017-01-20 01:10:49,198 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484845506952_0002_000001
2017-01-20 01:10:49,213 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484845506952_0002_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0002_000001
2017-01-20 01:10:49,213 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0002_000001 State change from ALLOCATED to LAUNCHED
2017-01-20 01:10:50,197 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0002_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-20 01:10:55,093 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484845506952_0002_000001 (auth:SIMPLE)
2017-01-20 01:10:55,098 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484845506952_0002_000001
2017-01-20 01:10:55,098 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484845506952_0002	APPATTEMPTID=appattempt_1484845506952_0002_000001
2017-01-20 01:10:55,098 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0002_000001 State change from LAUNCHED to RUNNING
2017-01-20 01:10:55,098 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0002 State change from ACCEPTED to RUNNING
2017-01-20 01:10:57,120 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484845506952_0002_000001 with final state: FINISHING, and exit status: -1000
2017-01-20 01:10:57,120 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0002_000001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:10:57,120 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484845506952_0002 with final state: FINISHING
2017-01-20 01:10:57,120 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0002 State change from RUNNING to FINAL_SAVING
2017-01-20 01:10:57,120 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0002_000001 State change from FINAL_SAVING to FINISHING
2017-01-20 01:10:57,120 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484845506952_0002
2017-01-20 01:10:57,120 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0002 State change from FINAL_SAVING to FINISHING
2017-01-20 01:10:58,125 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484845506952_0002 unregistered successfully. 
2017-01-20 01:11:03,379 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0002_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-20 01:11:03,379 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484845506952_0002_01_000001 in state: COMPLETED event:FINISHED
2017-01-20 01:11:03,380 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0002	CONTAINERID=container_1484845506952_0002_01_000001
2017-01-20 01:11:03,380 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484845506952_0002_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 01:11:03,380 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 01:11:03,380 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484845506952_0002_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 01:11:03,380 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 01:11:03,380 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 01:11:03,380 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484845506952_0002_000001 released container container_1484845506952_0002_01_000001 on node: host: 192.168.0.100:61207 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-20 01:11:03,380 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484845506952_0002_000001
2017-01-20 01:11:03,381 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484845506952_0002_000001
2017-01-20 01:11:03,381 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0002_000001 State change from FINISHING to FINISHED
2017-01-20 01:11:03,381 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0002 State change from FINISHING to FINISHED
2017-01-20 01:11:03,381 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484845506952_0002_000001 is done. finalState=FINISHED
2017-01-20 01:11:03,382 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484845506952_0002
2017-01-20 01:11:03,382 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484845506952_0002 requests cleared
2017-01-20 01:11:03,382 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484845506952_0002 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 01:11:03,382 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484845506952_0002 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-20 01:11:03,382 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484845506952_0002,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484845506952_0002/,appMasterHost=192.168.0.100,startTime=1484845848349,finishTime=1484845857120,finalStatus=SUCCEEDED,memorySeconds=29065,vcoreSeconds=14,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-20 01:11:03,382 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484845506952_0002_000001
2017-01-20 01:11:13,526 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 3
2017-01-20 01:11:14,353 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 3 submitted by user lybuestc
2017-01-20 01:11:14,353 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484845506952_0003
2017-01-20 01:11:14,353 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484845506952_0003
2017-01-20 01:11:14,353 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0003 State change from NEW to NEW_SAVING
2017-01-20 01:11:14,354 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484845506952_0003
2017-01-20 01:11:14,354 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0003 State change from NEW_SAVING to SUBMITTED
2017-01-20 01:11:14,354 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484845506952_0003 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-20 01:11:14,354 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484845506952_0003 from user: lybuestc, in queue: default
2017-01-20 01:11:14,377 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0003 State change from SUBMITTED to ACCEPTED
2017-01-20 01:11:14,377 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484845506952_0003_000001
2017-01-20 01:11:14,377 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0003_000001 State change from NEW to SUBMITTED
2017-01-20 01:11:14,378 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:11:14,378 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:11:14,378 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484845506952_0003 from user: lybuestc activated in queue: default
2017-01-20 01:11:14,378 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484845506952_0003 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@348e9cbd, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 01:11:14,378 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484845506952_0003_000001 to scheduler from user lybuestc in queue default
2017-01-20 01:11:14,379 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0003_000001 State change from SUBMITTED to SCHEDULED
2017-01-20 01:11:14,421 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0003_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 01:11:14,421 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0003	CONTAINERID=container_1484845506952_0003_01_000001
2017-01-20 01:11:14,421 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484845506952_0003_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 01:11:14,422 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484845506952_0003_000001 container=Container: [ContainerId: container_1484845506952_0003_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 01:11:14,422 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 01:11:14,422 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 01:11:14,423 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:61207 for container : container_1484845506952_0003_01_000001
2017-01-20 01:11:14,425 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0003_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 01:11:14,425 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484845506952_0003_000001
2017-01-20 01:11:14,425 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484845506952_0003 AttemptId: appattempt_1484845506952_0003_000001 MasterContainer: Container: [ContainerId: container_1484845506952_0003_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ]
2017-01-20 01:11:14,425 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0003_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 01:11:14,431 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0003_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 01:11:14,432 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484845506952_0003_000001
2017-01-20 01:11:14,440 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484845506952_0003_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0003_000001
2017-01-20 01:11:14,441 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484845506952_0003_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 01:11:14,441 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484845506952_0003_000001
2017-01-20 01:11:14,441 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484845506952_0003_000001
2017-01-20 01:11:14,455 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484845506952_0003_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0003_000001
2017-01-20 01:11:14,455 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0003_000001 State change from ALLOCATED to LAUNCHED
2017-01-20 01:11:15,427 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0003_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-20 01:11:19,582 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484845506952_0003_000001 (auth:SIMPLE)
2017-01-20 01:11:19,585 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484845506952_0003_000001
2017-01-20 01:11:19,585 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484845506952_0003	APPATTEMPTID=appattempt_1484845506952_0003_000001
2017-01-20 01:11:19,585 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0003_000001 State change from LAUNCHED to RUNNING
2017-01-20 01:11:19,585 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0003 State change from ACCEPTED to RUNNING
2017-01-20 01:11:21,981 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484845506952_0003_000001 with final state: FINISHING, and exit status: -1000
2017-01-20 01:11:21,981 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0003_000001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:11:21,981 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484845506952_0003 with final state: FINISHING
2017-01-20 01:11:21,981 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0003 State change from RUNNING to FINAL_SAVING
2017-01-20 01:11:21,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484845506952_0003
2017-01-20 01:11:21,981 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0003_000001 State change from FINAL_SAVING to FINISHING
2017-01-20 01:11:21,982 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0003 State change from FINAL_SAVING to FINISHING
2017-01-20 01:11:22,987 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484845506952_0003 unregistered successfully. 
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0003_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484845506952_0003_000001
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484845506952_0003_01_000001 in state: COMPLETED event:FINISHED
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484845506952_0003_000001
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0003	CONTAINERID=container_1484845506952_0003_01_000001
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0003_000001 State change from FINISHING to FINISHED
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484845506952_0003_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0003 State change from FINISHING to FINISHED
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484845506952_0003
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484845506952_0003_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484845506952_0003,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484845506952_0003/,appMasterHost=192.168.0.100,startTime=1484845874353,finishTime=1484845881981,finalStatus=SUCCEEDED,memorySeconds=28297,vcoreSeconds=13,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-20 01:11:28,238 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 01:11:28,248 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 01:11:28,248 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484845506952_0003_000001 released container container_1484845506952_0003_01_000001 on node: host: 192.168.0.100:61207 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-20 01:11:28,249 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484845506952_0003_000001 is done. finalState=FINISHED
2017-01-20 01:11:28,249 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484845506952_0003 requests cleared
2017-01-20 01:11:28,249 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484845506952_0003 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 01:11:28,249 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484845506952_0003 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-20 01:11:28,249 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484845506952_0003_000001
2017-01-20 01:11:30,261 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 4
2017-01-20 01:11:31,043 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 4 submitted by user lybuestc
2017-01-20 01:11:31,043 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484845506952_0004
2017-01-20 01:11:31,043 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484845506952_0004
2017-01-20 01:11:31,043 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0004 State change from NEW to NEW_SAVING
2017-01-20 01:11:31,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484845506952_0004
2017-01-20 01:11:31,044 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0004 State change from NEW_SAVING to SUBMITTED
2017-01-20 01:11:31,044 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484845506952_0004 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-20 01:11:31,044 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484845506952_0004 from user: lybuestc, in queue: default
2017-01-20 01:11:31,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0004 State change from SUBMITTED to ACCEPTED
2017-01-20 01:11:31,068 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484845506952_0004_000001
2017-01-20 01:11:31,068 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0004_000001 State change from NEW to SUBMITTED
2017-01-20 01:11:31,068 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:11:31,068 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:11:31,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484845506952_0004 from user: lybuestc activated in queue: default
2017-01-20 01:11:31,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484845506952_0004 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@19ac2093, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 01:11:31,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484845506952_0004_000001 to scheduler from user lybuestc in queue default
2017-01-20 01:11:31,074 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0004_000001 State change from SUBMITTED to SCHEDULED
2017-01-20 01:11:31,251 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0004_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 01:11:31,251 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0004	CONTAINERID=container_1484845506952_0004_01_000001
2017-01-20 01:11:31,251 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484845506952_0004_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 01:11:31,251 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484845506952_0004_000001 container=Container: [ContainerId: container_1484845506952_0004_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 01:11:31,251 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 01:11:31,252 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 01:11:31,252 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:61207 for container : container_1484845506952_0004_01_000001
2017-01-20 01:11:31,260 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0004_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 01:11:31,260 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484845506952_0004_000001
2017-01-20 01:11:31,260 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484845506952_0004 AttemptId: appattempt_1484845506952_0004_000001 MasterContainer: Container: [ContainerId: container_1484845506952_0004_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ]
2017-01-20 01:11:31,260 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0004_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 01:11:31,260 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0004_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 01:11:31,261 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484845506952_0004_000001
2017-01-20 01:11:31,263 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484845506952_0004_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0004_000001
2017-01-20 01:11:31,263 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484845506952_0004_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 01:11:31,264 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484845506952_0004_000001
2017-01-20 01:11:31,264 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484845506952_0004_000001
2017-01-20 01:11:31,273 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484845506952_0004_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0004_000001
2017-01-20 01:11:31,273 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0004_000001 State change from ALLOCATED to LAUNCHED
2017-01-20 01:11:32,255 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0004_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-20 01:11:36,274 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484845506952_0004_000001 (auth:SIMPLE)
2017-01-20 01:11:36,280 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484845506952_0004_000001
2017-01-20 01:11:36,280 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484845506952_0004	APPATTEMPTID=appattempt_1484845506952_0004_000001
2017-01-20 01:11:36,280 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0004_000001 State change from LAUNCHED to RUNNING
2017-01-20 01:11:36,280 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0004 State change from ACCEPTED to RUNNING
2017-01-20 01:11:38,158 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484845506952_0004_000001 with final state: FINISHING, and exit status: -1000
2017-01-20 01:11:38,158 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0004_000001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:11:38,158 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484845506952_0004 with final state: FINISHING
2017-01-20 01:11:38,158 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0004 State change from RUNNING to FINAL_SAVING
2017-01-20 01:11:38,158 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484845506952_0004
2017-01-20 01:11:38,158 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0004_000001 State change from FINAL_SAVING to FINISHING
2017-01-20 01:11:38,158 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0004 State change from FINAL_SAVING to FINISHING
2017-01-20 01:11:39,167 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484845506952_0004 unregistered successfully. 
2017-01-20 01:11:44,310 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0004_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-20 01:11:44,310 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484845506952_0004_01_000001 in state: COMPLETED event:FINISHED
2017-01-20 01:11:44,311 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0004	CONTAINERID=container_1484845506952_0004_01_000001
2017-01-20 01:11:44,311 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484845506952_0004_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 01:11:44,311 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 01:11:44,311 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484845506952_0004_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 01:11:44,311 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 01:11:44,311 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 01:11:44,311 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484845506952_0004_000001 released container container_1484845506952_0004_01_000001 on node: host: 192.168.0.100:61207 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-20 01:11:44,311 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484845506952_0004_000001
2017-01-20 01:11:44,312 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484845506952_0004_000001
2017-01-20 01:11:44,312 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0004_000001 State change from FINISHING to FINISHED
2017-01-20 01:11:44,312 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0004 State change from FINISHING to FINISHED
2017-01-20 01:11:44,313 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484845506952_0004
2017-01-20 01:11:44,314 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484845506952_0004,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484845506952_0004/,appMasterHost=192.168.0.100,startTime=1484845891043,finishTime=1484845898158,finalStatus=SUCCEEDED,memorySeconds=26744,vcoreSeconds=13,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-20 01:11:44,314 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484845506952_0004_000001 is done. finalState=FINISHED
2017-01-20 01:11:44,314 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484845506952_0004 requests cleared
2017-01-20 01:11:44,314 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484845506952_0004 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 01:11:44,314 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484845506952_0004 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-20 01:11:44,314 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484845506952_0004_000001
2017-01-20 01:11:44,557 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 5
2017-01-20 01:11:45,371 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 5 submitted by user lybuestc
2017-01-20 01:11:45,371 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484845506952_0005
2017-01-20 01:11:45,371 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484845506952_0005
2017-01-20 01:11:45,371 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0005 State change from NEW to NEW_SAVING
2017-01-20 01:11:45,371 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484845506952_0005
2017-01-20 01:11:45,372 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0005 State change from NEW_SAVING to SUBMITTED
2017-01-20 01:11:45,372 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484845506952_0005 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-20 01:11:45,372 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484845506952_0005 from user: lybuestc, in queue: default
2017-01-20 01:11:45,404 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0005 State change from SUBMITTED to ACCEPTED
2017-01-20 01:11:45,404 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484845506952_0005_000001
2017-01-20 01:11:45,405 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0005_000001 State change from NEW to SUBMITTED
2017-01-20 01:11:45,405 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:11:45,405 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:11:45,405 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484845506952_0005 from user: lybuestc activated in queue: default
2017-01-20 01:11:45,405 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484845506952_0005 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@a1fdbe6, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 01:11:45,405 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484845506952_0005_000001 to scheduler from user lybuestc in queue default
2017-01-20 01:11:45,406 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0005_000001 State change from SUBMITTED to SCHEDULED
2017-01-20 01:11:46,318 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0005_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 01:11:46,319 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0005	CONTAINERID=container_1484845506952_0005_01_000001
2017-01-20 01:11:46,319 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484845506952_0005_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 01:11:46,319 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484845506952_0005_000001 container=Container: [ContainerId: container_1484845506952_0005_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 01:11:46,319 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 01:11:46,319 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 01:11:46,320 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:61207 for container : container_1484845506952_0005_01_000001
2017-01-20 01:11:46,321 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0005_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 01:11:46,321 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484845506952_0005_000001
2017-01-20 01:11:46,321 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484845506952_0005 AttemptId: appattempt_1484845506952_0005_000001 MasterContainer: Container: [ContainerId: container_1484845506952_0005_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ]
2017-01-20 01:11:46,322 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0005_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 01:11:46,322 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0005_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 01:11:46,323 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484845506952_0005_000001
2017-01-20 01:11:46,325 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484845506952_0005_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0005_000001
2017-01-20 01:11:46,325 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484845506952_0005_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 01:11:46,325 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484845506952_0005_000001
2017-01-20 01:11:46,325 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484845506952_0005_000001
2017-01-20 01:11:46,336 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484845506952_0005_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0005_000001
2017-01-20 01:11:46,336 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0005_000001 State change from ALLOCATED to LAUNCHED
2017-01-20 01:11:47,324 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0005_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-20 01:11:51,249 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484845506952_0005_000001 (auth:SIMPLE)
2017-01-20 01:11:51,255 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484845506952_0005_000001
2017-01-20 01:11:51,255 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484845506952_0005	APPATTEMPTID=appattempt_1484845506952_0005_000001
2017-01-20 01:11:51,255 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0005_000001 State change from LAUNCHED to RUNNING
2017-01-20 01:11:51,256 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0005 State change from ACCEPTED to RUNNING
2017-01-20 01:11:53,136 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484845506952_0005_000001 with final state: FINISHING, and exit status: -1000
2017-01-20 01:11:53,136 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0005_000001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:11:53,136 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484845506952_0005 with final state: FINISHING
2017-01-20 01:11:53,136 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0005 State change from RUNNING to FINAL_SAVING
2017-01-20 01:11:53,136 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484845506952_0005
2017-01-20 01:11:53,136 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0005_000001 State change from FINAL_SAVING to FINISHING
2017-01-20 01:11:53,136 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0005 State change from FINAL_SAVING to FINISHING
2017-01-20 01:11:54,142 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484845506952_0005 unregistered successfully. 
2017-01-20 01:11:59,268 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0005_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-20 01:11:59,268 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484845506952_0005_000001
2017-01-20 01:11:59,268 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484845506952_0005_01_000001 in state: COMPLETED event:FINISHED
2017-01-20 01:11:59,268 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484845506952_0005_000001
2017-01-20 01:11:59,268 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0005	CONTAINERID=container_1484845506952_0005_01_000001
2017-01-20 01:11:59,268 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0005_000001 State change from FINISHING to FINISHED
2017-01-20 01:11:59,268 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484845506952_0005_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 01:11:59,268 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0005 State change from FINISHING to FINISHED
2017-01-20 01:11:59,268 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 01:11:59,269 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484845506952_0005
2017-01-20 01:11:59,269 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484845506952_0005_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 01:11:59,269 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 01:11:59,269 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 01:11:59,269 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484845506952_0005_000001 released container container_1484845506952_0005_01_000001 on node: host: 192.168.0.100:61207 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-20 01:11:59,269 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484845506952_0005,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484845506952_0005/,appMasterHost=192.168.0.100,startTime=1484845905371,finishTime=1484845913136,finalStatus=SUCCEEDED,memorySeconds=26521,vcoreSeconds=12,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-20 01:11:59,269 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484845506952_0005_000001 is done. finalState=FINISHED
2017-01-20 01:11:59,269 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484845506952_0005 requests cleared
2017-01-20 01:11:59,269 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484845506952_0005_000001
2017-01-20 01:11:59,269 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484845506952_0005 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 01:11:59,270 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484845506952_0005 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-20 01:12:01,423 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 6
2017-01-20 01:12:02,297 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 6 submitted by user lybuestc
2017-01-20 01:12:02,297 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484845506952_0006
2017-01-20 01:12:02,297 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484845506952_0006
2017-01-20 01:12:02,297 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0006 State change from NEW to NEW_SAVING
2017-01-20 01:12:02,298 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484845506952_0006
2017-01-20 01:12:02,298 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0006 State change from NEW_SAVING to SUBMITTED
2017-01-20 01:12:02,298 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484845506952_0006 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-20 01:12:02,298 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484845506952_0006 from user: lybuestc, in queue: default
2017-01-20 01:12:02,320 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0006 State change from SUBMITTED to ACCEPTED
2017-01-20 01:12:02,321 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484845506952_0006_000001
2017-01-20 01:12:02,321 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0006_000001 State change from NEW to SUBMITTED
2017-01-20 01:12:02,321 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:12:02,321 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:12:02,321 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484845506952_0006 from user: lybuestc activated in queue: default
2017-01-20 01:12:02,321 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484845506952_0006 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@7c9c2384, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 01:12:02,321 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484845506952_0006_000001 to scheduler from user lybuestc in queue default
2017-01-20 01:12:02,323 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0006_000001 State change from SUBMITTED to SCHEDULED
2017-01-20 01:12:03,283 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0006_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 01:12:03,283 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0006	CONTAINERID=container_1484845506952_0006_01_000001
2017-01-20 01:12:03,283 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484845506952_0006_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 01:12:03,283 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484845506952_0006_000001 container=Container: [ContainerId: container_1484845506952_0006_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 01:12:03,284 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 01:12:03,284 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 01:12:03,284 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:61207 for container : container_1484845506952_0006_01_000001
2017-01-20 01:12:03,286 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0006_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 01:12:03,286 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484845506952_0006_000001
2017-01-20 01:12:03,286 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484845506952_0006 AttemptId: appattempt_1484845506952_0006_000001 MasterContainer: Container: [ContainerId: container_1484845506952_0006_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ]
2017-01-20 01:12:03,287 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0006_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 01:12:03,287 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0006_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 01:12:03,287 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484845506952_0006_000001
2017-01-20 01:12:03,289 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484845506952_0006_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0006_000001
2017-01-20 01:12:03,289 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484845506952_0006_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 01:12:03,289 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484845506952_0006_000001
2017-01-20 01:12:03,289 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484845506952_0006_000001
2017-01-20 01:12:03,306 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484845506952_0006_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] for AM appattempt_1484845506952_0006_000001
2017-01-20 01:12:03,306 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0006_000001 State change from ALLOCATED to LAUNCHED
2017-01-20 01:12:04,285 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0006_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-20 01:12:08,359 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484845506952_0006_000001 (auth:SIMPLE)
2017-01-20 01:12:08,363 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484845506952_0006_000001
2017-01-20 01:12:08,363 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484845506952_0006	APPATTEMPTID=appattempt_1484845506952_0006_000001
2017-01-20 01:12:08,363 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0006_000001 State change from LAUNCHED to RUNNING
2017-01-20 01:12:08,363 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0006 State change from ACCEPTED to RUNNING
2017-01-20 01:12:10,667 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484845506952_0006_000001 with final state: FINISHING, and exit status: -1000
2017-01-20 01:12:10,667 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0006_000001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:12:10,667 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484845506952_0006 with final state: FINISHING
2017-01-20 01:12:10,667 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0006 State change from RUNNING to FINAL_SAVING
2017-01-20 01:12:10,667 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484845506952_0006
2017-01-20 01:12:10,667 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0006_000001 State change from FINAL_SAVING to FINISHING
2017-01-20 01:12:10,667 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0006 State change from FINAL_SAVING to FINISHING
2017-01-20 01:12:11,673 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484845506952_0006 unregistered successfully. 
2017-01-20 01:12:16,889 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484845506952_0006_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-20 01:12:16,889 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484845506952_0006_01_000001 in state: COMPLETED event:FINISHED
2017-01-20 01:12:16,889 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484845506952_0006	CONTAINERID=container_1484845506952_0006_01_000001
2017-01-20 01:12:16,889 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484845506952_0006_000001
2017-01-20 01:12:16,889 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484845506952_0006_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:61207, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 01:12:16,890 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484845506952_0006_000001
2017-01-20 01:12:16,890 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484845506952_0006_000001 State change from FINISHING to FINISHED
2017-01-20 01:12:16,890 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 01:12:16,890 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484845506952_0006 State change from FINISHING to FINISHED
2017-01-20 01:12:16,890 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484845506952_0006_01_000001, NodeId: 192.168.0.100:61207, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:61207 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 01:12:16,890 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484845506952_0006
2017-01-20 01:12:16,890 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484845506952_0006,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484845506952_0006/,appMasterHost=192.168.0.100,startTime=1484845922297,finishTime=1484845930667,finalStatus=SUCCEEDED,memorySeconds=27865,vcoreSeconds=13,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-20 01:12:16,890 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 01:12:16,891 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 01:12:16,891 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484845506952_0006_000001
2017-01-20 01:12:16,893 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484845506952_0006_000001 released container container_1484845506952_0006_01_000001 on node: host: 192.168.0.100:61207 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-20 01:12:16,894 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484845506952_0006_000001 is done. finalState=FINISHED
2017-01-20 01:12:16,894 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484845506952_0006 requests cleared
2017-01-20 01:12:16,894 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484845506952_0006 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 01:12:16,894 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484845506952_0006 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-20 01:12:42,912 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://192.168.0.100:19888/jobhistory/job/job_1484845506952_0006 which is the app master GUI of application_1484845506952_0006 owned by lybuestc
2017-01-20 01:15:06,858 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Release request cache is cleaned up
2017-01-20 01:16:59,027 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: RECEIVED SIGNAL 15: SIGTERM
2017-01-20 01:16:59,037 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2017-01-20 01:16:59,041 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2017-01-20 01:16:59,041 INFO org.apache.hadoop.ipc.Server: Stopping server on 8032
2017-01-20 01:16:59,050 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8032
2017-01-20 01:16:59,050 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:16:59,051 INFO org.apache.hadoop.ipc.Server: Stopping server on 8033
2017-01-20 01:16:59,061 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8033
2017-01-20 01:16:59,061 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:16:59,062 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to standby state
2017-01-20 01:16:59,062 WARN org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher: org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher$LauncherThread interrupted. Returning.
2017-01-20 01:16:59,062 INFO org.apache.hadoop.ipc.Server: Stopping server on 8030
2017-01-20 01:16:59,075 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8030
2017-01-20 01:16:59,075 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:16:59,075 INFO org.apache.hadoop.ipc.Server: Stopping server on 8031
2017-01-20 01:16:59,083 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8031
2017-01-20 01:16:59,083 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:16:59,083 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: NMLivelinessMonitor thread interrupted
2017-01-20 01:16:59,083 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Returning, interrupted : java.lang.InterruptedException
2017-01-20 01:16:59,083 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, igonring any new events.
2017-01-20 01:16:59,084 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2017-01-20 01:16:59,084 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer thread interrupted
2017-01-20 01:16:59,084 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2017-01-20 01:16:59,084 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2017-01-20 01:16:59,084 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ResourceManager metrics system...
2017-01-20 01:16:59,085 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system stopped.
2017-01-20 01:16:59,085 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.
2017-01-20 01:16:59,085 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, igonring any new events.
2017-01-20 01:16:59,085 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to standby state
2017-01-20 01:16:59,085 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down ResourceManager at chengguan-3.local/192.168.0.100
************************************************************/
2017-01-20 01:17:47,853 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = chengguan-3.local/192.168.0.100
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/rm-config/log4j.properties
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_91
************************************************************/
2017-01-20 01:17:47,865 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-20 01:17:48,201 INFO org.apache.hadoop.conf.Configuration: found resource core-site.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/core-site.xml
2017-01-20 01:17:48,468 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-01-20 01:17:48,512 INFO org.apache.hadoop.security.Groups: clearing userToGroupsMap cache
2017-01-20 01:17:48,636 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/yarn-site.xml
2017-01-20 01:17:49,043 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2017-01-20 01:17:49,424 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2017-01-20 01:17:49,454 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2017-01-20 01:17:49,464 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2017-01-20 01:17:49,544 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2017-01-20 01:17:49,547 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2017-01-20 01:17:49,547 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2017-01-20 01:17:49,567 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher
2017-01-20 01:17:49,568 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2017-01-20 01:17:49,569 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2017-01-20 01:17:49,570 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2017-01-20 01:17:49,722 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-20 01:17:49,838 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-20 01:17:49,838 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2017-01-20 01:17:49,872 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2017-01-20 01:17:49,879 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2017-01-20 01:17:49,881 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2017-01-20 01:17:49,883 INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instiantiated.
2017-01-20 01:17:49,884 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2017-01-20 01:17:49,885 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/capacity-scheduler.xml
2017-01-20 01:17:49,980 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2017-01-20 01:17:49,980 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2017-01-20 01:17:50,012 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, asboluteCapacity=1.0, maxCapacity=1.0, asboluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
, reservationsContinueLooking=true
2017-01-20 01:17:50,012 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2017-01-20 01:17:50,027 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2017-01-20 01:17:50,027 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2017-01-20 01:17:50,030 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
asboluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:8192, vCores:32> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
nodeLocalityDelay = 40
labels=*,
nodeLocalityDelay = 40
reservationsContinueLooking = true
preemptionDisabled = true

2017-01-20 01:17:50,031 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0
2017-01-20 01:17:50,031 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2017-01-20 01:17:50,032 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2017-01-20 01:17:50,032 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue mappings, override: false
2017-01-20 01:17:50,033 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:32>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2017-01-20 01:17:50,045 INFO org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher: YARN system metrics publishing service is not enabled
2017-01-20 01:17:50,045 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state
2017-01-20 01:17:50,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2017-01-20 01:17:50,065 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2017-01-20 01:17:50,066 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2017-01-20 01:17:50,066 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:17:50,069 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 1
2017-01-20 01:17:50,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2017-01-20 01:17:50,073 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2017-01-20 01:17:50,074 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:17:50,074 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 2
2017-01-20 01:17:50,074 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2017-01-20 01:17:50,077 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
2017-01-20 01:17:50,114 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:17:50,161 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8031
2017-01-20 01:17:50,180 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
2017-01-20 01:17:50,181 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:17:50,181 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8031: starting
2017-01-20 01:17:50,245 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:17:50,250 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8030
2017-01-20 01:17:50,262 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
2017-01-20 01:17:50,262 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:17:50,262 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8030: starting
2017-01-20 01:17:50,368 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:17:50,370 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8032
2017-01-20 01:17:50,372 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
2017-01-20 01:17:50,373 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:17:50,373 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8032: starting
2017-01-20 01:17:50,405 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
2017-01-20 01:17:50,588 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-20 01:17:50,612 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-20 01:17:50,622 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2017-01-20 01:17:50,634 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-20 01:17:50,637 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
2017-01-20 01:17:50,638 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
2017-01-20 01:17:50,638 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
2017-01-20 01:17:50,638 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2017-01-20 01:17:50,638 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-20 01:17:50,639 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-20 01:17:50,645 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /cluster/*
2017-01-20 01:17:50,645 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2017-01-20 01:17:51,463 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2017-01-20 01:17:51,471 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8088
2017-01-20 01:17:51,471 INFO org.mortbay.log: jetty-6.1.26
2017-01-20 01:17:51,508 INFO org.mortbay.log: Extract jar:file:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar!/webapps/cluster to /var/folders/xl/5zp3ynpd6l5cg_y4f6qjqxxm0000gn/T/Jetty_0_0_0_0_8088_cluster____u0rgz3/webapp
2017-01-20 01:17:51,774 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:17:51,774 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2017-01-20 01:17:51,775 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:17:53,973 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2017-01-20 01:17:53,974 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at 8088
2017-01-20 01:17:54,097 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:17:54,100 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2017-01-20 01:17:54,103 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2017-01-20 01:17:54,103 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:17:54,104 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8033: starting
2017-01-20 01:17:55,737 INFO org.apache.hadoop.yarn.util.RackResolver: Resolved 192.168.0.100 to /default-rack
2017-01-20 01:17:55,740 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node 192.168.0.100(cmPort: 51424 httpPort: 8042) registered with capability: <memory:8192, vCores:8>, assigned nodeId 192.168.0.100:51424
2017-01-20 01:17:55,746 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: 192.168.0.100:51424 Node Transitioned from NEW to RUNNING
2017-01-20 01:17:55,753 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node 192.168.0.100:51424 clusterResource: <memory:8192, vCores:8>
2017-01-20 01:18:18,520 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 1
2017-01-20 01:18:20,309 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 1 submitted by user lybuestc
2017-01-20 01:18:20,310 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484846270046_0001
2017-01-20 01:18:20,311 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484846270046_0001
2017-01-20 01:18:20,320 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0001 State change from NEW to NEW_SAVING
2017-01-20 01:18:20,320 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484846270046_0001
2017-01-20 01:18:20,322 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0001 State change from NEW_SAVING to SUBMITTED
2017-01-20 01:18:20,324 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484846270046_0001 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-20 01:18:20,324 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484846270046_0001 from user: lybuestc, in queue: default
2017-01-20 01:18:20,359 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0001 State change from SUBMITTED to ACCEPTED
2017-01-20 01:18:20,403 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484846270046_0001_000001
2017-01-20 01:18:20,404 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000001 State change from NEW to SUBMITTED
2017-01-20 01:18:20,426 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:18:20,427 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:18:20,427 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484846270046_0001 from user: lybuestc activated in queue: default
2017-01-20 01:18:20,427 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484846270046_0001 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@64764f1d, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 01:18:20,427 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484846270046_0001_000001 to scheduler from user lybuestc in queue default
2017-01-20 01:18:20,431 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000001 State change from SUBMITTED to SCHEDULED
2017-01-20 01:18:21,021 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0001_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 01:18:21,021 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846270046_0001	CONTAINERID=container_1484846270046_0001_01_000001
2017-01-20 01:18:21,024 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484846270046_0001_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:51424, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 01:18:21,024 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484846270046_0001_000001 container=Container: [ContainerId: container_1484846270046_0001_01_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 01:18:21,024 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 01:18:21,024 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 01:18:21,052 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:51424 for container : container_1484846270046_0001_01_000001
2017-01-20 01:18:21,068 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 01:18:21,069 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484846270046_0001_000001
2017-01-20 01:18:21,072 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484846270046_0001 AttemptId: appattempt_1484846270046_0001_000001 MasterContainer: Container: [ContainerId: container_1484846270046_0001_01_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ]
2017-01-20 01:18:21,087 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 01:18:21,089 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 01:18:21,094 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484846270046_0001_000001
2017-01-20 01:18:21,143 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484846270046_0001_01_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] for AM appattempt_1484846270046_0001_000001
2017-01-20 01:18:21,143 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484846270046_0001_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 01:18:21,146 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484846270046_0001_000001
2017-01-20 01:18:21,151 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484846270046_0001_000001
2017-01-20 01:18:21,814 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484846270046_0001_01_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] for AM appattempt_1484846270046_0001_000001
2017-01-20 01:18:21,815 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000001 State change from ALLOCATED to LAUNCHED
2017-01-20 01:18:22,176 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-20 01:18:30,162 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484846270046_0001_000001 (auth:SIMPLE)
2017-01-20 01:18:30,171 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484846270046_0001_000001
2017-01-20 01:18:30,171 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484846270046_0001	APPATTEMPTID=appattempt_1484846270046_0001_000001
2017-01-20 01:18:30,171 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000001 State change from LAUNCHED to RUNNING
2017-01-20 01:18:30,171 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0001 State change from ACCEPTED to RUNNING
2017-01-20 01:18:38,522 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0001_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-20 01:18:38,522 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484846270046_0001_01_000001 in state: COMPLETED event:FINISHED
2017-01-20 01:18:38,523 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846270046_0001	CONTAINERID=container_1484846270046_0001_01_000001
2017-01-20 01:18:38,523 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484846270046_0001_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:51424, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 01:18:38,523 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 01:18:38,523 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484846270046_0001_01_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 01:18:38,524 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 01:18:38,524 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 01:18:38,524 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484846270046_0001_000001 released container container_1484846270046_0001_01_000001 on node: host: 192.168.0.100:51424 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-20 01:18:38,574 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484846270046_0001_000001 with final state: FAILED, and exit status: 0
2017-01-20 01:18:38,576 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:18:38,576 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484846270046_0001_000001
2017-01-20 01:18:38,578 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484846270046_0001_000001
2017-01-20 01:18:38,578 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000001 State change from FINAL_SAVING to FAILED
2017-01-20 01:18:38,578 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2017-01-20 01:18:38,579 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0001 State change from RUNNING to ACCEPTED
2017-01-20 01:18:38,579 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484846270046_0001_000002
2017-01-20 01:18:38,579 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000002 State change from NEW to SUBMITTED
2017-01-20 01:18:38,579 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484846270046_0001_000001 is done. finalState=FAILED
2017-01-20 01:18:38,580 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484846270046_0001 requests cleared
2017-01-20 01:18:38,580 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484846270046_0001 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 01:18:38,580 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:18:38,580 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:18:38,580 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484846270046_0001 from user: lybuestc activated in queue: default
2017-01-20 01:18:38,580 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484846270046_0001 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@4ec64a28, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 01:18:38,580 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484846270046_0001_000002 to scheduler from user lybuestc in queue default
2017-01-20 01:18:38,580 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484846270046_0001_000001
2017-01-20 01:18:38,582 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000002 State change from SUBMITTED to SCHEDULED
2017-01-20 01:18:39,526 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0001_02_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 01:18:39,526 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846270046_0001	CONTAINERID=container_1484846270046_0001_02_000001
2017-01-20 01:18:39,526 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484846270046_0001_02_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:51424, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 01:18:39,527 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484846270046_0001_000002 container=Container: [ContainerId: container_1484846270046_0001_02_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 01:18:39,527 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 01:18:39,527 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 01:18:39,528 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:51424 for container : container_1484846270046_0001_02_000001
2017-01-20 01:18:39,529 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0001_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 01:18:39,530 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484846270046_0001_000002
2017-01-20 01:18:39,530 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484846270046_0001 AttemptId: appattempt_1484846270046_0001_000002 MasterContainer: Container: [ContainerId: container_1484846270046_0001_02_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ]
2017-01-20 01:18:39,530 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000002 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 01:18:39,530 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000002 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 01:18:39,531 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484846270046_0001_000002
2017-01-20 01:18:39,534 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484846270046_0001_02_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] for AM appattempt_1484846270046_0001_000002
2017-01-20 01:18:39,535 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484846270046_0001_02_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 01:18:39,535 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484846270046_0001_000002
2017-01-20 01:18:39,535 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484846270046_0001_000002
2017-01-20 01:18:39,547 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484846270046_0001_02_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] for AM appattempt_1484846270046_0001_000002
2017-01-20 01:18:39,548 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000002 State change from ALLOCATED to LAUNCHED
2017-01-20 01:18:40,533 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0001_02_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-20 01:18:42,715 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484846270046_0001_000002 (auth:SIMPLE)
2017-01-20 01:18:42,718 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484846270046_0001_000002
2017-01-20 01:18:42,718 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484846270046_0001	APPATTEMPTID=appattempt_1484846270046_0001_000002
2017-01-20 01:18:42,718 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000002 State change from LAUNCHED to RUNNING
2017-01-20 01:18:42,718 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0001 State change from ACCEPTED to RUNNING
2017-01-20 01:18:44,274 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0001_02_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-20 01:18:44,275 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484846270046_0001_02_000001 in state: COMPLETED event:FINISHED
2017-01-20 01:18:44,275 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846270046_0001	CONTAINERID=container_1484846270046_0001_02_000001
2017-01-20 01:18:44,275 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484846270046_0001_02_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:51424, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 01:18:44,275 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 01:18:44,275 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484846270046_0001_02_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 01:18:44,275 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 01:18:44,275 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 01:18:44,275 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484846270046_0001_000002 released container container_1484846270046_0001_02_000001 on node: host: 192.168.0.100:51424 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-20 01:18:44,308 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484846270046_0001_000002 with final state: FAILED, and exit status: 1
2017-01-20 01:18:44,309 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000002 State change from RUNNING to FINAL_SAVING
2017-01-20 01:18:44,309 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484846270046_0001_000002
2017-01-20 01:18:44,309 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484846270046_0001_000002
2017-01-20 01:18:44,309 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0001_000002 State change from FINAL_SAVING to FAILED
2017-01-20 01:18:44,309 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 2. The max attempts is 2
2017-01-20 01:18:44,310 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484846270046_0001 with final state: FAILED
2017-01-20 01:18:44,312 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:18:44,312 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484846270046_0001
2017-01-20 01:18:44,312 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Application application_1484846270046_0001 failed 2 times due to AM Container for appattempt_1484846270046_0001_000002 exited with  exitCode: 1
For more detailed output, check application tracking page:http://chengguan-3.local:8088/cluster/app/application_1484846270046_0001Then, click on links to logs of each attempt.
Diagnostics: Exception from container-launch.
Container id: container_1484846270046_0001_02_000001
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582)
	at org.apache.hadoop.util.Shell.run(Shell.java:479)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


Container exited with a non-zero exit code 1
Failing this attempt. Failing the application.
2017-01-20 01:18:44,312 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484846270046_0001_000002 is done. finalState=FAILED
2017-01-20 01:18:44,313 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484846270046_0001 requests cleared
2017-01-20 01:18:44,313 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484846270046_0001 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 01:18:44,316 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0001 State change from FINAL_SAVING to FAILED
2017-01-20 01:18:44,317 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484846270046_0001_000002
2017-01-20 01:18:44,317 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484846270046_0001 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-20 01:18:44,319 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Failed	TARGET=RMAppManager	RESULT=FAILURE	DESCRIPTION=App failed with state: FAILED	PERMISSIONS=Application application_1484846270046_0001 failed 2 times due to AM Container for appattempt_1484846270046_0001_000002 exited with  exitCode: 1
For more detailed output, check application tracking page:http://chengguan-3.local:8088/cluster/app/application_1484846270046_0001Then, click on links to logs of each attempt.
Diagnostics: Exception from container-launch.
Container id: container_1484846270046_0001_02_000001
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582)
	at org.apache.hadoop.util.Shell.run(Shell.java:479)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


Container exited with a non-zero exit code 1
Failing this attempt. Failing the application.	APPID=application_1484846270046_0001
2017-01-20 01:18:44,322 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484846270046_0001,name=word count,user=lybuestc,queue=default,state=FAILED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484846270046_0001,appMasterHost=N/A,startTime=1484846300308,finishTime=1484846324310,finalStatus=FAILED,memorySeconds=45567,vcoreSeconds=21,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-20 01:19:17,120 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 2
2017-01-20 01:19:18,145 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 2 submitted by user lybuestc
2017-01-20 01:19:18,145 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484846270046_0002
2017-01-20 01:19:18,145 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484846270046_0002
2017-01-20 01:19:18,146 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0002 State change from NEW to NEW_SAVING
2017-01-20 01:19:18,146 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484846270046_0002
2017-01-20 01:19:18,146 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0002 State change from NEW_SAVING to SUBMITTED
2017-01-20 01:19:18,147 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484846270046_0002 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-20 01:19:18,147 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484846270046_0002 from user: lybuestc, in queue: default
2017-01-20 01:19:18,170 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0002 State change from SUBMITTED to ACCEPTED
2017-01-20 01:19:18,171 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484846270046_0002_000001
2017-01-20 01:19:18,171 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000001 State change from NEW to SUBMITTED
2017-01-20 01:19:18,171 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:19:18,172 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:19:18,172 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484846270046_0002 from user: lybuestc activated in queue: default
2017-01-20 01:19:18,172 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484846270046_0002 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@31a1b5c2, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 01:19:18,172 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484846270046_0002_000001 to scheduler from user lybuestc in queue default
2017-01-20 01:19:18,173 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000001 State change from SUBMITTED to SCHEDULED
2017-01-20 01:19:18,430 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0002_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 01:19:18,430 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846270046_0002	CONTAINERID=container_1484846270046_0002_01_000001
2017-01-20 01:19:18,430 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484846270046_0002_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:51424, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 01:19:18,430 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484846270046_0002_000001 container=Container: [ContainerId: container_1484846270046_0002_01_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 01:19:18,430 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 01:19:18,431 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 01:19:18,432 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:51424 for container : container_1484846270046_0002_01_000001
2017-01-20 01:19:18,433 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0002_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 01:19:18,433 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484846270046_0002_000001
2017-01-20 01:19:18,433 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484846270046_0002 AttemptId: appattempt_1484846270046_0002_000001 MasterContainer: Container: [ContainerId: container_1484846270046_0002_01_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ]
2017-01-20 01:19:18,433 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 01:19:18,434 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 01:19:18,434 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484846270046_0002_000001
2017-01-20 01:19:18,440 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484846270046_0002_01_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] for AM appattempt_1484846270046_0002_000001
2017-01-20 01:19:18,441 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484846270046_0002_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 01:19:18,441 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484846270046_0002_000001
2017-01-20 01:19:18,441 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484846270046_0002_000001
2017-01-20 01:19:18,454 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484846270046_0002_01_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] for AM appattempt_1484846270046_0002_000001
2017-01-20 01:19:18,455 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000001 State change from ALLOCATED to LAUNCHED
2017-01-20 01:19:19,438 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0002_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-20 01:19:26,050 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484846270046_0002_000001 (auth:SIMPLE)
2017-01-20 01:19:26,055 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484846270046_0002_000001
2017-01-20 01:19:26,055 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484846270046_0002	APPATTEMPTID=appattempt_1484846270046_0002_000001
2017-01-20 01:19:26,055 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000001 State change from LAUNCHED to RUNNING
2017-01-20 01:19:26,055 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0002 State change from ACCEPTED to RUNNING
2017-01-20 01:19:33,287 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0002_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-20 01:19:33,287 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484846270046_0002_01_000001 in state: COMPLETED event:FINISHED
2017-01-20 01:19:33,287 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846270046_0002	CONTAINERID=container_1484846270046_0002_01_000001
2017-01-20 01:19:33,287 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484846270046_0002_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:51424, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 01:19:33,287 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484846270046_0002_000001 with final state: FAILED, and exit status: 0
2017-01-20 01:19:33,287 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 01:19:33,287 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:19:33,288 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484846270046_0002_01_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 01:19:33,288 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 01:19:33,288 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484846270046_0002_000001
2017-01-20 01:19:33,288 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 01:19:33,288 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484846270046_0002_000001
2017-01-20 01:19:33,289 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484846270046_0002_000001 released container container_1484846270046_0002_01_000001 on node: host: 192.168.0.100:51424 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-20 01:19:33,289 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000001 State change from FINAL_SAVING to FAILED
2017-01-20 01:19:33,289 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2017-01-20 01:19:33,289 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0002 State change from RUNNING to ACCEPTED
2017-01-20 01:19:33,289 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484846270046_0002_000001 is done. finalState=FAILED
2017-01-20 01:19:33,289 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484846270046_0002_000002
2017-01-20 01:19:33,290 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484846270046_0002 requests cleared
2017-01-20 01:19:33,290 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000002 State change from NEW to SUBMITTED
2017-01-20 01:19:33,290 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484846270046_0002 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 01:19:33,290 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:19:33,290 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:19:33,290 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484846270046_0002 from user: lybuestc activated in queue: default
2017-01-20 01:19:33,290 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484846270046_0002_000001
2017-01-20 01:19:33,290 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484846270046_0002 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@4dca3ed4, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 01:19:33,290 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484846270046_0002_000002 to scheduler from user lybuestc in queue default
2017-01-20 01:19:33,291 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000002 State change from SUBMITTED to SCHEDULED
2017-01-20 01:19:34,294 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0002_02_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 01:19:34,294 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846270046_0002	CONTAINERID=container_1484846270046_0002_02_000001
2017-01-20 01:19:34,294 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484846270046_0002_02_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:51424, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 01:19:34,294 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484846270046_0002_000002 container=Container: [ContainerId: container_1484846270046_0002_02_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 01:19:34,295 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 01:19:34,295 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 01:19:34,296 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:51424 for container : container_1484846270046_0002_02_000001
2017-01-20 01:19:34,297 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0002_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 01:19:34,297 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484846270046_0002_000002
2017-01-20 01:19:34,297 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484846270046_0002 AttemptId: appattempt_1484846270046_0002_000002 MasterContainer: Container: [ContainerId: container_1484846270046_0002_02_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ]
2017-01-20 01:19:34,297 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000002 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 01:19:34,297 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000002 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 01:19:34,298 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484846270046_0002_000002
2017-01-20 01:19:34,300 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484846270046_0002_02_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] for AM appattempt_1484846270046_0002_000002
2017-01-20 01:19:34,300 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484846270046_0002_02_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 01:19:34,300 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484846270046_0002_000002
2017-01-20 01:19:34,300 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484846270046_0002_000002
2017-01-20 01:19:34,315 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484846270046_0002_02_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] for AM appattempt_1484846270046_0002_000002
2017-01-20 01:19:34,315 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000002 State change from ALLOCATED to LAUNCHED
2017-01-20 01:19:35,301 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0002_02_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-20 01:19:38,118 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484846270046_0002_000002 (auth:SIMPLE)
2017-01-20 01:19:38,122 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484846270046_0002_000002
2017-01-20 01:19:38,122 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484846270046_0002	APPATTEMPTID=appattempt_1484846270046_0002_000002
2017-01-20 01:19:38,122 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000002 State change from LAUNCHED to RUNNING
2017-01-20 01:19:38,122 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0002 State change from ACCEPTED to RUNNING
2017-01-20 01:19:38,855 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846270046_0002_02_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-20 01:19:38,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484846270046_0002_02_000001 in state: COMPLETED event:FINISHED
2017-01-20 01:19:38,856 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846270046_0002	CONTAINERID=container_1484846270046_0002_02_000001
2017-01-20 01:19:38,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484846270046_0002_02_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:51424, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 01:19:38,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 01:19:38,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484846270046_0002_02_000001, NodeId: 192.168.0.100:51424, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:51424 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 01:19:38,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 01:19:38,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 01:19:38,856 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484846270046_0002_000002 released container container_1484846270046_0002_02_000001 on node: host: 192.168.0.100:51424 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-20 01:19:38,857 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484846270046_0002_000002 with final state: FAILED, and exit status: 1
2017-01-20 01:19:38,857 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000002 State change from RUNNING to FINAL_SAVING
2017-01-20 01:19:38,860 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484846270046_0002_000002
2017-01-20 01:19:38,861 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484846270046_0002_000002
2017-01-20 01:19:38,861 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846270046_0002_000002 State change from FINAL_SAVING to FAILED
2017-01-20 01:19:38,861 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 2. The max attempts is 2
2017-01-20 01:19:38,861 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484846270046_0002 with final state: FAILED
2017-01-20 01:19:38,861 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0002 State change from RUNNING to FINAL_SAVING
2017-01-20 01:19:38,862 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484846270046_0002
2017-01-20 01:19:38,862 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484846270046_0002_000002 is done. finalState=FAILED
2017-01-20 01:19:38,862 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484846270046_0002 requests cleared
2017-01-20 01:19:38,863 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484846270046_0002 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 01:19:38,863 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Application application_1484846270046_0002 failed 2 times due to AM Container for appattempt_1484846270046_0002_000002 exited with  exitCode: 1
For more detailed output, check application tracking page:http://chengguan-3.local:8088/cluster/app/application_1484846270046_0002Then, click on links to logs of each attempt.
Diagnostics: Exception from container-launch.
Container id: container_1484846270046_0002_02_000001
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582)
	at org.apache.hadoop.util.Shell.run(Shell.java:479)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


Container exited with a non-zero exit code 1
Failing this attempt. Failing the application.
2017-01-20 01:19:38,863 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846270046_0002 State change from FINAL_SAVING to FAILED
2017-01-20 01:19:38,863 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Failed	TARGET=RMAppManager	RESULT=FAILURE	DESCRIPTION=App failed with state: FAILED	PERMISSIONS=Application application_1484846270046_0002 failed 2 times due to AM Container for appattempt_1484846270046_0002_000002 exited with  exitCode: 1
For more detailed output, check application tracking page:http://chengguan-3.local:8088/cluster/app/application_1484846270046_0002Then, click on links to logs of each attempt.
Diagnostics: Exception from container-launch.
Container id: container_1484846270046_0002_02_000001
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:582)
	at org.apache.hadoop.util.Shell.run(Shell.java:479)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


Container exited with a non-zero exit code 1
Failing this attempt. Failing the application.	APPID=application_1484846270046_0002
2017-01-20 01:19:38,863 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484846270046_0002,name=word count,user=lybuestc,queue=default,state=FAILED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484846270046_0002,appMasterHost=N/A,startTime=1484846358144,finishTime=1484846378861,finalStatus=FAILED,memorySeconds=39767,vcoreSeconds=18,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-20 01:19:38,864 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484846270046_0002 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-20 01:19:38,869 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484846270046_0002_000002
2017-01-20 01:21:35,669 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: RECEIVED SIGNAL 15: SIGTERM
2017-01-20 01:21:35,682 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2017-01-20 01:21:35,684 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2017-01-20 01:21:35,685 INFO org.apache.hadoop.ipc.Server: Stopping server on 8032
2017-01-20 01:21:35,691 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8032
2017-01-20 01:21:35,691 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:21:35,692 INFO org.apache.hadoop.ipc.Server: Stopping server on 8033
2017-01-20 01:21:35,697 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8033
2017-01-20 01:21:35,698 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:21:35,698 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to standby state
2017-01-20 01:21:35,698 WARN org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher: org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher$LauncherThread interrupted. Returning.
2017-01-20 01:21:35,699 INFO org.apache.hadoop.ipc.Server: Stopping server on 8030
2017-01-20 01:21:35,707 INFO org.apache.hadoop.ipc.Server: Stopping server on 8031
2017-01-20 01:21:35,707 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8030
2017-01-20 01:21:35,707 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:21:35,717 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8031
2017-01-20 01:21:35,717 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2017-01-20 01:21:35,717 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: NMLivelinessMonitor thread interrupted
2017-01-20 01:21:35,717 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Returning, interrupted : java.lang.InterruptedException
2017-01-20 01:21:35,717 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, igonring any new events.
2017-01-20 01:21:35,717 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2017-01-20 01:21:35,717 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer thread interrupted
2017-01-20 01:21:35,717 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2017-01-20 01:21:35,717 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2017-01-20 01:21:35,718 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ResourceManager metrics system...
2017-01-20 01:21:35,719 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system stopped.
2017-01-20 01:21:35,719 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.
2017-01-20 01:21:35,719 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, igonring any new events.
2017-01-20 01:21:35,719 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to standby state
2017-01-20 01:21:35,719 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down ResourceManager at chengguan-3.local/192.168.0.100
************************************************************/
2017-01-20 01:22:51,425 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = chengguan-3.local/192.168.0.100
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/rm-config/log4j.properties
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_91
************************************************************/
2017-01-20 01:22:51,439 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-20 01:22:51,795 INFO org.apache.hadoop.conf.Configuration: found resource core-site.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/core-site.xml
2017-01-20 01:22:51,864 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-01-20 01:22:51,947 INFO org.apache.hadoop.security.Groups: clearing userToGroupsMap cache
2017-01-20 01:22:52,089 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/yarn-site.xml
2017-01-20 01:22:52,412 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2017-01-20 01:22:52,866 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2017-01-20 01:22:52,884 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2017-01-20 01:22:52,898 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2017-01-20 01:22:53,050 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2017-01-20 01:22:53,057 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2017-01-20 01:22:53,057 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2017-01-20 01:22:53,083 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher
2017-01-20 01:22:53,085 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2017-01-20 01:22:53,086 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2017-01-20 01:22:53,087 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2017-01-20 01:22:53,234 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-20 01:22:53,389 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-20 01:22:53,389 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2017-01-20 01:22:53,413 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2017-01-20 01:22:53,422 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2017-01-20 01:22:53,424 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2017-01-20 01:22:53,428 INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instiantiated.
2017-01-20 01:22:53,429 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2017-01-20 01:22:53,431 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/Users/lybuestc/soft/hadoop-2.7.3/etc/hadoop/capacity-scheduler.xml
2017-01-20 01:22:53,524 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2017-01-20 01:22:53,525 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2017-01-20 01:22:53,533 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, asboluteCapacity=1.0, maxCapacity=1.0, asboluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
, reservationsContinueLooking=true
2017-01-20 01:22:53,533 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2017-01-20 01:22:53,545 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2017-01-20 01:22:53,545 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2017-01-20 01:22:53,546 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
asboluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:8192, vCores:32> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
nodeLocalityDelay = 40
labels=*,
nodeLocalityDelay = 40
reservationsContinueLooking = true
preemptionDisabled = true

2017-01-20 01:22:53,546 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0
2017-01-20 01:22:53,546 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2017-01-20 01:22:53,546 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2017-01-20 01:22:53,547 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue mappings, override: false
2017-01-20 01:22:53,547 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:32>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2017-01-20 01:22:53,559 INFO org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher: YARN system metrics publishing service is not enabled
2017-01-20 01:22:53,559 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state
2017-01-20 01:22:53,588 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2017-01-20 01:22:53,588 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2017-01-20 01:22:53,589 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2017-01-20 01:22:53,590 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:22:53,591 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 1
2017-01-20 01:22:53,595 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2017-01-20 01:22:53,599 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2017-01-20 01:22:53,599 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:22:53,600 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 2
2017-01-20 01:22:53,600 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2017-01-20 01:22:53,603 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
2017-01-20 01:22:53,656 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:22:53,697 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8031
2017-01-20 01:22:53,728 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
2017-01-20 01:22:53,728 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:22:53,728 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8031: starting
2017-01-20 01:22:53,797 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:22:53,803 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8030
2017-01-20 01:22:53,812 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
2017-01-20 01:22:53,813 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:22:53,813 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8030: starting
2017-01-20 01:22:53,904 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:22:53,905 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8032
2017-01-20 01:22:53,909 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
2017-01-20 01:22:53,911 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:22:53,918 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8032: starting
2017-01-20 01:22:53,942 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
2017-01-20 01:22:54,203 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-20 01:22:54,225 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-20 01:22:54,241 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2017-01-20 01:22:54,258 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-20 01:22:54,263 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
2017-01-20 01:22:54,263 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
2017-01-20 01:22:54,263 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
2017-01-20 01:22:54,264 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2017-01-20 01:22:54,264 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-20 01:22:54,264 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-20 01:22:54,270 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /cluster/*
2017-01-20 01:22:54,271 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2017-01-20 01:22:55,112 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2017-01-20 01:22:55,115 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8088
2017-01-20 01:22:55,115 INFO org.mortbay.log: jetty-6.1.26
2017-01-20 01:22:55,152 INFO org.mortbay.log: Extract jar:file:/Users/lybuestc/soft/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar!/webapps/cluster to /var/folders/xl/5zp3ynpd6l5cg_y4f6qjqxxm0000gn/T/Jetty_0_0_0_0_8088_cluster____u0rgz3/webapp
2017-01-20 01:22:55,433 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:22:55,433 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2017-01-20 01:22:55,434 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2017-01-20 01:22:57,502 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8088
2017-01-20 01:22:57,504 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at 8088
2017-01-20 01:22:57,556 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-20 01:22:57,557 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2017-01-20 01:22:57,559 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2017-01-20 01:22:57,559 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-20 01:22:57,560 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8033: starting
2017-01-20 01:22:58,664 INFO org.apache.hadoop.yarn.util.RackResolver: Resolved 192.168.0.100 to /default-rack
2017-01-20 01:22:58,667 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node 192.168.0.100(cmPort: 54094 httpPort: 8042) registered with capability: <memory:8192, vCores:8>, assigned nodeId 192.168.0.100:54094
2017-01-20 01:22:58,673 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: 192.168.0.100:54094 Node Transitioned from NEW to RUNNING
2017-01-20 01:22:58,683 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node 192.168.0.100:54094 clusterResource: <memory:8192, vCores:8>
2017-01-20 01:23:14,900 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 1
2017-01-20 01:23:16,351 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 1 submitted by user lybuestc
2017-01-20 01:23:16,351 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484846573561_0001
2017-01-20 01:23:16,354 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484846573561_0001
2017-01-20 01:23:16,359 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484846573561_0001
2017-01-20 01:23:16,359 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0001 State change from NEW to NEW_SAVING
2017-01-20 01:23:16,360 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0001 State change from NEW_SAVING to SUBMITTED
2017-01-20 01:23:16,363 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484846573561_0001 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-20 01:23:16,364 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484846573561_0001 from user: lybuestc, in queue: default
2017-01-20 01:23:16,400 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0001 State change from SUBMITTED to ACCEPTED
2017-01-20 01:23:16,434 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484846573561_0001_000001
2017-01-20 01:23:16,436 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0001_000001 State change from NEW to SUBMITTED
2017-01-20 01:23:16,451 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:23:16,452 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 01:23:16,452 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484846573561_0001 from user: lybuestc activated in queue: default
2017-01-20 01:23:16,452 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484846573561_0001 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@79bf6dbc, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 01:23:16,452 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484846573561_0001_000001 to scheduler from user lybuestc in queue default
2017-01-20 01:23:16,455 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0001_000001 State change from SUBMITTED to SCHEDULED
2017-01-20 01:23:16,887 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0001_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 01:23:16,887 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846573561_0001	CONTAINERID=container_1484846573561_0001_01_000001
2017-01-20 01:23:16,888 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484846573561_0001_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:54094, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 01:23:16,888 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484846573561_0001_000001 container=Container: [ContainerId: container_1484846573561_0001_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 01:23:16,888 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 01:23:16,889 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 01:23:16,907 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:54094 for container : container_1484846573561_0001_01_000001
2017-01-20 01:23:16,917 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 01:23:16,918 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484846573561_0001_000001
2017-01-20 01:23:16,921 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484846573561_0001 AttemptId: appattempt_1484846573561_0001_000001 MasterContainer: Container: [ContainerId: container_1484846573561_0001_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ]
2017-01-20 01:23:16,931 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 01:23:16,933 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 01:23:16,936 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484846573561_0001_000001
2017-01-20 01:23:16,981 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484846573561_0001_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ] for AM appattempt_1484846573561_0001_000001
2017-01-20 01:23:16,981 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484846573561_0001_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 01:23:16,983 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484846573561_0001_000001
2017-01-20 01:23:16,987 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484846573561_0001_000001
2017-01-20 01:23:17,434 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1484846573561_0001_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ] for AM appattempt_1484846573561_0001_000001
2017-01-20 01:23:17,435 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0001_000001 State change from ALLOCATED to LAUNCHED
2017-01-20 01:23:17,881 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
2017-01-20 01:23:23,426 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1484846573561_0001_000001 (auth:SIMPLE)
2017-01-20 01:23:23,436 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1484846573561_0001_000001
2017-01-20 01:23:23,437 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=192.168.0.100	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1484846573561_0001	APPATTEMPTID=appattempt_1484846573561_0001_000001
2017-01-20 01:23:23,437 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0001_000001 State change from LAUNCHED to RUNNING
2017-01-20 01:23:23,438 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0001 State change from ACCEPTED to RUNNING
2017-01-20 01:23:25,814 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484846573561_0001_000001 with final state: FINISHING, and exit status: -1000
2017-01-20 01:23:25,815 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0001_000001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:23:25,815 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484846573561_0001 with final state: FINISHING
2017-01-20 01:23:25,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0001 State change from RUNNING to FINAL_SAVING
2017-01-20 01:23:25,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484846573561_0001
2017-01-20 01:23:25,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0001_000001 State change from FINAL_SAVING to FINISHING
2017-01-20 01:23:25,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0001 State change from FINAL_SAVING to FINISHING
2017-01-20 01:23:26,820 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1484846573561_0001 unregistered successfully. 
2017-01-20 01:23:32,045 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0001_01_000001 Container Transitioned from RUNNING to COMPLETED
2017-01-20 01:23:32,045 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484846573561_0001_000001
2017-01-20 01:23:32,046 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484846573561_0001_01_000001 in state: COMPLETED event:FINISHED
2017-01-20 01:23:32,046 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846573561_0001	CONTAINERID=container_1484846573561_0001_01_000001
2017-01-20 01:23:32,046 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484846573561_0001_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:54094, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 01:23:32,046 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 01:23:32,046 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484846573561_0001_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 01:23:32,046 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 01:23:32,046 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 01:23:32,047 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484846573561_0001_000001 released container container_1484846573561_0001_01_000001 on node: host: 192.168.0.100:54094 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: FINISHED
2017-01-20 01:23:32,047 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484846573561_0001_000001
2017-01-20 01:23:32,049 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0001_000001 State change from FINISHING to FINISHED
2017-01-20 01:23:32,051 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0001 State change from FINISHING to FINISHED
2017-01-20 01:23:32,051 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484846573561_0001_000001 is done. finalState=FINISHED
2017-01-20 01:23:32,051 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484846573561_0001 requests cleared
2017-01-20 01:23:32,052 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484846573561_0001 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 01:23:32,052 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1484846573561_0001_000001
2017-01-20 01:23:32,052 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484846573561_0001 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-20 01:23:32,052 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1484846573561_0001
2017-01-20 01:23:32,055 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484846573561_0001,name=word count,user=lybuestc,queue=default,state=FINISHED,trackingUrl=http://chengguan-3.local:8088/proxy/application_1484846573561_0001/,appMasterHost=192.168.0.100,startTime=1484846596350,finishTime=1484846605815,finalStatus=SUCCEEDED,memorySeconds=31045,vcoreSeconds=15,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-20 01:32:53,448 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Release request cache is cleaned up
2017-01-20 09:42:27,555 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 2
2017-01-20 09:42:29,141 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 2 submitted by user lybuestc
2017-01-20 09:42:29,142 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484846573561_0002
2017-01-20 09:42:29,143 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484846573561_0002
2017-01-20 09:42:29,143 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0002 State change from NEW to NEW_SAVING
2017-01-20 09:42:29,144 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484846573561_0002
2017-01-20 09:42:29,144 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0002 State change from NEW_SAVING to SUBMITTED
2017-01-20 09:42:29,145 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484846573561_0002 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-20 09:42:29,145 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484846573561_0002 from user: lybuestc, in queue: default
2017-01-20 09:42:29,149 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0002 State change from SUBMITTED to ACCEPTED
2017-01-20 09:42:29,150 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484846573561_0002_000001
2017-01-20 09:42:29,150 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000001 State change from NEW to SUBMITTED
2017-01-20 09:42:29,151 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 09:42:29,151 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 09:42:29,151 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484846573561_0002 from user: lybuestc activated in queue: default
2017-01-20 09:42:29,151 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484846573561_0002 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@fe64643, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 09:42:29,151 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484846573561_0002_000001 to scheduler from user lybuestc in queue default
2017-01-20 09:42:29,153 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000001 State change from SUBMITTED to SCHEDULED
2017-01-20 09:42:29,615 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0002_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 09:42:29,615 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846573561_0002	CONTAINERID=container_1484846573561_0002_01_000001
2017-01-20 09:42:29,615 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484846573561_0002_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:54094, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 09:42:29,615 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484846573561_0002_000001 container=Container: [ContainerId: container_1484846573561_0002_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 09:42:29,617 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 09:42:29,619 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:54094 for container : container_1484846573561_0002_01_000001
2017-01-20 09:42:29,619 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 09:42:29,622 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0002_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 09:42:29,622 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484846573561_0002_000001
2017-01-20 09:42:29,623 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484846573561_0002 AttemptId: appattempt_1484846573561_0002_000001 MasterContainer: Container: [ContainerId: container_1484846573561_0002_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ]
2017-01-20 09:42:29,623 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 09:42:29,623 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 09:42:29,625 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484846573561_0002_000001
2017-01-20 09:42:29,632 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484846573561_0002_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ] for AM appattempt_1484846573561_0002_000001
2017-01-20 09:42:29,632 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484846573561_0002_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 09:42:29,633 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484846573561_0002_000001
2017-01-20 09:42:29,633 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484846573561_0002_000001
2017-01-20 09:51:49,874 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484846573561_0002_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 192.168.0.100:54094 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-20 09:51:49,874 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484846573561_0002_000001 with final state: FAILED, and exit status: -1000
2017-01-20 09:51:49,874 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-20 09:51:49,874 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484846573561_0002_000001
2017-01-20 09:51:49,875 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484846573561_0002_000001
2017-01-20 09:51:49,875 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000001 State change from FINAL_SAVING to FAILED
2017-01-20 09:51:49,875 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2017-01-20 09:51:49,875 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484846573561_0002_000002
2017-01-20 09:51:49,875 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484846573561_0002_000001 is done. finalState=FAILED
2017-01-20 09:51:49,876 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000002 State change from NEW to SUBMITTED
2017-01-20 09:51:49,891 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0002_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-20 09:51:49,891 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484846573561_0002_01_000001 in state: KILLED event:KILL
2017-01-20 09:51:49,892 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846573561_0002	CONTAINERID=container_1484846573561_0002_01_000001
2017-01-20 09:51:49,892 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484846573561_0002_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:54094, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 09:51:49,892 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 09:51:49,892 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484846573561_0002_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 09:51:49,892 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 09:51:49,892 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 09:51:49,892 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484846573561_0002_000001 released container container_1484846573561_0002_01_000001 on node: host: 192.168.0.100:54094 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-20 09:51:49,892 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484846573561_0002 requests cleared
2017-01-20 09:51:49,893 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484846573561_0002 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 09:51:49,893 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 09:51:49,893 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 09:51:49,893 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484846573561_0002 from user: lybuestc activated in queue: default
2017-01-20 09:51:49,893 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484846573561_0002 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@65b2058e, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 09:51:49,893 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484846573561_0002_000002 to scheduler from user lybuestc in queue default
2017-01-20 09:51:49,894 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000002 State change from SUBMITTED to SCHEDULED
2017-01-20 09:51:50,641 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0002_02_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 09:51:50,641 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846573561_0002	CONTAINERID=container_1484846573561_0002_02_000001
2017-01-20 09:51:50,641 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484846573561_0002_02_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:54094, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 09:51:50,642 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484846573561_0002_000002 container=Container: [ContainerId: container_1484846573561_0002_02_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 09:51:50,642 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 09:51:50,642 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 09:51:50,645 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:54094 for container : container_1484846573561_0002_02_000001
2017-01-20 09:51:50,646 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0002_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 09:51:50,647 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484846573561_0002_000002
2017-01-20 09:51:50,647 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484846573561_0002 AttemptId: appattempt_1484846573561_0002_000002 MasterContainer: Container: [ContainerId: container_1484846573561_0002_02_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ]
2017-01-20 09:51:50,647 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000002 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 09:51:50,647 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000002 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 09:51:50,648 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484846573561_0002_000002
2017-01-20 09:51:50,653 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484846573561_0002_02_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ] for AM appattempt_1484846573561_0002_000002
2017-01-20 09:51:50,653 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484846573561_0002_02_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 09:51:50,653 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484846573561_0002_000002
2017-01-20 09:51:50,654 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484846573561_0002_000002
2017-01-20 10:01:10,810 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484846573561_0002_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 192.168.0.100:54094 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor40.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-20 10:01:10,810 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484846573561_0002_000002 with final state: FAILED, and exit status: -1000
2017-01-20 10:01:10,810 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000002 State change from ALLOCATED to FINAL_SAVING
2017-01-20 10:01:10,810 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484846573561_0002_000002
2017-01-20 10:01:10,810 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484846573561_0002_000002
2017-01-20 10:01:10,810 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0002_000002 State change from FINAL_SAVING to FAILED
2017-01-20 10:01:10,810 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 2. The max attempts is 2
2017-01-20 10:01:10,812 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484846573561_0002 with final state: FAILED
2017-01-20 10:01:10,813 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0002 State change from ACCEPTED to FINAL_SAVING
2017-01-20 10:01:10,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484846573561_0002
2017-01-20 10:01:10,813 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484846573561_0002_000002 is done. finalState=FAILED
2017-01-20 10:01:10,813 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Application application_1484846573561_0002 failed 2 times due to Error launching appattempt_1484846573561_0002_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 192.168.0.100:54094 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor40.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.
2017-01-20 10:01:10,813 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0002_02_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-20 10:01:10,813 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0002 State change from FINAL_SAVING to FAILED
2017-01-20 10:01:10,814 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484846573561_0002_02_000001 in state: KILLED event:KILL
2017-01-20 10:01:10,814 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846573561_0002	CONTAINERID=container_1484846573561_0002_02_000001
2017-01-20 10:01:10,814 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484846573561_0002_02_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:54094, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 10:01:10,814 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Failed	TARGET=RMAppManager	RESULT=FAILURE	DESCRIPTION=App failed with state: FAILED	PERMISSIONS=Application application_1484846573561_0002 failed 2 times due to Error launching appattempt_1484846573561_0002_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 192.168.0.100:54094 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor40.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.	APPID=application_1484846573561_0002
2017-01-20 10:01:10,814 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 10:01:10,814 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484846573561_0002_02_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 10:01:10,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 10:01:10,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 10:01:10,815 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484846573561_0002,name=word count,user=lybuestc,queue=default,state=FAILED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484846573561_0002,appMasterHost=N/A,startTime=1484876549140,finishTime=1484877670812,finalStatus=FAILED,memorySeconds=2294677,vcoreSeconds=1120,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-20 10:01:10,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484846573561_0002_000002 released container container_1484846573561_0002_02_000001 on node: host: 192.168.0.100:54094 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-20 10:01:10,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484846573561_0002 requests cleared
2017-01-20 10:01:10,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484846573561_0002 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 10:01:10,815 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484846573561_0002 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-20 10:03:07,356 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 3
2017-01-20 10:03:08,248 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 3 submitted by user lybuestc
2017-01-20 10:03:08,248 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1484846573561_0003
2017-01-20 10:03:08,248 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	IP=172.17.45.96	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1484846573561_0003
2017-01-20 10:03:08,248 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0003 State change from NEW to NEW_SAVING
2017-01-20 10:03:08,249 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1484846573561_0003
2017-01-20 10:03:08,249 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0003 State change from NEW_SAVING to SUBMITTED
2017-01-20 10:03:08,249 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1484846573561_0003 user: lybuestc leaf-queue of parent: root #applications: 1
2017-01-20 10:03:08,249 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1484846573561_0003 from user: lybuestc, in queue: default
2017-01-20 10:03:08,255 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0003 State change from SUBMITTED to ACCEPTED
2017-01-20 10:03:08,255 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484846573561_0003_000001
2017-01-20 10:03:08,255 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000001 State change from NEW to SUBMITTED
2017-01-20 10:03:08,255 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 10:03:08,255 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 10:03:08,255 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484846573561_0003 from user: lybuestc activated in queue: default
2017-01-20 10:03:08,255 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484846573561_0003 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@421f47e1, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 10:03:08,255 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484846573561_0003_000001 to scheduler from user lybuestc in queue default
2017-01-20 10:03:08,256 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000001 State change from SUBMITTED to SCHEDULED
2017-01-20 10:03:09,027 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0003_01_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 10:03:09,027 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846573561_0003	CONTAINERID=container_1484846573561_0003_01_000001
2017-01-20 10:03:09,027 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484846573561_0003_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:54094, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 10:03:09,027 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484846573561_0003_000001 container=Container: [ContainerId: container_1484846573561_0003_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 10:03:09,028 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 10:03:09,028 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 10:03:09,029 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:54094 for container : container_1484846573561_0003_01_000001
2017-01-20 10:03:09,033 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0003_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 10:03:09,033 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484846573561_0003_000001
2017-01-20 10:03:09,033 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484846573561_0003 AttemptId: appattempt_1484846573561_0003_000001 MasterContainer: Container: [ContainerId: container_1484846573561_0003_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ]
2017-01-20 10:03:09,033 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000001 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 10:03:09,033 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000001 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 10:03:09,034 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484846573561_0003_000001
2017-01-20 10:03:09,036 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484846573561_0003_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ] for AM appattempt_1484846573561_0003_000001
2017-01-20 10:03:09,036 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484846573561_0003_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 10:03:09,036 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484846573561_0003_000001
2017-01-20 10:03:09,036 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484846573561_0003_000001
2017-01-20 10:05:56,836 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://192.168.0.100:19888/jobhistory/job/job_1484846573561_0001 which is the app master GUI of application_1484846573561_0001 owned by lybuestc
2017-01-20 10:12:29,819 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484846573561_0003_000001. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 192.168.0.100:54094 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor40.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-20 10:12:29,819 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484846573561_0003_000001 with final state: FAILED, and exit status: -1000
2017-01-20 10:12:29,819 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000001 State change from ALLOCATED to FINAL_SAVING
2017-01-20 10:12:29,819 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484846573561_0003_000001
2017-01-20 10:12:29,819 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484846573561_0003_000001
2017-01-20 10:12:29,819 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000001 State change from FINAL_SAVING to FAILED
2017-01-20 10:12:29,819 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2017-01-20 10:12:29,820 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484846573561_0003_000001 is done. finalState=FAILED
2017-01-20 10:12:29,820 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1484846573561_0003_000002
2017-01-20 10:12:29,820 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000002 State change from NEW to SUBMITTED
2017-01-20 10:12:29,820 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0003_01_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-20 10:12:29,820 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484846573561_0003_01_000001 in state: KILLED event:KILL
2017-01-20 10:12:29,820 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846573561_0003	CONTAINERID=container_1484846573561_0003_01_000001
2017-01-20 10:12:29,820 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484846573561_0003_01_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:54094, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 10:12:29,820 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 10:12:29,820 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484846573561_0003_01_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 10:12:29,820 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 10:12:29,821 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 10:12:29,821 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484846573561_0003_000001 released container container_1484846573561_0003_01_000001 on node: host: 192.168.0.100:54094 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-20 10:12:29,821 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484846573561_0003 requests cleared
2017-01-20 10:12:29,821 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484846573561_0003 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 10:12:29,821 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 10:12:29,821 WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start
2017-01-20 10:12:29,821 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1484846573561_0003 from user: lybuestc activated in queue: default
2017-01-20 10:12:29,821 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1484846573561_0003 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@65016d35, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2017-01-20 10:12:29,821 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1484846573561_0003_000002 to scheduler from user lybuestc in queue default
2017-01-20 10:12:29,821 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000002 State change from SUBMITTED to SCHEDULED
2017-01-20 10:12:29,939 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0003_02_000001 Container Transitioned from NEW to ALLOCATED
2017-01-20 10:12:29,939 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846573561_0003	CONTAINERID=container_1484846573561_0003_02_000001
2017-01-20 10:12:29,939 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1484846573561_0003_02_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:54094, which has 1 containers, <memory:2048, vCores:1> used and <memory:6144, vCores:7> available after allocation
2017-01-20 10:12:29,940 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application attempt=appattempt_1484846573561_0003_000002 container=Container: [ContainerId: container_1484846573561_0003_02_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: null, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 clusterResource=<memory:8192, vCores:8> type=OFF_SWITCH
2017-01-20 10:12:29,940 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:1>, usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=1
2017-01-20 10:12:29,940 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:1> cluster=<memory:8192, vCores:8>
2017-01-20 10:12:29,941 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : 192.168.0.100:54094 for container : container_1484846573561_0003_02_000001
2017-01-20 10:12:29,942 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0003_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2017-01-20 10:12:29,942 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1484846573561_0003_000002
2017-01-20 10:12:29,942 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1484846573561_0003 AttemptId: appattempt_1484846573561_0003_000002 MasterContainer: Container: [ContainerId: container_1484846573561_0003_02_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ]
2017-01-20 10:12:29,942 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000002 State change from SCHEDULED to ALLOCATED_SAVING
2017-01-20 10:12:29,942 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000002 State change from ALLOCATED_SAVING to ALLOCATED
2017-01-20 10:12:29,943 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1484846573561_0003_000002
2017-01-20 10:12:29,945 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1484846573561_0003_02_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ] for AM appattempt_1484846573561_0003_000002
2017-01-20 10:12:29,946 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1484846573561_0003_02_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
2017-01-20 10:12:29,946 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1484846573561_0003_000002
2017-01-20 10:12:29,946 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1484846573561_0003_000002
2017-01-20 10:21:50,144 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1484846573561_0003_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 192.168.0.100:54094 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor40.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more

2017-01-20 10:21:50,144 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1484846573561_0003_000002 with final state: FAILED, and exit status: -1000
2017-01-20 10:21:50,145 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000002 State change from ALLOCATED to FINAL_SAVING
2017-01-20 10:21:50,145 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1484846573561_0003_000002
2017-01-20 10:21:50,145 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1484846573561_0003_000002
2017-01-20 10:21:50,145 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1484846573561_0003_000002 State change from FINAL_SAVING to FAILED
2017-01-20 10:21:50,145 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 2. The max attempts is 2
2017-01-20 10:21:50,145 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1484846573561_0003 with final state: FAILED
2017-01-20 10:21:50,145 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0003 State change from ACCEPTED to FINAL_SAVING
2017-01-20 10:21:50,145 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1484846573561_0003
2017-01-20 10:21:50,145 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1484846573561_0003_000002 is done. finalState=FAILED
2017-01-20 10:21:50,145 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Application application_1484846573561_0003 failed 2 times due to Error launching appattempt_1484846573561_0003_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 192.168.0.100:54094 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor40.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.
2017-01-20 10:21:50,146 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1484846573561_0003_02_000001 Container Transitioned from ACQUIRED to KILLED
2017-01-20 10:21:50,146 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1484846573561_0003 State change from FINAL_SAVING to FAILED
2017-01-20 10:21:50,146 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Completed container: container_1484846573561_0003_02_000001 in state: KILLED event:KILL
2017-01-20 10:21:50,146 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1484846573561_0003	CONTAINERID=container_1484846573561_0003_02_000001
2017-01-20 10:21:50,146 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=lybuestc	OPERATION=Application Finished - Failed	TARGET=RMAppManager	RESULT=FAILURE	DESCRIPTION=App failed with state: FAILED	PERMISSIONS=Application application_1484846573561_0003 failed 2 times due to Error launching appattempt_1484846573561_0003_000002. Got exception: org.apache.hadoop.net.ConnectTimeoutException: Call From chengguan-3.local/172.17.45.96 to 192.168.0.100:54094 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor40.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy82.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy83.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:250)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.0.100/192.168.0.100:54094]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 15 more
. Failing the application.	APPID=application_1484846573561_0003
2017-01-20 10:21:50,146 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1484846573561_0003_02_000001 of capacity <memory:2048, vCores:1> on host 192.168.0.100:54094, which currently has 0 containers, <memory:0, vCores:0> used and <memory:8192, vCores:8> available, release resources=true
2017-01-20 10:21:50,146 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=<memory:0, vCores:0> numContainers=0 user=lybuestc user-resources=<memory:0, vCores:0>
2017-01-20 10:21:50,146 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: completedContainer container=Container: [ContainerId: container_1484846573561_0003_02_000001, NodeId: 192.168.0.100:54094, NodeHttpAddress: 192.168.0.100:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 192.168.0.100:54094 }, ] queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0 cluster=<memory:8192, vCores:8>
2017-01-20 10:21:50,146 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1484846573561_0003,name=word count,user=lybuestc,queue=default,state=FAILED,trackingUrl=http://chengguan-3.local:8088/cluster/app/application_1484846573561_0003,appMasterHost=N/A,startTime=1484877788248,finishTime=1484878910145,finalStatus=FAILED,memorySeconds=2295805,vcoreSeconds=1120,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE
2017-01-20 10:21:50,146 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:8192, vCores:8>
2017-01-20 10:21:50,148 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0
2017-01-20 10:21:50,148 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1484846573561_0003_000002 released container container_1484846573561_0003_02_000001 on node: host: 192.168.0.100:54094 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2017-01-20 10:21:50,148 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1484846573561_0003 requests cleared
2017-01-20 10:21:50,148 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1484846573561_0003 user: lybuestc queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2017-01-20 10:21:50,148 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1484846573561_0003 user: lybuestc leaf-queue of parent: root #applications: 0
2017-01-20 11:53:44,253 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8031: readAndProcess from client 192.168.0.100 threw exception [java.io.IOException: Operation timed out]
java.io.IOException: Operation timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:2603)
	at org.apache.hadoop.ipc.Server.access$2800(Server.java:136)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1481)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:771)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:637)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
